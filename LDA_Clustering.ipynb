{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import gensim\n",
    "import logging\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import cycle\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.dirname(os.path.realpath('__file__'))\n",
    "essay_path = root + '/../Larson_Project/essays/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all essays into hash table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(essay_path)\n",
    "\n",
    "essays = {}\n",
    "count = 0\n",
    "for file in files:\n",
    "    # attempt to confidently guess encoding; otherwise, default to ISO-8859-1\n",
    "    encoding = \"ISO-8859-1\"\n",
    "    guess = chardet.detect(open(essay_path + file, \"rb\").read())\n",
    "    if (guess[\"confidence\"] >= 0.95):\n",
    "        encoding = guess[\"encoding\"]\n",
    "    \n",
    "    with open(essay_path + file, \"r\", encoding=encoding) as f:\n",
    "        essays[file] = f.read()\n",
    "        \n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging for Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text into lowercase tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_essays = {label: gensim.utils.simple_preprocess(corpus, deacc=True, min_len=2, max_len=15) for (label, corpus) in essays.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokenized_essays = {label: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in token_lst if w not in string.punctuation] for (label, token_lst) in tokenized_essays.items()}\n",
    "tokenized_essays_list = list(tokenized_essays.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "custom_stopwords = [\n",
    "        \"prison\",\n",
    "        \"prisoner\",\n",
    "        \"jail\",\n",
    "        \"also\",\n",
    "        \"said\",\n",
    "        \"would\",\n",
    "        \"could\",\n",
    "        \"should\",\n",
    "        \"first\",\n",
    "        \"like\",\n",
    "        \"get\",\n",
    "        \"going\",\n",
    "        \"thing\",\n",
    "        \"something\",\n",
    "        \"use\",\n",
    "        \"get\",\n",
    "        \"go\",\n",
    "        \"one\"\n",
    "    ]\n",
    "tokenized_essays = {label: [w for w in token_lst if w not in english_stopwords and w not in custom_stopwords] for (label, token_lst) in tokenized_essays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:49:51,603 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-21 20:49:53,336 : INFO : built Dictionary(57330 unique tokens: ['aggressive', 'air', 'anger', 'another', 'anyone']...) from 1573 documents (total 1194020 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from ‘processed_docs’ \n",
    "# containing the number of times a word appears in the training set.\n",
    "dictionary = gensim.corpora.Dictionary(tokenized_essays_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aggressive\n",
      "1 air\n",
      "2 anger\n",
      "3 another\n",
      "4 anyone\n",
      "5 anything\n",
      "6 attack\n",
      "7 august\n",
      "8 away\n",
      "9 bad\n",
      "10 battlefield\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:49:53,502 : INFO : discarding 51271 tokens: [('another', 856), ('battlefield', 10), ('bile', 4), ('blight', 11), ('bludgeon', 4), ('caustic', 3), ('cheapen', 1), ('coarseness', 1), ('come', 1071), ('defective', 12)]...\n",
      "2019-10-21 20:49:53,503 : INFO : keeping 6059 tokens which were in no less than 15 and no more than 786 (=50.0%) documents\n",
      "2019-10-21 20:49:53,539 : INFO : resulting dictionary: Dictionary(6059 unique tokens: ['aggressive', 'air', 'anger', 'anyone', 'anything']...)\n"
     ]
    }
   ],
   "source": [
    "# Filter out tokens that appear in\n",
    "# less than 15 documents (absolute number) or\n",
    "# more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "# after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 1),\n",
       " (18, 1),\n",
       " (22, 2),\n",
       " (24, 3),\n",
       " (31, 3),\n",
       " (60, 2),\n",
       " (65, 1),\n",
       " (71, 4),\n",
       " (83, 1),\n",
       " (85, 2),\n",
       " (91, 3),\n",
       " (103, 1),\n",
       " (107, 2),\n",
       " (108, 3),\n",
       " (112, 1),\n",
       " (119, 1),\n",
       " (122, 1),\n",
       " (149, 2),\n",
       " (152, 1),\n",
       " (163, 2),\n",
       " (174, 1),\n",
       " (186, 1),\n",
       " (194, 2),\n",
       " (202, 2),\n",
       " (210, 1),\n",
       " (227, 2),\n",
       " (241, 1),\n",
       " (247, 1),\n",
       " (248, 2),\n",
       " (256, 3),\n",
       " (257, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (271, 1),\n",
       " (275, 7),\n",
       " (277, 1),\n",
       " (280, 2),\n",
       " (288, 8),\n",
       " (289, 1),\n",
       " (291, 1),\n",
       " (298, 1),\n",
       " (307, 5),\n",
       " (325, 2),\n",
       " (333, 1),\n",
       " (334, 1),\n",
       " (336, 5),\n",
       " (337, 1),\n",
       " (338, 1),\n",
       " (347, 1),\n",
       " (348, 1),\n",
       " (351, 1),\n",
       " (353, 2),\n",
       " (367, 5),\n",
       " (371, 1),\n",
       " (375, 6),\n",
       " (377, 8),\n",
       " (383, 1),\n",
       " (386, 28),\n",
       " (391, 2),\n",
       " (409, 1),\n",
       " (411, 1),\n",
       " (415, 1),\n",
       " (424, 1),\n",
       " (434, 1),\n",
       " (437, 1),\n",
       " (439, 8),\n",
       " (453, 1),\n",
       " (460, 1),\n",
       " (464, 2),\n",
       " (470, 4),\n",
       " (471, 2),\n",
       " (493, 1),\n",
       " (505, 2),\n",
       " (517, 1),\n",
       " (530, 1),\n",
       " (531, 7),\n",
       " (533, 1),\n",
       " (544, 2),\n",
       " (552, 2),\n",
       " (556, 1),\n",
       " (561, 1),\n",
       " (565, 3),\n",
       " (570, 8),\n",
       " (583, 1),\n",
       " (601, 1),\n",
       " (604, 1),\n",
       " (613, 1),\n",
       " (615, 1),\n",
       " (626, 3),\n",
       " (631, 1),\n",
       " (658, 1),\n",
       " (663, 1),\n",
       " (671, 1),\n",
       " (673, 1),\n",
       " (685, 1),\n",
       " (711, 2),\n",
       " (721, 1),\n",
       " (723, 1),\n",
       " (732, 1),\n",
       " (747, 1),\n",
       " (765, 1),\n",
       " (768, 2),\n",
       " (771, 7),\n",
       " (776, 3),\n",
       " (801, 1),\n",
       " (806, 1),\n",
       " (829, 1),\n",
       " (837, 2),\n",
       " (863, 1),\n",
       " (883, 1),\n",
       " (886, 1),\n",
       " (910, 1),\n",
       " (912, 1),\n",
       " (913, 1),\n",
       " (915, 1),\n",
       " (934, 2),\n",
       " (946, 1),\n",
       " (966, 2),\n",
       " (973, 1),\n",
       " (996, 2),\n",
       " (1064, 3),\n",
       " (1065, 1),\n",
       " (1078, 4),\n",
       " (1087, 1),\n",
       " (1094, 1),\n",
       " (1111, 1),\n",
       " (1124, 1),\n",
       " (1125, 2),\n",
       " (1130, 1),\n",
       " (1133, 6),\n",
       " (1135, 1),\n",
       " (1139, 2),\n",
       " (1146, 1),\n",
       " (1151, 3),\n",
       " (1154, 51),\n",
       " (1156, 2),\n",
       " (1175, 2),\n",
       " (1190, 3),\n",
       " (1195, 8),\n",
       " (1196, 18),\n",
       " (1208, 1),\n",
       " (1229, 1),\n",
       " (1231, 4),\n",
       " (1233, 1),\n",
       " (1237, 1),\n",
       " (1255, 7),\n",
       " (1280, 2),\n",
       " (1285, 1),\n",
       " (1290, 2),\n",
       " (1297, 1),\n",
       " (1301, 3),\n",
       " (1303, 2),\n",
       " (1314, 1),\n",
       " (1319, 3),\n",
       " (1334, 1),\n",
       " (1336, 1),\n",
       " (1338, 1),\n",
       " (1341, 11),\n",
       " (1345, 3),\n",
       " (1352, 2),\n",
       " (1354, 1),\n",
       " (1355, 6),\n",
       " (1365, 1),\n",
       " (1366, 3),\n",
       " (1378, 1),\n",
       " (1380, 2),\n",
       " (1399, 1),\n",
       " (1406, 3),\n",
       " (1432, 2),\n",
       " (1448, 1),\n",
       " (1465, 1),\n",
       " (1481, 1),\n",
       " (1486, 1),\n",
       " (1488, 1),\n",
       " (1493, 1),\n",
       " (1499, 1),\n",
       " (1506, 1),\n",
       " (1517, 1),\n",
       " (1536, 1),\n",
       " (1544, 1),\n",
       " (1561, 1),\n",
       " (1584, 1),\n",
       " (1687, 1),\n",
       " (1696, 1),\n",
       " (1721, 2),\n",
       " (1792, 2),\n",
       " (1797, 1),\n",
       " (1846, 2),\n",
       " (1849, 1),\n",
       " (1887, 1),\n",
       " (1888, 1),\n",
       " (1911, 1),\n",
       " (1912, 1),\n",
       " (1922, 1),\n",
       " (1943, 1),\n",
       " (1944, 1),\n",
       " (1972, 1),\n",
       " (1975, 2),\n",
       " (1994, 1),\n",
       " (2014, 1),\n",
       " (2061, 1),\n",
       " (2065, 1),\n",
       " (2118, 1),\n",
       " (2165, 1),\n",
       " (2206, 1),\n",
       " (2261, 3),\n",
       " (2265, 1),\n",
       " (2270, 1),\n",
       " (2272, 1),\n",
       " (2273, 1),\n",
       " (2289, 1),\n",
       " (2291, 1),\n",
       " (2293, 1),\n",
       " (2300, 1),\n",
       " (2308, 1),\n",
       " (2310, 1),\n",
       " (2312, 1),\n",
       " (2314, 2),\n",
       " (2324, 1),\n",
       " (2328, 1),\n",
       " (2331, 2),\n",
       " (2337, 3),\n",
       " (2356, 1),\n",
       " (2359, 2),\n",
       " (2363, 1),\n",
       " (2373, 1),\n",
       " (2388, 1),\n",
       " (2401, 1),\n",
       " (2413, 1),\n",
       " (2436, 1),\n",
       " (2446, 2),\n",
       " (2454, 1),\n",
       " (2455, 1),\n",
       " (2470, 1),\n",
       " (2471, 7),\n",
       " (2474, 2),\n",
       " (2486, 1),\n",
       " (2507, 2),\n",
       " (2513, 1),\n",
       " (2516, 1),\n",
       " (2527, 1),\n",
       " (2533, 1),\n",
       " (2559, 1),\n",
       " (2567, 1),\n",
       " (2593, 2),\n",
       " (2594, 1),\n",
       " (2603, 1),\n",
       " (2622, 1),\n",
       " (2625, 1),\n",
       " (2629, 1),\n",
       " (2648, 1),\n",
       " (2649, 1),\n",
       " (2668, 1),\n",
       " (2679, 1),\n",
       " (2681, 2),\n",
       " (2687, 4),\n",
       " (2689, 5),\n",
       " (2697, 1),\n",
       " (2804, 6),\n",
       " (2810, 1),\n",
       " (2814, 3),\n",
       " (2815, 2),\n",
       " (2817, 4),\n",
       " (2819, 1),\n",
       " (2822, 1),\n",
       " (2827, 1),\n",
       " (2830, 8),\n",
       " (2833, 3),\n",
       " (2845, 1),\n",
       " (2855, 1),\n",
       " (2870, 1),\n",
       " (2871, 1),\n",
       " (2939, 3),\n",
       " (2958, 2),\n",
       " (2961, 1),\n",
       " (2993, 1),\n",
       " (3034, 2),\n",
       " (3054, 1),\n",
       " (3055, 1),\n",
       " (3056, 1),\n",
       " (3057, 3),\n",
       " (3058, 1),\n",
       " (3059, 1),\n",
       " (3060, 1),\n",
       " (3061, 2),\n",
       " (3062, 1),\n",
       " (3063, 1),\n",
       " (3064, 1),\n",
       " (3065, 2),\n",
       " (3066, 1),\n",
       " (3067, 41),\n",
       " (3068, 3),\n",
       " (3069, 2),\n",
       " (3070, 1),\n",
       " (3071, 1),\n",
       " (3072, 5),\n",
       " (3073, 1),\n",
       " (3074, 1),\n",
       " (3075, 2),\n",
       " (3076, 2),\n",
       " (3077, 5),\n",
       " (3078, 1),\n",
       " (3079, 1),\n",
       " (3080, 1),\n",
       " (3081, 1),\n",
       " (3082, 1),\n",
       " (3083, 1),\n",
       " (3084, 1),\n",
       " (3085, 1),\n",
       " (3086, 1),\n",
       " (3087, 7),\n",
       " (3088, 1),\n",
       " (3089, 1),\n",
       " (3090, 1),\n",
       " (3091, 1),\n",
       " (3092, 1),\n",
       " (3093, 1),\n",
       " (3094, 2),\n",
       " (3095, 1),\n",
       " (3096, 1),\n",
       " (3097, 7),\n",
       " (3098, 1),\n",
       " (3099, 2),\n",
       " (3100, 1),\n",
       " (3101, 1),\n",
       " (3102, 1),\n",
       " (3103, 1),\n",
       " (3104, 1),\n",
       " (3105, 8),\n",
       " (3106, 1),\n",
       " (3107, 1),\n",
       " (3108, 1),\n",
       " (3109, 2),\n",
       " (3110, 1),\n",
       " (3111, 1),\n",
       " (3112, 7),\n",
       " (3113, 1),\n",
       " (3114, 1),\n",
       " (3115, 19),\n",
       " (3116, 1),\n",
       " (3117, 3),\n",
       " (3118, 1),\n",
       " (3119, 4),\n",
       " (3120, 1),\n",
       " (3121, 2),\n",
       " (3122, 1),\n",
       " (3123, 1),\n",
       " (3124, 1),\n",
       " (3125, 1),\n",
       " (3126, 1),\n",
       " (3127, 1),\n",
       " (3128, 1),\n",
       " (3129, 1),\n",
       " (3130, 2),\n",
       " (3131, 1),\n",
       " (3132, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each document, create a dictionary reporting how many\n",
    "# words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_essays_list]\n",
    "bow_corpus[4310]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 6 (\"august\") appears 1 time.\n",
      "Word 18 (\"cannot\") appears 1 time.\n",
      "Word 22 (\"citizen\") appears 2 time.\n",
      "Word 24 (\"community\") appears 3 time.\n",
      "Word 31 (\"criminal\") appears 3 time.\n",
      "Word 60 (\"good\") appears 2 time.\n",
      "Word 65 (\"help\") appears 1 time.\n",
      "Word 71 (\"incarceration\") appears 4 time.\n",
      "Word 83 (\"love\") appears 1 time.\n",
      "Word 85 (\"mass\") appears 2 time.\n",
      "Word 91 (\"must\") appears 3 time.\n",
      "Word 103 (\"prison\") appears 1 time.\n",
      "Word 107 (\"remain\") appears 2 time.\n",
      "Word 108 (\"restore\") appears 3 time.\n",
      "Word 112 (\"robert\") appears 1 time.\n",
      "Word 119 (\"shock\") appears 1 time.\n",
      "Word 122 (\"side\") appears 1 time.\n",
      "Word 149 (\"accord\") appears 2 time.\n",
      "Word 152 (\"actual\") appears 1 time.\n",
      "Word 163 (\"american\") appears 2 time.\n",
      "Word 174 (\"bill\") appears 1 time.\n",
      "Word 186 (\"change\") appears 1 time.\n",
      "Word 194 (\"consider\") appears 2 time.\n",
      "Word 202 (\"create\") appears 2 time.\n",
      "Word 210 (\"democrat\") appears 1 time.\n",
      "Word 227 (\"education\") appears 2 time.\n",
      "Word 241 (\"face\") appears 1 time.\n",
      "Word 247 (\"finance\") appears 1 time.\n",
      "Word 248 (\"five\") appears 2 time.\n",
      "Word 256 (\"general\") appears 3 time.\n",
      "Word 257 (\"george\") appears 1 time.\n",
      "Word 264 (\"history\") appears 1 time.\n",
      "Word 265 (\"house\") appears 1 time.\n",
      "Word 266 (\"housing\") appears 1 time.\n",
      "Word 271 (\"institute\") appears 1 time.\n",
      "Word 275 (\"issue\") appears 7 time.\n",
      "Word 277 (\"january\") appears 1 time.\n",
      "Word 280 (\"job\") appears 2 time.\n",
      "Word 288 (\"law\") appears 8 time.\n",
      "Word 289 (\"let\") appears 1 time.\n",
      "Word 291 (\"liberty\") appears 1 time.\n",
      "Word 298 (\"mail\") appears 1 time.\n",
      "Word 307 (\"member\") appears 5 time.\n",
      "Word 325 (\"new\") appears 2 time.\n",
      "Word 333 (\"oppose\") appears 1 time.\n",
      "Word 334 (\"opposition\") appears 1 time.\n",
      "Word 336 (\"order\") appears 5 time.\n",
      "Word 337 (\"orientation\") appears 1 time.\n",
      "Word 338 (\"outcome\") appears 1 time.\n",
      "Word 347 (\"petition\") appears 1 time.\n",
      "Word 348 (\"play\") appears 1 time.\n",
      "Word 351 (\"political\") appears 1 time.\n",
      "Word 353 (\"poll\") appears 2 time.\n",
      "Word 367 (\"public\") appears 5 time.\n",
      "Word 371 (\"race\") appears 1 time.\n",
      "Word 375 (\"reform\") appears 6 time.\n",
      "Word 377 (\"republican\") appears 8 time.\n",
      "Word 383 (\"senate\") appears 1 time.\n",
      "Word 386 (\"sentence\") appears 28 time.\n",
      "Word 391 (\"since\") appears 2 time.\n",
      "Word 409 (\"support\") appears 1 time.\n",
      "Word 411 (\"surprise\") appears 1 time.\n",
      "Word 415 (\"term\") appears 1 time.\n",
      "Word 424 (\"union\") appears 1 time.\n",
      "Word 434 (\"william\") appears 1 time.\n",
      "Word 437 (\"write\") appears 1 time.\n",
      "Word 439 (\"address\") appears 8 time.\n",
      "Word 453 (\"box\") appears 1 time.\n",
      "Word 460 (\"chance\") appears 1 time.\n",
      "Word 464 (\"complex\") appears 2 time.\n",
      "Word 470 (\"court\") appears 4 time.\n",
      "Word 471 (\"crime\") appears 2 time.\n",
      "Word 493 (\"follow\") appears 1 time.\n",
      "Word 505 (\"identify\") appears 2 time.\n",
      "Word 517 (\"legislator\") appears 1 time.\n",
      "Word 530 (\"news\") appears 1 time.\n",
      "Word 531 (\"non\") appears 7 time.\n",
      "Word 533 (\"offender\") appears 1 time.\n",
      "Word 544 (\"post\") appears 2 time.\n",
      "Word 552 (\"reentry\") appears 2 time.\n",
      "Word 556 (\"release\") appears 1 time.\n",
      "Word 561 (\"result\") appears 1 time.\n",
      "Word 565 (\"safety\") appears 3 time.\n",
      "Word 570 (\"serve\") appears 8 time.\n",
      "Word 583 (\"staff\") appears 1 time.\n",
      "Word 601 (\"thus\") appears 1 time.\n",
      "Word 604 (\"toward\") appears 1 time.\n",
      "Word 613 (\"violate\") appears 1 time.\n",
      "Word 615 (\"violent\") appears 1 time.\n",
      "Word 626 (\"allow\") appears 3 time.\n",
      "Word 631 (\"around\") appears 1 time.\n",
      "Word 658 (\"co\") appears 1 time.\n",
      "Word 663 (\"conclude\") appears 1 time.\n",
      "Word 671 (\"declaration\") appears 1 time.\n",
      "Word 673 (\"deputy\") appears 1 time.\n",
      "Word 685 (\"extremely\") appears 1 time.\n",
      "Word 711 (\"impact\") appears 2 time.\n",
      "Word 721 (\"knowledge\") appears 1 time.\n",
      "Word 723 (\"leave\") appears 1 time.\n",
      "Word 732 (\"manner\") appears 1 time.\n",
      "Word 747 (\"next\") appears 1 time.\n",
      "Word 765 (\"policy\") appears 1 time.\n",
      "Word 768 (\"program\") appears 2 time.\n",
      "Word 771 (\"provide\") appears 7 time.\n",
      "Word 776 (\"reason\") appears 3 time.\n",
      "Word 801 (\"show\") appears 1 time.\n",
      "Word 806 (\"speaker\") appears 1 time.\n",
      "Word 829 (\"true\") appears 1 time.\n",
      "Word 837 (\"without\") appears 2 time.\n",
      "Word 863 (\"black\") appears 1 time.\n",
      "Word 883 (\"comprehensive\") appears 1 time.\n",
      "Word 886 (\"cost\") appears 1 time.\n",
      "Word 910 (\"exchange\") appears 1 time.\n",
      "Word 912 (\"expensive\") appears 1 time.\n",
      "Word 913 (\"fair\") appears 1 time.\n",
      "Word 915 (\"favor\") appears 1 time.\n",
      "Word 934 (\"heard\") appears 2 time.\n",
      "Word 946 (\"lack\") appears 1 time.\n",
      "Word 966 (\"name\") appears 2 time.\n",
      "Word 973 (\"phone\") appears 1 time.\n",
      "Word 996 (\"reveal\") appears 2 time.\n",
      "Word 1064 (\"age\") appears 3 time.\n",
      "Word 1065 (\"allen\") appears 1 time.\n",
      "Word 1078 (\"case\") appears 4 time.\n",
      "Word 1087 (\"compliance\") appears 1 time.\n",
      "Word 1094 (\"currently\") appears 1 time.\n",
      "Word 1111 (\"felony\") appears 1 time.\n",
      "Word 1124 (\"instead\") appears 1 time.\n",
      "Word 1125 (\"interest\") appears 2 time.\n",
      "Word 1130 (\"judge\") appears 1 time.\n",
      "Word 1133 (\"justice\") appears 6 time.\n",
      "Word 1135 (\"laid\") appears 1 time.\n",
      "Word 1139 (\"least\") appears 2 time.\n",
      "Word 1146 (\"mr\") appears 1 time.\n",
      "Word 1151 (\"old\") appears 3 time.\n",
      "Word 1154 (\"parole\") appears 51 time.\n",
      "Word 1156 (\"pay\") appears 2 time.\n",
      "Word 1175 (\"secretary\") appears 2 time.\n",
      "Word 1190 (\"truth\") appears 3 time.\n",
      "Word 1195 (\"va\") appears 8 time.\n",
      "Word 1196 (\"virginia\") appears 18 time.\n",
      "Word 1208 (\"achieve\") appears 1 time.\n",
      "Word 1229 (\"available\") appears 1 time.\n",
      "Word 1231 (\"base\") appears 4 time.\n",
      "Word 1233 (\"believe\") appears 1 time.\n",
      "Word 1237 (\"body\") appears 1 time.\n",
      "Word 1255 (\"code\") appears 7 time.\n",
      "Word 1280 (\"decision\") appears 2 time.\n",
      "Word 1285 (\"determine\") appears 1 time.\n",
      "Word 1290 (\"disadvantage\") appears 2 time.\n",
      "Word 1297 (\"divide\") appears 1 time.\n",
      "Word 1301 (\"due\") appears 3 time.\n",
      "Word 1303 (\"economic\") appears 2 time.\n",
      "Word 1314 (\"ensure\") appears 1 time.\n",
      "Word 1319 (\"establish\") appears 3 time.\n",
      "Word 1334 (\"fail\") appears 1 time.\n",
      "Word 1336 (\"faith\") appears 1 time.\n",
      "Word 1338 (\"february\") appears 1 time.\n",
      "Word 1341 (\"final\") appears 11 time.\n",
      "Word 1345 (\"former\") appears 3 time.\n",
      "Word 1352 (\"goal\") appears 2 time.\n",
      "Word 1354 (\"government\") appears 1 time.\n",
      "Word 1355 (\"guideline\") appears 6 time.\n",
      "Word 1365 (\"historical\") appears 1 time.\n",
      "Word 1366 (\"idea\") appears 3 time.\n",
      "Word 1378 (\"ineffective\") appears 1 time.\n",
      "Word 1380 (\"information\") appears 2 time.\n",
      "Word 1399 (\"legal\") appears 1 time.\n",
      "Word 1406 (\"limited\") appears 3 time.\n",
      "Word 1432 (\"objective\") appears 2 time.\n",
      "Word 1448 (\"perform\") appears 1 time.\n",
      "Word 1465 (\"probation\") appears 1 time.\n",
      "Word 1481 (\"rate\") appears 1 time.\n",
      "Word 1486 (\"recidivism\") appears 1 time.\n",
      "Word 1488 (\"reduction\") appears 1 time.\n",
      "Word 1493 (\"rehabilitation\") appears 1 time.\n",
      "Word 1499 (\"represent\") appears 1 time.\n",
      "Word 1506 (\"role\") appears 1 time.\n",
      "Word 1517 (\"social\") appears 1 time.\n",
      "Word 1536 (\"supreme\") appears 1 time.\n",
      "Word 1544 (\"truly\") appears 1 time.\n",
      "Word 1561 (\"voluntary\") appears 1 time.\n",
      "Word 1584 (\"ass\") appears 1 time.\n",
      "Word 1687 (\"dick\") appears 1 time.\n",
      "Word 1696 (\"dollar\") appears 1 time.\n",
      "Word 1721 (\"examine\") appears 2 time.\n",
      "Word 1792 (\"happen\") appears 2 time.\n",
      "Word 1797 (\"hearing\") appears 1 time.\n",
      "Word 1846 (\"main\") appears 2 time.\n",
      "Word 1849 (\"mechanism\") appears 1 time.\n",
      "Word 1887 (\"opposite\") appears 1 time.\n",
      "Word 1888 (\"pa\") appears 1 time.\n",
      "Word 1911 (\"plenty\") appears 1 time.\n",
      "Word 1912 (\"po\") appears 1 time.\n",
      "Word 1922 (\"progress\") appears 1 time.\n",
      "Word 1943 (\"refer\") appears 1 time.\n",
      "Word 1944 (\"reflect\") appears 1 time.\n",
      "Word 1972 (\"school\") appears 1 time.\n",
      "Word 1975 (\"second\") appears 2 time.\n",
      "Word 1994 (\"sign\") appears 1 time.\n",
      "Word 2014 (\"spent\") appears 1 time.\n",
      "Word 2061 (\"ten\") appears 1 time.\n",
      "Word 2065 (\"though\") appears 1 time.\n",
      "Word 2118 (\"white\") appears 1 time.\n",
      "Word 2165 (\"disappointed\") appears 1 time.\n",
      "Word 2206 (\"office\") appears 1 time.\n",
      "Word 2261 (\"whether\") appears 3 time.\n",
      "Word 2265 (\"access\") appears 1 time.\n",
      "Word 2270 (\"addition\") appears 1 time.\n",
      "Word 2272 (\"advise\") appears 1 time.\n",
      "Word 2273 (\"affair\") appears 1 time.\n",
      "Word 2289 (\"association\") appears 1 time.\n",
      "Word 2291 (\"avail\") appears 1 time.\n",
      "Word 2293 (\"bell\") appears 1 time.\n",
      "Word 2300 (\"center\") appears 1 time.\n",
      "Word 2308 (\"committee\") appears 1 time.\n",
      "Word 2310 (\"completion\") appears 1 time.\n",
      "Word 2312 (\"concern\") appears 1 time.\n",
      "Word 2314 (\"conduct\") appears 2 time.\n",
      "Word 2324 (\"correction\") appears 1 time.\n",
      "Word 2328 (\"december\") appears 1 time.\n",
      "Word 2331 (\"detailed\") appears 2 time.\n",
      "Word 2337 (\"director\") appears 3 time.\n",
      "Word 2356 (\"establishment\") appears 1 time.\n",
      "Word 2359 (\"evidence\") appears 2 time.\n",
      "Word 2363 (\"explanation\") appears 1 time.\n",
      "Word 2373 (\"fund\") appears 1 time.\n",
      "Word 2388 (\"implementation\") appears 1 time.\n",
      "Word 2401 (\"intend\") appears 1 time.\n",
      "Word 2413 (\"legislative\") appears 1 time.\n",
      "Word 2436 (\"october\") appears 1 time.\n",
      "Word 2446 (\"period\") appears 2 time.\n",
      "Word 2454 (\"present\") appears 1 time.\n",
      "Word 2455 (\"presentation\") appears 1 time.\n",
      "Word 2470 (\"religion\") appears 1 time.\n",
      "Word 2471 (\"report\") appears 7 time.\n",
      "Word 2474 (\"research\") appears 2 time.\n",
      "Word 2486 (\"sexual\") appears 1 time.\n",
      "Word 2507 (\"topic\") appears 2 time.\n",
      "Word 2513 (\"university\") appears 1 time.\n",
      "Word 2516 (\"vocal\") appears 1 time.\n",
      "Word 2527 (\"challenge\") appears 1 time.\n",
      "Word 2533 (\"contrary\") appears 1 time.\n",
      "Word 2559 (\"meaning\") appears 1 time.\n",
      "Word 2567 (\"overturn\") appears 1 time.\n",
      "Word 2593 (\"ﬁrst\") appears 2 time.\n",
      "Word 2594 (\"ﬁt\") appears 1 time.\n",
      "Word 2603 (\"civil\") appears 1 time.\n",
      "Word 2622 (\"factor\") appears 1 time.\n",
      "Word 2625 (\"impetus\") appears 1 time.\n",
      "Word 2629 (\"jail\") appears 1 time.\n",
      "Word 2648 (\"recent\") appears 1 time.\n",
      "Word 2649 (\"related\") appears 1 time.\n",
      "Word 2668 (\"ﬁve\") appears 1 time.\n",
      "Word 2679 (\"contempt\") appears 1 time.\n",
      "Word 2681 (\"decade\") appears 2 time.\n",
      "Word 2687 (\"impose\") appears 4 time.\n",
      "Word 2689 (\"include\") appears 5 time.\n",
      "Word 2697 (\"render\") appears 1 time.\n",
      "Word 2804 (\"abolition\") appears 6 time.\n",
      "Word 2810 (\"blog\") appears 1 time.\n",
      "Word 2814 (\"com\") appears 3 time.\n",
      "Word 2815 (\"comment\") appears 2 time.\n",
      "Word 2817 (\"conscious\") appears 4 time.\n",
      "Word 2819 (\"david\") appears 1 time.\n",
      "Word 2822 (\"doc\") appears 1 time.\n",
      "Word 2827 (\"evolution\") appears 1 time.\n",
      "Word 2830 (\"governor\") appears 8 time.\n",
      "Word 2833 (\"http\") appears 3 time.\n",
      "Word 2845 (\"portion\") appears 1 time.\n",
      "Word 2855 (\"tag\") appears 1 time.\n",
      "Word 2870 (\"writing\") appears 1 time.\n",
      "Word 2871 (\"aid\") appears 1 time.\n",
      "Word 2939 (\"abolish\") appears 3 time.\n",
      "Word 2958 (\"draconian\") appears 2 time.\n",
      "Word 2961 (\"eligibility\") appears 1 time.\n",
      "Word 2993 (\"therefore\") appears 1 time.\n",
      "Word 3034 (\"email\") appears 2 time.\n",
      "Word 3054 (\"adequate\") appears 1 time.\n",
      "Word 3055 (\"alabama\") appears 1 time.\n",
      "Word 3056 (\"alliance\") appears 1 time.\n",
      "Word 3057 (\"amend\") appears 3 time.\n",
      "Word 3058 (\"annual\") appears 1 time.\n",
      "Word 3059 (\"app\") appears 1 time.\n",
      "Word 3060 (\"appellate\") appears 1 time.\n",
      "Word 3061 (\"assembly\") appears 2 time.\n",
      "Word 3062 (\"attorney\") appears 1 time.\n",
      "Word 3063 (\"background\") appears 1 time.\n",
      "Word 3064 (\"billion\") appears 1 time.\n",
      "Word 3065 (\"bully\") appears 2 time.\n",
      "Word 3066 (\"chair\") appears 1 time.\n",
      "Word 3067 (\"commission\") appears 41 time.\n",
      "Word 3068 (\"commonwealth\") appears 3 time.\n",
      "Word 3069 (\"consideration\") appears 2 time.\n",
      "Word 3070 (\"conﬁned\") appears 1 time.\n",
      "Word 3071 (\"credit\") appears 1 time.\n",
      "Word 3072 (\"data\") appears 5 time.\n",
      "Word 3073 (\"deceive\") appears 1 time.\n",
      "Word 3074 (\"deeply\") appears 1 time.\n",
      "Word 3075 (\"departure\") appears 2 time.\n",
      "Word 3076 (\"difﬁcult\") appears 2 time.\n",
      "Word 3077 (\"discretionary\") appears 5 time.\n",
      "Word 3078 (\"discussion\") appears 1 time.\n",
      "Word 3079 (\"divert\") appears 1 time.\n",
      "Word 3080 (\"douglas\") appears 1 time.\n",
      "Word 3081 (\"download\") appears 1 time.\n",
      "Word 3082 (\"duty\") appears 1 time.\n",
      "Word 3083 (\"economy\") appears 1 time.\n",
      "Word 3084 (\"enhancement\") appears 1 time.\n",
      "Word 3085 (\"eric\") appears 1 time.\n",
      "Word 3086 (\"exceed\") appears 1 time.\n",
      "Word 3087 (\"executive\") appears 7 time.\n",
      "Word 3088 (\"florida\") appears 1 time.\n",
      "Word 3089 (\"grassroots\") appears 1 time.\n",
      "Word 3090 (\"historically\") appears 1 time.\n",
      "Word 3091 (\"ing\") appears 1 time.\n",
      "Word 3092 (\"jerry\") appears 1 time.\n",
      "Word 3093 (\"june\") appears 1 time.\n",
      "Word 3094 (\"juvenile\") appears 2 time.\n",
      "Word 3095 (\"mar\") appears 1 time.\n",
      "Word 3096 (\"meant\") appears 1 time.\n",
      "Word 3097 (\"meeting\") appears 7 time.\n",
      "Word 3098 (\"miller\") appears 1 time.\n",
      "Word 3099 (\"minimum\") appears 2 time.\n",
      "Word 3100 (\"monday\") appears 1 time.\n",
      "Word 3101 (\"ole\") appears 1 time.\n",
      "Word 3102 (\"opponent\") appears 1 time.\n",
      "Word 3103 (\"optimistic\") appears 1 time.\n",
      "Word 3104 (\"organization\") appears 1 time.\n",
      "Word 3105 (\"percent\") appears 8 time.\n",
      "Word 3106 (\"percentage\") appears 1 time.\n",
      "Word 3107 (\"pertain\") appears 1 time.\n",
      "Word 3108 (\"placement\") appears 1 time.\n",
      "Word 3109 (\"propose\") appears 2 time.\n",
      "Word 3110 (\"qualify\") appears 1 time.\n",
      "Word 3111 (\"recommend\") appears 1 time.\n",
      "Word 3112 (\"recommendation\") appears 7 time.\n",
      "Word 3113 (\"repeal\") appears 1 time.\n",
      "Word 3114 (\"resource\") appears 1 time.\n",
      "Word 3115 (\"review\") appears 19 time.\n",
      "Word 3116 (\"reﬂect\") appears 1 time.\n",
      "Word 3117 (\"section\") appears 3 time.\n",
      "Word 3118 (\"specifically\") appears 1 time.\n",
      "Word 3119 (\"speciﬁcally\") appears 4 time.\n",
      "Word 3120 (\"spectrum\") appears 1 time.\n",
      "Word 3121 (\"statement\") appears 2 time.\n",
      "Word 3122 (\"status\") appears 1 time.\n",
      "Word 3123 (\"summary\") appears 1 time.\n",
      "Word 3124 (\"tackle\") appears 1 time.\n",
      "Word 3125 (\"tactic\") appears 1 time.\n",
      "Word 3126 (\"thru\") appears 1 time.\n",
      "Word 3127 (\"timely\") appears 1 time.\n",
      "Word 3128 (\"unclear\") appears 1 time.\n",
      "Word 3129 (\"unfair\") appears 1 time.\n",
      "Word 3130 (\"verse\") appears 2 time.\n",
      "Word 3131 (\"via\") appears 1 time.\n",
      "Word 3132 (\"ﬁnal\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview Bag Of Words for our sample preprocessed document.\n",
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:49:54,378 : INFO : collecting document frequencies\n",
      "2019-10-21 20:49:54,383 : INFO : PROGRESS: processing document #0\n",
      "2019-10-21 20:49:54,546 : INFO : calculating IDF weights for 1573 documents and 6059 features (508322 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "# Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, \n",
    "# then apply transformation to the entire corpus and call it ‘corpus_tfidf’. \n",
    "# Finally we preview TF-IDF scores for our first document.\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.09379295575241038),\n",
      " (1, 0.06161466199713889),\n",
      " (2, 0.060857478466811464),\n",
      " (3, 0.03628722174097065),\n",
      " (4, 0.0349662870830923),\n",
      " (5, 0.06384579965159101),\n",
      " (6, 0.06989847196027979),\n",
      " (7, 0.09899251907681905),\n",
      " (8, 0.02314700881373919),\n",
      " (9, 0.020260837005040288),\n",
      " (10, 0.05444269060802119),\n",
      " (11, 0.11959175426347554),\n",
      " (12, 0.08910184732085523),\n",
      " (13, 0.06792813246358596),\n",
      " (14, 0.09570511174292377),\n",
      " (15, 0.07862612145798246),\n",
      " (16, 0.12581742367884752),\n",
      " (17, 0.08990052541608053),\n",
      " (18, 0.08056189427860502),\n",
      " (19, 0.057467045277238185),\n",
      " (20, 0.1337442716118708),\n",
      " (21, 0.04969328230319575),\n",
      " (22, 0.04473755288291632),\n",
      " (23, 0.04226574003633351),\n",
      " (24, 0.03828457317206136),\n",
      " (25, 0.07549992208583974),\n",
      " (26, 0.04791409055319777),\n",
      " (27, 0.0592578292891936),\n",
      " (28, 0.07862612145798246),\n",
      " (29, 0.042344844364168834),\n",
      " (30, 0.08472113208481645),\n",
      " (31, 0.028783224145832013),\n",
      " (32, 0.11446455419041415),\n",
      " (33, 0.06485688113206525),\n",
      " (34, 0.09072170442088967),\n",
      " (35, 0.06384579965159101),\n",
      " (36, 0.08212831936785163),\n",
      " (37, 0.08405098930867502),\n",
      " (38, 0.04380670689357871),\n",
      " (39, 0.13197706831646025),\n",
      " (40, 0.08032465587141692),\n",
      " (41, 0.04156329816888919),\n",
      " (42, 0.07574800806768528),\n",
      " (43, 0.10607520572542427),\n",
      " (44, 0.12873484918153388),\n",
      " (45, 0.06451594242773241),\n",
      " (46, 0.061309424371265575),\n",
      " (47, 0.045345770144285105),\n",
      " (48, 0.0601194662143329),\n",
      " (49, 0.08990052541608053),\n",
      " (50, 0.060857478466811464),\n",
      " (51, 0.022498304948760848),\n",
      " (52, 0.04202970694980895),\n",
      " (53, 0.051156309688124124),\n",
      " (54, 0.025896608764306585),\n",
      " (55, 0.1337442716118708),\n",
      " (56, 0.03271832332645063),\n",
      " (57, 0.1063146758892078),\n",
      " (58, 0.09333389229770274),\n",
      " (59, 0.11959175426347554),\n",
      " (60, 0.0213195758863417),\n",
      " (61, 0.10171980368543364),\n",
      " (62, 0.06555095980136913),\n",
      " (63, 0.08339590720293762),\n",
      " (64, 0.11959175426347554),\n",
      " (65, 0.0221396547070848),\n",
      " (66, 0.1056122340217635),\n",
      " (67, 0.03577692552480757),\n",
      " (68, 0.027517753149417897),\n",
      " (69, 0.1332481688549516),\n",
      " (70, 0.11959175426347554),\n",
      " (71, 0.0347823752621922),\n",
      " (72, 0.08190561236371802),\n",
      " (73, 0.06680684318862935),\n",
      " (74, 0.10703446374164387),\n",
      " (75, 0.07136149934520815),\n",
      " (76, 0.07754637109553213),\n",
      " (77, 0.029641379205747016),\n",
      " (78, 0.06799713157193485),\n",
      " (79, 0.10233350902493565),\n",
      " (80, 0.10852965952444986),\n",
      " (81, 0.05480480615812268),\n",
      " (82, 0.13197706831646025),\n",
      " (83, 0.059385308752513015),\n",
      " (84, 0.08061762242705282),\n",
      " (85, 0.052583898343017015),\n",
      " (86, 0.056172430941176583),\n",
      " (87, 0.030790808388205866),\n",
      " (88, 0.02571694762905476),\n",
      " (89, 0.08682927408455983),\n",
      " (90, 0.1063146758892078),\n",
      " (91, 0.02419042467835614),\n",
      " (92, 0.10769797043832412),\n",
      " (93, 0.050000665751655966),\n",
      " (94, 0.10492632174190047),\n",
      " (95, 0.06792813246358596),\n",
      " (96, 0.10052984552850096),\n",
      " (97, 0.06368064381648664),\n",
      " (98, 0.0947333575283992),\n",
      " (99, 0.05292351298353467),\n",
      " (100, 0.19141022348584755),\n",
      " (101, 0.03603095699671393),\n",
      " (102, 0.08990052541608053),\n",
      " (103, 0.02770890089611374),\n",
      " (104, 0.04339322582019763),\n",
      " (105, 0.08472113208481645),\n",
      " (106, 0.11177187865937621),\n",
      " (107, 0.04422613753875684),\n",
      " (108, 0.09425936453333011),\n",
      " (109, 0.11010571507316455),\n",
      " (110, 0.06146164365668862),\n",
      " (111, 0.1063146758892078),\n",
      " (112, 0.07918137494930748),\n",
      " (113, 0.09671038125509675),\n",
      " (114, 0.0696953357589965),\n",
      " (115, 0.05073075575189974),\n",
      " (116, 0.03100475376635197),\n",
      " (117, 0.09798433620666737),\n",
      " (118, 0.08032465587141692),\n",
      " (119, 0.07010303369042457),\n",
      " (120, 0.07245338455564551),\n",
      " (121, 0.06870036111781823),\n",
      " (122, 0.04164051656349717),\n",
      " (123, 0.02381047818091224),\n",
      " (124, 0.12769159930318202),\n",
      " (125, 0.031328645175467726),\n",
      " (126, 0.05787605748964338),\n",
      " (127, 0.13562556478645882),\n",
      " (128, 0.0675495566631032),\n",
      " (129, 0.1063146758892078),\n",
      " (130, 0.06792813246358596),\n",
      " (131, 0.08032465587141692),\n",
      " (132, 0.11010571507316455),\n",
      " (133, 0.1337442716118708),\n",
      " (134, 0.10296041333542093),\n",
      " (135, 0.12192499334251769),\n",
      " (136, 0.1303109047302486),\n",
      " (137, 0.12316560299250495),\n",
      " (138, 0.12316560299250495),\n",
      " (139, 0.06287482737138009),\n",
      " (140, 0.10052984552850096),\n",
      " (141, 0.04422613753875684),\n",
      " (142, 0.12723965339872792),\n",
      " (143, 0.09333389229770274),\n",
      " (144, 0.06451594242773241),\n",
      " (145, 0.031058486365748022),\n",
      " (146, 0.036806610571500406),\n",
      " (147, 0.05589922363469199)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:49:54,625 : INFO : using symmetric alpha at 0.1\n",
      "2019-10-21 20:49:54,634 : INFO : using symmetric eta at 0.1\n",
      "2019-10-21 20:49:54,636 : INFO : using serial LDA version on this node\n",
      "2019-10-21 20:49:54,646 : INFO : running online LDA training, 10 topics, 2 passes over the supplied corpus of 1573 documents, updating every 4000 documents, evaluating every ~1573 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-21 20:49:54,651 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2019-10-21 20:49:54,656 : INFO : training LDA model using 2 processes\n",
      "2019-10-21 20:49:54,714 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #1573/1573, outstanding queue size 1\n",
      "2019-10-21 20:49:59,124 : INFO : topic #8 (0.100): 0.004*\"inmate\" + 0.004*\"law\" + 0.003*\"cell\" + 0.003*\"court\" + 0.003*\"sentence\" + 0.003*\"become\" + 0.003*\"officer\" + 0.003*\"write\" + 0.003*\"love\" + 0.002*\"new\"\n",
      "2019-10-21 20:49:59,125 : INFO : topic #3 (0.100): 0.004*\"world\" + 0.004*\"society\" + 0.004*\"family\" + 0.003*\"write\" + 0.003*\"cell\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"become\" + 0.003*\"inmate\" + 0.003*\"think\"\n",
      "2019-10-21 20:49:59,127 : INFO : topic #0 (0.100): 0.007*\"inmate\" + 0.004*\"program\" + 0.003*\"change\" + 0.003*\"crime\" + 0.003*\"world\" + 0.003*\"society\" + 0.003*\"help\" + 0.003*\"sentence\" + 0.003*\"good\" + 0.003*\"court\"\n",
      "2019-10-21 20:49:59,131 : INFO : topic #1 (0.100): 0.007*\"inmate\" + 0.004*\"black\" + 0.004*\"sentence\" + 0.003*\"men\" + 0.003*\"new\" + 0.003*\"society\" + 0.003*\"cell\" + 0.003*\"law\" + 0.003*\"change\" + 0.003*\"criminal\"\n",
      "2019-10-21 20:49:59,133 : INFO : topic #4 (0.100): 0.004*\"sentence\" + 0.004*\"law\" + 0.003*\"inmate\" + 0.003*\"court\" + 0.003*\"criminal\" + 0.003*\"person\" + 0.003*\"think\" + 0.003*\"new\" + 0.003*\"man\" + 0.003*\"cell\"\n",
      "2019-10-21 20:49:59,134 : INFO : topic diff=1.038608, rho=1.000000\n",
      "2019-10-21 20:50:04,082 : INFO : -8.067 per-word bound, 268.2 perplexity estimate based on a held-out corpus of 1573 documents with 892355 words\n",
      "2019-10-21 20:50:04,084 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #1573/1573, outstanding queue size 1\n",
      "2019-10-21 20:50:07,414 : INFO : topic #8 (0.100): 0.004*\"court\" + 0.004*\"cell\" + 0.004*\"law\" + 0.003*\"inmate\" + 0.003*\"officer\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"sentence\" + 0.003*\"become\" + 0.003*\"told\"\n",
      "2019-10-21 20:50:07,415 : INFO : topic #3 (0.100): 0.004*\"world\" + 0.004*\"family\" + 0.004*\"cell\" + 0.004*\"society\" + 0.004*\"become\" + 0.004*\"write\" + 0.004*\"feel\" + 0.003*\"think\" + 0.003*\"mind\" + 0.003*\"thought\"\n",
      "2019-10-21 20:50:07,417 : INFO : topic #1 (0.100): 0.008*\"inmate\" + 0.007*\"black\" + 0.004*\"men\" + 0.003*\"new\" + 0.003*\"society\" + 0.003*\"world\" + 0.003*\"cell\" + 0.003*\"sentence\" + 0.003*\"seem\" + 0.003*\"law\"\n",
      "2019-10-21 20:50:07,418 : INFO : topic #6 (0.100): 0.007*\"inmate\" + 0.004*\"society\" + 0.003*\"crime\" + 0.003*\"family\" + 0.003*\"law\" + 0.003*\"offender\" + 0.003*\"think\" + 0.003*\"release\" + 0.003*\"man\" + 0.003*\"new\"\n",
      "2019-10-21 20:50:07,421 : INFO : topic #9 (0.100): 0.006*\"crime\" + 0.004*\"write\" + 0.004*\"sentence\" + 0.004*\"parole\" + 0.004*\"society\" + 0.004*\"change\" + 0.003*\"world\" + 0.003*\"become\" + 0.003*\"criminal\" + 0.003*\"death\"\n",
      "2019-10-21 20:50:07,422 : INFO : topic diff=0.214049, rho=0.599060\n",
      "2019-10-21 20:50:11,465 : INFO : -7.978 per-word bound, 252.1 perplexity estimate based on a held-out corpus of 1573 documents with 892355 words\n"
     ]
    }
   ],
   "source": [
    "# Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:50:11,524 : INFO : topic #0 (0.100): 0.009*\"inmate\" + 0.005*\"program\" + 0.003*\"world\" + 0.003*\"god\" + 0.003*\"change\" + 0.003*\"society\" + 0.003*\"prison\" + 0.003*\"help\" + 0.003*\"good\" + 0.003*\"crime\"\n",
      "2019-10-21 20:50:11,528 : INFO : topic #1 (0.100): 0.008*\"inmate\" + 0.007*\"black\" + 0.004*\"men\" + 0.003*\"new\" + 0.003*\"society\" + 0.003*\"world\" + 0.003*\"cell\" + 0.003*\"sentence\" + 0.003*\"seem\" + 0.003*\"law\"\n",
      "2019-10-21 20:50:11,531 : INFO : topic #2 (0.100): 0.009*\"inmate\" + 0.004*\"write\" + 0.004*\"officer\" + 0.004*\"law\" + 0.003*\"staff\" + 0.003*\"program\" + 0.003*\"become\" + 0.003*\"think\" + 0.003*\"good\" + 0.003*\"court\"\n",
      "2019-10-21 20:50:11,533 : INFO : topic #3 (0.100): 0.004*\"world\" + 0.004*\"family\" + 0.004*\"cell\" + 0.004*\"society\" + 0.004*\"become\" + 0.004*\"write\" + 0.004*\"feel\" + 0.003*\"think\" + 0.003*\"mind\" + 0.003*\"thought\"\n",
      "2019-10-21 20:50:11,536 : INFO : topic #4 (0.100): 0.006*\"law\" + 0.005*\"sentence\" + 0.005*\"court\" + 0.004*\"criminal\" + 0.003*\"case\" + 0.003*\"crime\" + 0.003*\"person\" + 0.003*\"new\" + 0.003*\"government\" + 0.003*\"release\"\n",
      "2019-10-21 20:50:11,539 : INFO : topic #5 (0.100): 0.004*\"black\" + 0.003*\"become\" + 0.003*\"must\" + 0.003*\"may\" + 0.003*\"man\" + 0.003*\"cell\" + 0.002*\"law\" + 0.002*\"mind\" + 0.002*\"inmate\" + 0.002*\"society\"\n",
      "2019-10-21 20:50:11,542 : INFO : topic #6 (0.100): 0.007*\"inmate\" + 0.004*\"society\" + 0.003*\"crime\" + 0.003*\"family\" + 0.003*\"law\" + 0.003*\"offender\" + 0.003*\"think\" + 0.003*\"release\" + 0.003*\"man\" + 0.003*\"new\"\n",
      "2019-10-21 20:50:11,544 : INFO : topic #7 (0.100): 0.008*\"inmate\" + 0.003*\"man\" + 0.003*\"good\" + 0.003*\"cell\" + 0.003*\"think\" + 0.003*\"officer\" + 0.003*\"law\" + 0.003*\"family\" + 0.003*\"death\" + 0.003*\"help\"\n",
      "2019-10-21 20:50:11,547 : INFO : topic #8 (0.100): 0.004*\"court\" + 0.004*\"cell\" + 0.004*\"law\" + 0.003*\"inmate\" + 0.003*\"officer\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"sentence\" + 0.003*\"become\" + 0.003*\"told\"\n",
      "2019-10-21 20:50:11,549 : INFO : topic #9 (0.100): 0.006*\"crime\" + 0.004*\"write\" + 0.004*\"sentence\" + 0.004*\"parole\" + 0.004*\"society\" + 0.004*\"change\" + 0.003*\"world\" + 0.003*\"become\" + 0.003*\"criminal\" + 0.003*\"death\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.009*\"inmate\" + 0.005*\"program\" + 0.003*\"world\" + 0.003*\"god\" + 0.003*\"change\" + 0.003*\"society\" + 0.003*\"prison\" + 0.003*\"help\" + 0.003*\"good\" + 0.003*\"crime\"\n",
      "Topic: 1 \n",
      "Words: 0.008*\"inmate\" + 0.007*\"black\" + 0.004*\"men\" + 0.003*\"new\" + 0.003*\"society\" + 0.003*\"world\" + 0.003*\"cell\" + 0.003*\"sentence\" + 0.003*\"seem\" + 0.003*\"law\"\n",
      "Topic: 2 \n",
      "Words: 0.009*\"inmate\" + 0.004*\"write\" + 0.004*\"officer\" + 0.004*\"law\" + 0.003*\"staff\" + 0.003*\"program\" + 0.003*\"become\" + 0.003*\"think\" + 0.003*\"good\" + 0.003*\"court\"\n",
      "Topic: 3 \n",
      "Words: 0.004*\"world\" + 0.004*\"family\" + 0.004*\"cell\" + 0.004*\"society\" + 0.004*\"become\" + 0.004*\"write\" + 0.004*\"feel\" + 0.003*\"think\" + 0.003*\"mind\" + 0.003*\"thought\"\n",
      "Topic: 4 \n",
      "Words: 0.006*\"law\" + 0.005*\"sentence\" + 0.005*\"court\" + 0.004*\"criminal\" + 0.003*\"case\" + 0.003*\"crime\" + 0.003*\"person\" + 0.003*\"new\" + 0.003*\"government\" + 0.003*\"release\"\n",
      "Topic: 5 \n",
      "Words: 0.004*\"black\" + 0.003*\"become\" + 0.003*\"must\" + 0.003*\"may\" + 0.003*\"man\" + 0.003*\"cell\" + 0.002*\"law\" + 0.002*\"mind\" + 0.002*\"inmate\" + 0.002*\"society\"\n",
      "Topic: 6 \n",
      "Words: 0.007*\"inmate\" + 0.004*\"society\" + 0.003*\"crime\" + 0.003*\"family\" + 0.003*\"law\" + 0.003*\"offender\" + 0.003*\"think\" + 0.003*\"release\" + 0.003*\"man\" + 0.003*\"new\"\n",
      "Topic: 7 \n",
      "Words: 0.008*\"inmate\" + 0.003*\"man\" + 0.003*\"good\" + 0.003*\"cell\" + 0.003*\"think\" + 0.003*\"officer\" + 0.003*\"law\" + 0.003*\"family\" + 0.003*\"death\" + 0.003*\"help\"\n",
      "Topic: 8 \n",
      "Words: 0.004*\"court\" + 0.004*\"cell\" + 0.004*\"law\" + 0.003*\"inmate\" + 0.003*\"officer\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"sentence\" + 0.003*\"become\" + 0.003*\"told\"\n",
      "Topic: 9 \n",
      "Words: 0.006*\"crime\" + 0.004*\"write\" + 0.004*\"sentence\" + 0.004*\"parole\" + 0.004*\"society\" + 0.004*\"change\" + 0.003*\"world\" + 0.003*\"become\" + 0.003*\"criminal\" + 0.003*\"death\"\n"
     ]
    }
   ],
   "source": [
    "# For each topic, we will explore the words occuring in that topic and its relative weight.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:50:11,564 : INFO : using symmetric alpha at 0.1\n",
      "2019-10-21 20:50:11,567 : INFO : using symmetric eta at 0.1\n",
      "2019-10-21 20:50:11,571 : INFO : using serial LDA version on this node\n",
      "2019-10-21 20:50:11,592 : INFO : running online LDA training, 10 topics, 2 passes over the supplied corpus of 1573 documents, updating every 8000 documents, evaluating every ~1573 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-21 20:50:11,597 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2019-10-21 20:50:11,604 : INFO : training LDA model using 4 processes\n",
      "2019-10-21 20:50:14,476 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #1573/1573, outstanding queue size 1\n",
      "2019-10-21 20:50:19,478 : INFO : topic #7 (0.100): 0.002*\"inmate\" + 0.001*\"death\" + 0.001*\"parole\" + 0.001*\"officer\" + 0.001*\"cell\" + 0.001*\"love\" + 0.001*\"prison\" + 0.001*\"sentence\" + 0.001*\"guard\" + 0.001*\"woman\"\n",
      "2019-10-21 20:50:19,480 : INFO : topic #5 (0.100): 0.001*\"inmate\" + 0.001*\"cell\" + 0.001*\"society\" + 0.001*\"program\" + 0.001*\"sentence\" + 0.001*\"release\" + 0.001*\"offender\" + 0.001*\"guard\" + 0.001*\"staff\" + 0.001*\"officer\"\n",
      "2019-10-21 20:50:19,482 : INFO : topic #6 (0.100): 0.002*\"inmate\" + 0.001*\"officer\" + 0.001*\"cell\" + 0.001*\"gang\" + 0.001*\"death\" + 0.001*\"parole\" + 0.001*\"program\" + 0.001*\"sentence\" + 0.001*\"god\" + 0.001*\"crime\"\n",
      "2019-10-21 20:50:19,484 : INFO : topic #8 (0.100): 0.002*\"inmate\" + 0.001*\"cell\" + 0.001*\"program\" + 0.001*\"guard\" + 0.001*\"officer\" + 0.001*\"family\" + 0.001*\"offender\" + 0.001*\"parole\" + 0.001*\"texas\" + 0.001*\"person\"\n",
      "2019-10-21 20:50:19,487 : INFO : topic #0 (0.100): 0.002*\"inmate\" + 0.001*\"law\" + 0.001*\"crime\" + 0.001*\"american\" + 0.001*\"police\" + 0.001*\"criminal\" + 0.001*\"cell\" + 0.001*\"god\" + 0.001*\"world\" + 0.001*\"society\"\n",
      "2019-10-21 20:50:19,489 : INFO : topic diff=2.760819, rho=1.000000\n",
      "2019-10-21 20:50:23,134 : INFO : -11.311 per-word bound, 2541.0 perplexity estimate based on a held-out corpus of 1573 documents with 19525 words\n",
      "2019-10-21 20:50:25,867 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #1573/1573, outstanding queue size 1\n",
      "2019-10-21 20:50:30,426 : INFO : topic #7 (0.100): 0.002*\"inmate\" + 0.002*\"death\" + 0.001*\"parole\" + 0.001*\"cell\" + 0.001*\"love\" + 0.001*\"officer\" + 0.001*\"military\" + 0.001*\"guard\" + 0.001*\"woman\" + 0.001*\"men\"\n",
      "2019-10-21 20:50:30,427 : INFO : topic #0 (0.100): 0.001*\"law\" + 0.001*\"american\" + 0.001*\"police\" + 0.001*\"inmate\" + 0.001*\"crime\" + 0.001*\"criminal\" + 0.001*\"political\" + 0.001*\"carolina\" + 0.001*\"consciousness\" + 0.001*\"african\"\n",
      "2019-10-21 20:50:30,429 : INFO : topic #5 (0.100): 0.001*\"cell\" + 0.001*\"society\" + 0.001*\"release\" + 0.001*\"inmate\" + 0.001*\"love\" + 0.001*\"program\" + 0.001*\"sentence\" + 0.001*\"offender\" + 0.001*\"child\" + 0.001*\"officer\"\n",
      "2019-10-21 20:50:30,432 : INFO : topic #1 (0.100): 0.001*\"inmate\" + 0.001*\"pennsylvania\" + 0.001*\"god\" + 0.001*\"violent\" + 0.001*\"book\" + 0.001*\"te\" + 0.001*\"guard\" + 0.001*\"remember\" + 0.001*\"page\" + 0.001*\"john\"\n",
      "2019-10-21 20:50:30,433 : INFO : topic #3 (0.100): 0.001*\"inmate\" + 0.001*\"woman\" + 0.001*\"california\" + 0.001*\"program\" + 0.001*\"write\" + 0.001*\"god\" + 0.001*\"parole\" + 0.001*\"help\" + 0.001*\"guy\" + 0.001*\"cell\"\n",
      "2019-10-21 20:50:30,435 : INFO : topic diff=0.742897, rho=0.599060\n",
      "2019-10-21 20:50:33,755 : INFO : -10.860 per-word bound, 1858.4 perplexity estimate based on a held-out corpus of 1573 documents with 19525 words\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:50:33,952 : INFO : topic #0 (0.100): 0.001*\"law\" + 0.001*\"american\" + 0.001*\"police\" + 0.001*\"inmate\" + 0.001*\"crime\" + 0.001*\"criminal\" + 0.001*\"political\" + 0.001*\"carolina\" + 0.001*\"consciousness\" + 0.001*\"african\"\n",
      "2019-10-21 20:50:33,958 : INFO : topic #1 (0.100): 0.001*\"inmate\" + 0.001*\"pennsylvania\" + 0.001*\"god\" + 0.001*\"violent\" + 0.001*\"book\" + 0.001*\"te\" + 0.001*\"guard\" + 0.001*\"remember\" + 0.001*\"page\" + 0.001*\"john\"\n",
      "2019-10-21 20:50:33,961 : INFO : topic #2 (0.100): 0.002*\"offender\" + 0.002*\"inmate\" + 0.001*\"sentence\" + 0.001*\"society\" + 0.001*\"crime\" + 0.001*\"program\" + 0.001*\"parole\" + 0.001*\"family\" + 0.001*\"incarcerate\" + 0.001*\"law\"\n",
      "2019-10-21 20:50:33,964 : INFO : topic #3 (0.100): 0.001*\"inmate\" + 0.001*\"woman\" + 0.001*\"california\" + 0.001*\"program\" + 0.001*\"write\" + 0.001*\"god\" + 0.001*\"parole\" + 0.001*\"help\" + 0.001*\"guy\" + 0.001*\"cell\"\n",
      "2019-10-21 20:50:33,968 : INFO : topic #4 (0.100): 0.001*\"inmate\" + 0.001*\"men\" + 0.001*\"white\" + 0.001*\"officer\" + 0.001*\"move\" + 0.001*\"start\" + 0.001*\"drug\" + 0.001*\"visit\" + 0.001*\"eye\" + 0.001*\"write\"\n",
      "2019-10-21 20:50:33,970 : INFO : topic #5 (0.100): 0.001*\"cell\" + 0.001*\"society\" + 0.001*\"release\" + 0.001*\"inmate\" + 0.001*\"love\" + 0.001*\"program\" + 0.001*\"sentence\" + 0.001*\"offender\" + 0.001*\"child\" + 0.001*\"officer\"\n",
      "2019-10-21 20:50:33,977 : INFO : topic #6 (0.100): 0.003*\"inmate\" + 0.001*\"cell\" + 0.001*\"officer\" + 0.001*\"gang\" + 0.001*\"guard\" + 0.001*\"program\" + 0.001*\"parole\" + 0.001*\"sentence\" + 0.001*\"death\" + 0.001*\"staff\"\n",
      "2019-10-21 20:50:33,979 : INFO : topic #7 (0.100): 0.002*\"inmate\" + 0.002*\"death\" + 0.001*\"parole\" + 0.001*\"cell\" + 0.001*\"love\" + 0.001*\"officer\" + 0.001*\"military\" + 0.001*\"guard\" + 0.001*\"woman\" + 0.001*\"men\"\n",
      "2019-10-21 20:50:33,983 : INFO : topic #8 (0.100): 0.002*\"inmate\" + 0.001*\"cell\" + 0.001*\"michigan\" + 0.001*\"officer\" + 0.001*\"facility\" + 0.001*\"conform\" + 0.001*\"guard\" + 0.001*\"mail\" + 0.001*\"family\" + 0.001*\"person\"\n",
      "2019-10-21 20:50:33,985 : INFO : topic #9 (0.100): 0.001*\"inmate\" + 0.001*\"guard\" + 0.001*\"black\" + 0.001*\"court\" + 0.001*\"medical\" + 0.001*\"parole\" + 0.001*\"california\" + 0.001*\"cdcr\" + 0.001*\"young\" + 0.001*\"texas\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.001*\"law\" + 0.001*\"american\" + 0.001*\"police\" + 0.001*\"inmate\" + 0.001*\"crime\" + 0.001*\"criminal\" + 0.001*\"political\" + 0.001*\"carolina\" + 0.001*\"consciousness\" + 0.001*\"african\"\n",
      "Topic: 1 Word: 0.001*\"inmate\" + 0.001*\"pennsylvania\" + 0.001*\"god\" + 0.001*\"violent\" + 0.001*\"book\" + 0.001*\"te\" + 0.001*\"guard\" + 0.001*\"remember\" + 0.001*\"page\" + 0.001*\"john\"\n",
      "Topic: 2 Word: 0.002*\"offender\" + 0.002*\"inmate\" + 0.001*\"sentence\" + 0.001*\"society\" + 0.001*\"crime\" + 0.001*\"program\" + 0.001*\"parole\" + 0.001*\"family\" + 0.001*\"incarcerate\" + 0.001*\"law\"\n",
      "Topic: 3 Word: 0.001*\"inmate\" + 0.001*\"woman\" + 0.001*\"california\" + 0.001*\"program\" + 0.001*\"write\" + 0.001*\"god\" + 0.001*\"parole\" + 0.001*\"help\" + 0.001*\"guy\" + 0.001*\"cell\"\n",
      "Topic: 4 Word: 0.001*\"inmate\" + 0.001*\"men\" + 0.001*\"white\" + 0.001*\"officer\" + 0.001*\"move\" + 0.001*\"start\" + 0.001*\"drug\" + 0.001*\"visit\" + 0.001*\"eye\" + 0.001*\"write\"\n",
      "Topic: 5 Word: 0.001*\"cell\" + 0.001*\"society\" + 0.001*\"release\" + 0.001*\"inmate\" + 0.001*\"love\" + 0.001*\"program\" + 0.001*\"sentence\" + 0.001*\"offender\" + 0.001*\"child\" + 0.001*\"officer\"\n",
      "Topic: 6 Word: 0.003*\"inmate\" + 0.001*\"cell\" + 0.001*\"officer\" + 0.001*\"gang\" + 0.001*\"guard\" + 0.001*\"program\" + 0.001*\"parole\" + 0.001*\"sentence\" + 0.001*\"death\" + 0.001*\"staff\"\n",
      "Topic: 7 Word: 0.002*\"inmate\" + 0.002*\"death\" + 0.001*\"parole\" + 0.001*\"cell\" + 0.001*\"love\" + 0.001*\"officer\" + 0.001*\"military\" + 0.001*\"guard\" + 0.001*\"woman\" + 0.001*\"men\"\n",
      "Topic: 8 Word: 0.002*\"inmate\" + 0.001*\"cell\" + 0.001*\"michigan\" + 0.001*\"officer\" + 0.001*\"facility\" + 0.001*\"conform\" + 0.001*\"guard\" + 0.001*\"mail\" + 0.001*\"family\" + 0.001*\"person\"\n",
      "Topic: 9 Word: 0.001*\"inmate\" + 0.001*\"guard\" + 0.001*\"black\" + 0.001*\"court\" + 0.001*\"medical\" + 0.001*\"parole\" + 0.001*\"california\" + 0.001*\"cdcr\" + 0.001*\"young\" + 0.001*\"texas\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non',\n",
       " 'review',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'commission',\n",
       " 'conscious',\n",
       " 'prisoner',\n",
       " 'conscious',\n",
       " 'prisoner',\n",
       " 'evolution',\n",
       " 'uhuru',\n",
       " 'non',\n",
       " 'review',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'commission',\n",
       " 'ﬁt',\n",
       " 'monday',\n",
       " 'mar',\n",
       " 'post',\n",
       " 'bobdylan',\n",
       " 'writing',\n",
       " 'uhuru',\n",
       " 'leave',\n",
       " 'comment',\n",
       " 'tag',\n",
       " 'law',\n",
       " 'abolition',\n",
       " 'parole',\n",
       " 'executive',\n",
       " 'order',\n",
       " 'governor',\n",
       " 'terry',\n",
       " 'mcauliffe',\n",
       " 'parole',\n",
       " 'republican',\n",
       " 'truth',\n",
       " 'sentence',\n",
       " 'law',\n",
       " 'uhuru',\n",
       " 'rowe',\n",
       " 'virginia',\n",
       " 'virginia',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'commision',\n",
       " 'virginia',\n",
       " 'sentence',\n",
       " 'commission',\n",
       " 'uhuru',\n",
       " 'rowe',\n",
       " 'february',\n",
       " 'commission',\n",
       " 'speciﬁcally',\n",
       " 'address',\n",
       " 'reinstate',\n",
       " 'discretionary',\n",
       " 'parole',\n",
       " 'due',\n",
       " 'limited',\n",
       " 'time',\n",
       " 'commission',\n",
       " 'interrelate',\n",
       " 'complex',\n",
       " 'issue',\n",
       " 'include',\n",
       " 'incomplete',\n",
       " 'data',\n",
       " 'make',\n",
       " 'issue',\n",
       " 'difﬁcult',\n",
       " 'address',\n",
       " 'time',\n",
       " 'period',\n",
       " 'allow',\n",
       " 'governor',\n",
       " 'terry',\n",
       " 'mcauliffe',\n",
       " 'reveal',\n",
       " 'commission',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'executive',\n",
       " 'summary',\n",
       " 'section',\n",
       " 'december',\n",
       " 'governor',\n",
       " 'commission',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'final',\n",
       " 'report',\n",
       " 'recommendation',\n",
       " 'final',\n",
       " 'statement',\n",
       " 'come',\n",
       " 'surprise',\n",
       " 'virginia',\n",
       " 'prisoner',\n",
       " 'sentence',\n",
       " 'draconian',\n",
       " 'parole',\n",
       " 'percent',\n",
       " 'law',\n",
       " 'love',\n",
       " 'one',\n",
       " 'community',\n",
       " 'grassroots',\n",
       " 'organization',\n",
       " 'extremely',\n",
       " 'optimistic',\n",
       " 'commission',\n",
       " 'would',\n",
       " 'fair',\n",
       " 'thing',\n",
       " 'recommend',\n",
       " 'parole',\n",
       " 'reinstate',\n",
       " 'even',\n",
       " 'shock',\n",
       " 'commission',\n",
       " 'even',\n",
       " 'consider',\n",
       " 'address',\n",
       " 'issue',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'even',\n",
       " 'though',\n",
       " 'main',\n",
       " 'reason',\n",
       " 'commission',\n",
       " 'establish',\n",
       " 'ﬁrst',\n",
       " 'place',\n",
       " 'let',\n",
       " 'examine',\n",
       " 'statement',\n",
       " 'commission',\n",
       " 'lawfulness',\n",
       " 'truthfulness',\n",
       " 'first',\n",
       " 'state',\n",
       " 'commission',\n",
       " 'final',\n",
       " 'commission',\n",
       " 'speciﬁcally',\n",
       " 'address',\n",
       " 'reinstate',\n",
       " 'discretionary',\n",
       " 'parole',\n",
       " 'detailed',\n",
       " 'final',\n",
       " 'main',\n",
       " 'reason',\n",
       " 'governor',\n",
       " 'mcauliffe',\n",
       " 'establish',\n",
       " 'commission',\n",
       " 'june',\n",
       " 'via',\n",
       " 'executive',\n",
       " 'order',\n",
       " 'review',\n",
       " 'decision',\n",
       " 'abolish',\n",
       " 'parole',\n",
       " 'final',\n",
       " 'commission',\n",
       " 'never',\n",
       " 'get',\n",
       " 'around',\n",
       " 'speciﬁcally',\n",
       " 'address',\n",
       " 'ing',\n",
       " 'reinstate',\n",
       " 'parole',\n",
       " 'violate',\n",
       " 'contempt',\n",
       " 'governor',\n",
       " 'executive',\n",
       " 'order',\n",
       " 'second',\n",
       " 'commission',\n",
       " 'state',\n",
       " 'speciﬁcally',\n",
       " 'address',\n",
       " 'reinstate',\n",
       " 'discretionary',\n",
       " 'parole',\n",
       " 'due',\n",
       " 'interrelate',\n",
       " 'complex',\n",
       " 'issue',\n",
       " 'include',\n",
       " 'incomplete',\n",
       " 'data',\n",
       " 'make',\n",
       " 'issue',\n",
       " 'difﬁcult',\n",
       " 'address',\n",
       " 'time',\n",
       " 'period',\n",
       " 'allow',\n",
       " 'declaration',\n",
       " 'meant',\n",
       " 'deceive',\n",
       " 'citizen',\n",
       " 'virginia',\n",
       " 'believe',\n",
       " 'commission',\n",
       " 'lack',\n",
       " 'knowledge',\n",
       " 'research',\n",
       " 'data',\n",
       " 'related',\n",
       " 'report',\n",
       " 'concern',\n",
       " 'abolition',\n",
       " 'parole',\n",
       " 'truth',\n",
       " 'sentence',\n",
       " 'social',\n",
       " 'economic',\n",
       " 'impact',\n",
       " 'law',\n",
       " 'prisoner',\n",
       " 'disadvantage',\n",
       " 'community',\n",
       " 'economy',\n",
       " 'instead',\n",
       " 'opposite',\n",
       " 'true',\n",
       " 'consider',\n",
       " 'follow',\n",
       " 'governor',\n",
       " 'state',\n",
       " 'executive',\n",
       " 'order',\n",
       " 'virginia',\n",
       " 'two',\n",
       " 'decade',\n",
       " 'evidence',\n",
       " 'ass',\n",
       " 'progress',\n",
       " 'public',\n",
       " 'safety',\n",
       " 'outcome',\n",
       " 'determine',\n",
       " 'whether',\n",
       " 'abolish',\n",
       " 'parole',\n",
       " 'achieve',\n",
       " 'intend',\n",
       " 'goal',\n",
       " 'happen',\n",
       " 'two',\n",
       " 'decade',\n",
       " 'evidence',\n",
       " 'ﬁve',\n",
       " 'commission',\n",
       " 'http',\n",
       " 'wordp',\n",
       " 'es',\n",
       " 'com',\n",
       " 'non',\n",
       " 'revievv',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'commission',\n",
       " 'non',\n",
       " 'review',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'commission',\n",
       " 'conscious',\n",
       " 'prisoner',\n",
       " 'meeting',\n",
       " 'ﬁrst',\n",
       " 'commission',\n",
       " 'meeting',\n",
       " 'eric',\n",
       " 'finkbeiner',\n",
       " 'former',\n",
       " 'executive',\n",
       " 'director',\n",
       " 'former',\n",
       " 'governor',\n",
       " 'george',\n",
       " 'allen',\n",
       " 'commission',\n",
       " 'parole',\n",
       " 'abolition',\n",
       " 'sentence',\n",
       " 'reform',\n",
       " 'provide',\n",
       " 'presentation',\n",
       " 'history',\n",
       " 'impact',\n",
       " 'parole',\n",
       " 'abolition',\n",
       " 'sentence',\n",
       " 'reform',\n",
       " 'mr',\n",
       " 'finkbeiner',\n",
       " 'report',\n",
       " 'provide',\n",
       " 'historical',\n",
       " 'overview',\n",
       " 'objective',\n",
       " 'final',\n",
       " 'august',\n",
       " 'commission',\n",
       " 'meeting',\n",
       " 'meridith',\n",
       " 'farar',\n",
       " 'owen',\n",
       " 'director',\n",
       " 'virginia',\n",
       " 'sentence',\n",
       " 'commission',\n",
       " 'provide',\n",
       " 'overview',\n",
       " 'virginia',\n",
       " 'voluntary',\n",
       " 'sentence',\n",
       " 'guideline',\n",
       " 'topic',\n",
       " 'present',\n",
       " 'include',\n",
       " 'limited',\n",
       " 'impetus',\n",
       " 'sentence',\n",
       " 'guideline',\n",
       " 'goal',\n",
       " 'sentence',\n",
       " 'reform',\n",
       " 'methodology',\n",
       " 'use',\n",
       " 'create',\n",
       " 'historically',\n",
       " 'base',\n",
       " 'sentence',\n",
       " 'guideline',\n",
       " 'methodology',\n",
       " 'use',\n",
       " 'parole',\n",
       " 'abolish',\n",
       " 'violent',\n",
       " 'offender',\n",
       " 'term',\n",
       " 'incarceration',\n",
       " 'guideline',\n",
       " 'enhancement',\n",
       " 'overview',\n",
       " 'final',\n",
       " 'commission',\n",
       " 'meeting',\n",
       " 'dick',\n",
       " 'hickman',\n",
       " 'deputy',\n",
       " 'staff',\n",
       " 'director',\n",
       " 'virginia',\n",
       " 'senate',\n",
       " 'finance',\n",
       " 'committee',\n",
       " 'provide',\n",
       " 'comprehensive',\n",
       " 'overview',\n",
       " 'implementation',\n",
       " 'abolition',\n",
       " 'parole',\n",
       " 'adoption',\n",
       " 'felony',\n",
       " 'sentence',\n",
       " 'guideline',\n",
       " 'topic',\n",
       " 'include',\n",
       " 'limited',\n",
       " 'reform',\n",
       " 'objective',\n",
       " 'background',\n",
       " 'crime',\n",
       " 'bill',\n",
       " 'old',\n",
       " 'law',\n",
       " 'verse',\n",
       " 'new',\n",
       " 'law',\n",
       " 'parole',\n",
       " 'percentage',\n",
       " 'time',\n",
       " 'serve',\n",
       " 'verse',\n",
       " 'actual',\n",
       " 'time',\n",
       " 'serve',\n",
       " 'sentence',\n",
       " 'cost',\n",
       " 'recidivism',\n",
       " 'crime',\n",
       " 'incarceration',\n",
       " 'rate',\n",
       " 'final',\n",
       " 'five',\n",
       " 'commission',\n",
       " 'meeting',\n",
       " 'member',\n",
       " 'access',\n",
       " 'two',\n",
       " 'report',\n",
       " 'release',\n",
       " 'justice',\n",
       " 'policy',\n",
       " 'institute',\n",
       " 'billion',\n",
       " 'dollar',\n",
       " 'divide',\n",
       " 'virginia',\n",
       " 'sentence',\n",
       " 'correction',\n",
       " 'criminal',\n",
       " 'justice',\n",
       " 'challenge',\n",
       " 'virginia',\n",
       " 'justice',\n",
       " 'system',\n",
       " 'expensive',\n",
       " 'ineffective',\n",
       " 'unfair',\n",
       " 'addition',\n",
       " 'mound',\n",
       " 'research',\n",
       " 'data',\n",
       " 'information',\n",
       " 'provide',\n",
       " 'american',\n",
       " 'probation',\n",
       " 'parole',\n",
       " 'association',\n",
       " 'american',\n",
       " 'civil',\n",
       " 'liberty',\n",
       " 'union',\n",
       " 'legal',\n",
       " 'aid',\n",
       " 'justice',\n",
       " 'center',\n",
       " 'virginia',\n",
       " 'alliance',\n",
       " 'mass',\n",
       " 'incarceration',\n",
       " 'resource',\n",
       " 'information',\n",
       " 'help',\n",
       " 'disadvantage',\n",
       " 'name',\n",
       " 'plenty',\n",
       " 'available',\n",
       " 'data',\n",
       " 'commission',\n",
       " 'member',\n",
       " 'could',\n",
       " 'avail',\n",
       " 'address',\n",
       " 'examine',\n",
       " 'issue',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'timely',\n",
       " 'manner',\n",
       " 'commission',\n",
       " 'ﬁnal',\n",
       " 'report',\n",
       " 'recommendation',\n",
       " 'reveal',\n",
       " 'state',\n",
       " 'republican',\n",
       " 'remain',\n",
       " 'diametrically',\n",
       " 'oppose',\n",
       " 'establishment',\n",
       " 'commission',\n",
       " 'idea',\n",
       " 'restore',\n",
       " 'parole',\n",
       " 'since',\n",
       " 'recent',\n",
       " 'vocal',\n",
       " 'republican',\n",
       " 'opponent',\n",
       " 'state',\n",
       " 'delegate',\n",
       " 'david',\n",
       " 'albo',\n",
       " 'todd',\n",
       " 'gilbert',\n",
       " 'robert',\n",
       " 'bell',\n",
       " 'house',\n",
       " 'speaker',\n",
       " 'william',\n",
       " 'howell',\n",
       " 'former',\n",
       " 'state',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'jerry',\n",
       " 'kilgore',\n",
       " 'accord',\n",
       " 'albo',\n",
       " 'commission',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'legislative',\n",
       " 'body',\n",
       " 'therefore',\n",
       " 'cannot',\n",
       " 'overturn',\n",
       " 'abolition',\n",
       " 'parole',\n",
       " 'republican',\n",
       " 'member',\n",
       " 'general',\n",
       " 'assembly',\n",
       " 'never',\n",
       " 'reinstate',\n",
       " 'parole',\n",
       " 'final',\n",
       " 'whether',\n",
       " 'refer',\n",
       " 'second',\n",
       " 'look',\n",
       " 'reinstatement',\n",
       " 'never',\n",
       " 'happen',\n",
       " 'albo',\n",
       " 'conclude',\n",
       " 'state',\n",
       " 'republican',\n",
       " 'remain',\n",
       " 'ﬁerce',\n",
       " 'opposition',\n",
       " 'idea',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'secretary',\n",
       " 'commonwealth',\n",
       " 'co',\n",
       " 'chair',\n",
       " 'commission',\n",
       " 'levar',\n",
       " 'stoney',\n",
       " 'advise',\n",
       " 'commission',\n",
       " 'member',\n",
       " 'october',\n",
       " 'meeting',\n",
       " 'discussion',\n",
       " 'next',\n",
       " 'meeting',\n",
       " 'pertain',\n",
       " 'specifically',\n",
       " 'parole',\n",
       " 'even',\n",
       " 'propose',\n",
       " 'change',\n",
       " 'name',\n",
       " 'commission',\n",
       " 'criminal',\n",
       " 'justice',\n",
       " 'reform',\n",
       " 'final',\n",
       " 'never',\n",
       " 'chance',\n",
       " 'u',\n",
       " 'faith',\n",
       " 'bipartisan',\n",
       " 'commission',\n",
       " 'would',\n",
       " 'right',\n",
       " 'thing',\n",
       " 'deeply',\n",
       " 'disappointed',\n",
       " 'commission',\n",
       " 'hijack',\n",
       " 'republican',\n",
       " 'bully',\n",
       " 'result',\n",
       " 'smalltime',\n",
       " 'recommendation',\n",
       " 'reform',\n",
       " 'contrary',\n",
       " 'citizen',\n",
       " 'virginia',\n",
       " 'side',\n",
       " 'political',\n",
       " 'spectrum',\n",
       " 'commonwealth',\n",
       " 'poll',\n",
       " 'conduct',\n",
       " 'douglas',\n",
       " 'wilder',\n",
       " 'school',\n",
       " 'government',\n",
       " 'public',\n",
       " 'affair',\n",
       " 'virginia',\n",
       " 'commonwealth',\n",
       " 'university',\n",
       " 'collaboration',\n",
       " 'office',\n",
       " 'virginia',\n",
       " 'secretary',\n",
       " 'public',\n",
       " 'safety',\n",
       " 'show',\n",
       " 'percent',\n",
       " 'virginian',\n",
       " 'include',\n",
       " 'percent',\n",
       " 'identify',\n",
       " 'democrat',\n",
       " 'percent',\n",
       " 'identify',\n",
       " 'republican',\n",
       " 'percent',\n",
       " 'black',\n",
       " 'respondent',\n",
       " 'percent',\n",
       " 'white',\n",
       " 'respondent',\n",
       " 'support',\n",
       " 'idea',\n",
       " 'parole',\n",
       " 'reinstement',\n",
       " 'poll',\n",
       " 'truly',\n",
       " 'reflect',\n",
       " 'people',\n",
       " 'job',\n",
       " 'state',\n",
       " 'legislator',\n",
       " 'serve',\n",
       " 'represent',\n",
       " 'interest',\n",
       " 'people',\n",
       " 'commission',\n",
       " 'final',\n",
       " 'report',\n",
       " 'recommendation',\n",
       " 'reﬂect',\n",
       " 'interest',\n",
       " 'people',\n",
       " 'overwhelmingly',\n",
       " 'favor',\n",
       " 'reinstate',\n",
       " 'parole',\n",
       " 'http',\n",
       " 'com',\n",
       " 'non',\n",
       " 'review',\n",
       " 'pa',\n",
       " 'ole',\n",
       " 'reinstatement',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'commission',\n",
       " 'non',\n",
       " 'review',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'commission',\n",
       " 'conscious',\n",
       " 'prisoner',\n",
       " 'since',\n",
       " 'commission',\n",
       " 'fail',\n",
       " 'perform',\n",
       " 'duty',\n",
       " 'laid',\n",
       " 'executive',\n",
       " 'order',\n",
       " 'face',\n",
       " 'republican',\n",
       " 'bully',\n",
       " 'tactic',\n",
       " 'propose',\n",
       " 'recommendation',\n",
       " 'governor',\n",
       " 'mcauliffe',\n",
       " 'member',\n",
       " 'state',\n",
       " 'general',\n",
       " 'assembly',\n",
       " 'serve',\n",
       " 'justice',\n",
       " 'u',\n",
       " 'conﬁned',\n",
       " 'parole',\n",
       " 'percent',\n",
       " 'draconian',\n",
       " 'truth',\n",
       " 'sentence',\n",
       " 'law',\n",
       " 'tackle',\n",
       " 'issue',\n",
       " 'mass',\n",
       " 'incarceration',\n",
       " 'virginia',\n",
       " 'ensure',\n",
       " 'public',\n",
       " 'safety',\n",
       " 'divert',\n",
       " 'fund',\n",
       " 'spent',\n",
       " 'prison',\n",
       " 'jail',\n",
       " 'toward',\n",
       " 'community',\n",
       " 'base',\n",
       " 'reentry',\n",
       " 'job',\n",
       " 'placement',\n",
       " 'housing',\n",
       " 'program',\n",
       " 'public',\n",
       " 'education',\n",
       " 'reinstate',\n",
       " 'discretionary',\n",
       " 'parole',\n",
       " 'parole',\n",
       " 'percent',\n",
       " 'law',\n",
       " 'prisoner',\n",
       " 'repeal',\n",
       " 'virginia',\n",
       " 'code',\n",
       " 'section',\n",
       " 'va',\n",
       " 'code',\n",
       " 'restore',\n",
       " 'discretionary',\n",
       " 'parole',\n",
       " 'va',\n",
       " 'code',\n",
       " 'section',\n",
       " 'thru',\n",
       " 'restore',\n",
       " 'good',\n",
       " 'conduct',\n",
       " 'allowance',\n",
       " 'good',\n",
       " 'time',\n",
       " 'credit',\n",
       " 'va',\n",
       " 'code',\n",
       " 'compliance',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'precedent',\n",
       " 'miller',\n",
       " 'alabama',\n",
       " 'graham',\n",
       " 'florida',\n",
       " 'render',\n",
       " 'juvenile',\n",
       " 'life',\n",
       " 'without',\n",
       " 'parole',\n",
       " 'sentence',\n",
       " 'establish',\n",
       " 'parole',\n",
       " 'eligibility',\n",
       " 'every',\n",
       " 'prisoner',\n",
       " 'sentence',\n",
       " 'life',\n",
       " 'without',\n",
       " 'parole',\n",
       " 'juvenile',\n",
       " 'serve',\n",
       " 'base',\n",
       " 'minimum',\n",
       " 'year',\n",
       " 'create',\n",
       " 'mechanism',\n",
       " 'new',\n",
       " 'sentence',\n",
       " 'hearing',\n",
       " 'court',\n",
       " 'heard',\n",
       " 'case',\n",
       " 'amend',\n",
       " 'reenact',\n",
       " 'va',\n",
       " 'code',\n",
       " 'prisoner',\n",
       " 'currently',\n",
       " 'va',\n",
       " 'doc',\n",
       " 'petition',\n",
       " 'court',\n",
       " 'heard',\n",
       " 'case',\n",
       " 'suspension',\n",
       " 'reduction',\n",
       " 'modiﬁcation',\n",
       " 'unserved',\n",
       " 'portion',\n",
       " 'sentence',\n",
       " 'serve',\n",
       " 'base',\n",
       " 'minimum',\n",
       " 'year',\n",
       " 'completion',\n",
       " 'education',\n",
       " 'rehabilitation',\n",
       " 'reentry',\n",
       " 'preparedness',\n",
       " 'program',\n",
       " 'amend',\n",
       " 'reenact',\n",
       " 'va',\n",
       " 'code',\n",
       " 'qualify',\n",
       " 'age',\n",
       " 'consideration',\n",
       " 'call',\n",
       " 'geriatric',\n",
       " 'parole',\n",
       " 'age',\n",
       " 'old',\n",
       " 'serve',\n",
       " 'least',\n",
       " 'five',\n",
       " 'year',\n",
       " 'sentence',\n",
       " 'impose',\n",
       " 'age',\n",
       " 'old',\n",
       " 'serve',\n",
       " 'least',\n",
       " 'ten',\n",
       " 'year',\n",
       " 'sentence',\n",
       " 'impose',\n",
       " 'must',\n",
       " 'retroactive',\n",
       " 'amend',\n",
       " 'reenact',\n",
       " 'va',\n",
       " 'code',\n",
       " 'sentence',\n",
       " 'court',\n",
       " 'must',\n",
       " 'provide',\n",
       " 'adequate',\n",
       " 'detailed',\n",
       " 'explanation',\n",
       " 'impose',\n",
       " 'sentence',\n",
       " 'exceeds',\n",
       " 'sentence',\n",
       " 'guideline',\n",
       " 'recommendation',\n",
       " 'allow',\n",
       " 'meaning',\n",
       " 'appellate',\n",
       " 'review',\n",
       " 'sentence',\n",
       " 'accord',\n",
       " 'virginia',\n",
       " 'criminal',\n",
       " 'sentence',\n",
       " 'commission',\n",
       " 'annual',\n",
       " 'report',\n",
       " 'fy',\n",
       " 'fy',\n",
       " 'write',\n",
       " 'reason',\n",
       " 'departure',\n",
       " 'provide',\n",
       " 'judge',\n",
       " 'case',\n",
       " 'sentence',\n",
       " 'impose',\n",
       " 'exceed',\n",
       " 'guidlines',\n",
       " 'recommendation',\n",
       " 'thus',\n",
       " 'unclear',\n",
       " 'whether',\n",
       " 'departure',\n",
       " 'case',\n",
       " 'due',\n",
       " 'factor',\n",
       " 'like',\n",
       " 'race',\n",
       " 'religion',\n",
       " 'sexual',\n",
       " 'orientation',\n",
       " 'economic',\n",
       " 'status',\n",
       " 'consideration',\n",
       " 'play',\n",
       " 'role',\n",
       " 'sentence',\n",
       " 'decision',\n",
       " 'news',\n",
       " 'january',\n",
       " 'must',\n",
       " 'retroactive',\n",
       " 'comment',\n",
       " 'blog',\n",
       " 'post',\n",
       " 'mail',\n",
       " 'uhuru',\n",
       " 'rowe',\n",
       " 'po',\n",
       " 'box',\n",
       " 'dillwyn',\n",
       " 'va',\n",
       " 'email',\n",
       " 'pay',\n",
       " 'download',\n",
       " 'pay',\n",
       " 'mobile',\n",
       " 'app',\n",
       " 'phone',\n",
       " 'sign',\n",
       " 'exchange',\n",
       " 'email',\n",
       " 'http',\n",
       " 'wordpress',\n",
       " 'com',\n",
       " 'non',\n",
       " 'review',\n",
       " 'parole',\n",
       " 'reinstatement',\n",
       " 'parole',\n",
       " 'review',\n",
       " 'commission']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will check where our test document would be classified.\n",
    "tokenized_essays_list[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9988597631454468\t \n",
      "Topic: 0.006*\"law\" + 0.005*\"sentence\" + 0.005*\"court\" + 0.004*\"criminal\" + 0.003*\"case\" + 0.003*\"crime\" + 0.003*\"person\" + 0.003*\"new\" + 0.003*\"government\" + 0.003*\"release\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[20]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9076688885688782\t \n",
      "Topic: 0.003*\"inmate\" + 0.001*\"cell\" + 0.001*\"officer\" + 0.001*\"gang\" + 0.001*\"guard\" + 0.001*\"program\" + 0.001*\"parole\" + 0.001*\"sentence\" + 0.001*\"death\" + 0.001*\"staff\"\n",
      "\n",
      "Score: 0.09131407737731934\t \n",
      "Topic: 0.001*\"law\" + 0.001*\"american\" + 0.001*\"police\" + 0.001*\"inmate\" + 0.001*\"crime\" + 0.001*\"criminal\" + 0.001*\"political\" + 0.001*\"carolina\" + 0.001*\"consciousness\" + 0.001*\"african\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:50:34,223 : INFO : collecting all words and their counts\n",
      "2019-10-21 20:50:34,225 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-10-21 20:50:34,600 : INFO : collected 57330 word types from a corpus of 1194020 raw words and 1573 sentences\n",
      "2019-10-21 20:50:34,601 : INFO : Loading a fresh vocabulary\n",
      "2019-10-21 20:50:34,824 : INFO : effective_min_count=5 retains 13796 unique words (24% of original 57330, drops 43534)\n",
      "2019-10-21 20:50:34,825 : INFO : effective_min_count=5 leaves 1132837 word corpus (94% of original 1194020, drops 61183)\n",
      "2019-10-21 20:50:34,905 : INFO : deleting the raw counts dictionary of 57330 items\n",
      "2019-10-21 20:50:34,907 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2019-10-21 20:50:34,909 : INFO : downsampling leaves estimated 1100373 word corpus (97.1% of prior 1132837)\n",
      "2019-10-21 20:50:34,962 : INFO : estimated required memory for 13796 words and 100 dimensions: 17934800 bytes\n",
      "2019-10-21 20:50:34,963 : INFO : resetting layer weights\n",
      "2019-10-21 20:50:38,062 : INFO : training model with 3 workers on 13796 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-10-21 20:50:39,100 : INFO : EPOCH 1 - PROGRESS: at 91.23% examples, 997374 words/s, in_qsize 4, out_qsize 1\n",
      "2019-10-21 20:50:39,195 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-21 20:50:39,202 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-21 20:50:39,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-21 20:50:39,218 : INFO : EPOCH - 1 : training on 1194020 raw words (1100561 effective words) took 1.1s, 976709 effective words/s\n",
      "2019-10-21 20:50:40,243 : INFO : EPOCH 2 - PROGRESS: at 86.40% examples, 939139 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-21 20:50:40,393 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-21 20:50:40,395 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-21 20:50:40,399 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-21 20:50:40,401 : INFO : EPOCH - 2 : training on 1194020 raw words (1100657 effective words) took 1.2s, 936941 effective words/s\n",
      "2019-10-21 20:50:41,406 : INFO : EPOCH 3 - PROGRESS: at 83.79% examples, 926938 words/s, in_qsize 6, out_qsize 0\n",
      "2019-10-21 20:50:41,586 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-21 20:50:41,593 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-21 20:50:41,603 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-21 20:50:41,604 : INFO : EPOCH - 3 : training on 1194020 raw words (1100657 effective words) took 1.2s, 918244 effective words/s\n",
      "2019-10-21 20:50:42,632 : INFO : EPOCH 4 - PROGRESS: at 75.02% examples, 812779 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-21 20:50:43,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-21 20:50:43,085 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-21 20:50:43,097 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-21 20:50:43,098 : INFO : EPOCH - 4 : training on 1194020 raw words (1100287 effective words) took 1.5s, 738672 effective words/s\n",
      "2019-10-21 20:50:44,112 : INFO : EPOCH 5 - PROGRESS: at 62.62% examples, 706752 words/s, in_qsize 4, out_qsize 1\n",
      "2019-10-21 20:50:44,702 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-21 20:50:44,707 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-21 20:50:44,711 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-21 20:50:44,712 : INFO : EPOCH - 5 : training on 1194020 raw words (1100194 effective words) took 1.6s, 685383 effective words/s\n",
      "2019-10-21 20:50:44,713 : INFO : training on a 5970100 raw words (5502356 effective words) took 6.7s, 827393 effective words/s\n"
     ]
    }
   ],
   "source": [
    "vector_dim = 100\n",
    "model = gensim.models.Word2Vec(tokenized_essays.values(), size=vector_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide for saving / loading word embedding spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(root + \"/mymodel.space\")\n",
    "# model = gensim.models.Word2Vec.load(root + \"/mymodel.space\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with most_similar terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-21 20:50:44,731 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mistreatment', 0.9414080381393433),\n",
       " ('abusive', 0.9273565411567688),\n",
       " ('suffers', 0.9173864126205444),\n",
       " ('degrade', 0.9134754538536072),\n",
       " ('foster', 0.909307599067688),\n",
       " ('homelessness', 0.9036213159561157),\n",
       " ('diagnosis', 0.8962069749832153),\n",
       " ('unnecessary', 0.8932418823242188),\n",
       " ('symptom', 0.8928334712982178),\n",
       " ('indifference', 0.8922271132469177)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"neglect\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of words in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13796"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tokens to their respective vectors and linearly combine to make single essay:vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_essays = {label: np.sum(np.array([model.wv.word_vec(token) for token in token_lst if token in model.wv.vocab]), axis=0) for (label, token_lst) in tokenized_essays.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it a dataframe and create index reference\n",
    "vectorized_df = pd.DataFrame.from_dict(vectorized_essays, orient='index')\n",
    "index_ref = vectorized_df.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling through standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsclr = StandardScaler()\n",
    "standardized_df = pd.DataFrame(stdsclr.fit_transform(vectorized_df), index=index_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "reduced_df = pd.DataFrame(pca.fit_transform(standardized_df), index=index_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide for output to visualize effectiveness of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.to_csv('new.csv', sep='\\t', index=False, header=False)\n",
    "pd.DataFrame(index_ref).to_csv('index.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Import the wordcloud library\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# # Join the different processed titles together.\n",
    "# long_string = ','.join(texts)\n",
    "\n",
    "# # Create a WordCloud object\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# # Generate a word cloud\n",
    "# wordcloud.generate(long_string)\n",
    "\n",
    "# # Visualize the word cloud\n",
    "# wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the library with the CountVectorizer method\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_style('whitegrid')\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Helper function\n",
    "# def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     words = count_vectorizer.get_feature_names()\n",
    "#     total_counts = np.zeros(len(words))\n",
    "#     for t in count_data:\n",
    "#         total_counts+=t.toarray()[0]\n",
    "    \n",
    "#     count_dict = (zip(words, total_counts))\n",
    "#     count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "#     words = [w[0] for w in count_dict]\n",
    "#     counts = [w[1] for w in count_dict]\n",
    "#     x_pos = np.arange(len(words)) \n",
    "    \n",
    "#     plt.figure(2, figsize=(15, 15/1.6180))\n",
    "#     plt.subplot(title='10 most common words')\n",
    "#     sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "#     sns.barplot(x_pos, counts, palette='husl')\n",
    "#     plt.xticks(x_pos, words, rotation=90) \n",
    "#     plt.xlabel('words')\n",
    "#     plt.ylabel('counts')\n",
    "#     plt.show()\n",
    "\n",
    "# file1 = open(\"desktop/Larson_Project/APWATranscriptions-WC2.txt\",\"r\")\n",
    "\n",
    "# # Initialise the count vectorizer with the English stop words\n",
    "# count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# # Fit and transform the processed titles\n",
    "# count_data = count_vectorizer.fit_transform(file1)\n",
    "\n",
    "# # Visualise the 10 most common words\n",
    "# plot_10_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# # Load the LDA model from sk-learn\n",
    "# from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# # Helper function\n",
    "# def print_topics(model, count_vectorizer, n_top_words):\n",
    "#     words = count_vectorizer.get_feature_names()\n",
    "#     for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(\"\\nTopic #%d:\" % topic_idx)\n",
    "#         print(\" \".join([words[i]\n",
    "#                         for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# # Tweak the two parameters below (use int values below 15)\n",
    "# number_topics = 5\n",
    "# number_words = 10\n",
    "\n",
    "# # Create and fit the LDA model\n",
    "# lda = LDA(n_components=number_topics)\n",
    "# lda.fit(count_data)\n",
    "\n",
    "# # Print the topics found by the LDA model\n",
    "# print(\"Topics found via LDA:\")\n",
    "# print_topics(lda, count_vectorizer, number_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
