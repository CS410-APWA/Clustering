{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import csv\n",
    "import gensim\n",
    "import logging\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Essays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_path = 'essays/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(essay_path)\n",
    "\n",
    "essays = {}\n",
    "for file in files:\n",
    "    # attempt to confidently guess encoding; otherwise, default to ISO-8859-1\n",
    "    encoding = \"ISO-8859-1\"\n",
    "    guess = chardet.detect(open(essay_path + file, \"rb\").read())\n",
    "    if (guess[\"confidence\"] >= 0.95):\n",
    "        encoding = guess[\"encoding\"]\n",
    "    \n",
    "    with open(essay_path + file, \"r\", encoding=encoding) as f:\n",
    "        essays[file] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize + Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_essays = {label: gensim.utils.simple_preprocess(corpus, deacc=True, min_len=2, max_len=15) for (label, corpus) in essays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "custom_stopwords = [\n",
    "        \"prison\",\n",
    "        \"prisoner\",\n",
    "        \"also\",\n",
    "        \"said\",\n",
    "        \"mr\",\n",
    "        \"mrs\",\n",
    "        \"im\",\n",
    "        \"would\",\n",
    "        \"could\",\n",
    "        \"should\",\n",
    "        \"first\",\n",
    "        \"like\",\n",
    "        \"dont\",\n",
    "        \"wont\",\n",
    "        \"get\",\n",
    "        \"going\",\n",
    "        \"thing\",\n",
    "        \"something\",\n",
    "        \"use\",\n",
    "        \"get\",\n",
    "        \"go\",\n",
    "        \"one\"\n",
    "    ]\n",
    "\n",
    "combined_stopwords = english_stopwords + custom_stopwords\n",
    "\n",
    "tokenized_essays = {label: [w for w in token_lst if w not in combined_stopwords] for (label, token_lst) in tokenized_essays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokenized_essays = {label: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in token_lst if w not in string.punctuation and w not in combined_stopwords] for (label, token_lst) in tokenized_essays.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize w/ doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts processed:  1573\n"
     ]
    }
   ],
   "source": [
    "LabeledSentence1 = gensim.models.doc2vec.TaggedDocument\n",
    "all_content_train = []\n",
    "j=0\n",
    "for essay in tokenized_essays.values():\n",
    "    all_content_train.append(LabeledSentence1(essay, [j]))\n",
    "    j+=1\n",
    "    \n",
    "print(\"Number of texts processed: \", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(all_content_train, vector_size = 100, window = 10, min_count = 500, workers=7, dm = 1,alpha=0.025, min_alpha=0.001)\n",
    "d2v_model.train(all_content_train, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=-0.016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_vectors = d2v_model.docvecs.vectors_docs\n",
    "vectorized_df = pd.DataFrame(essay_vectors)\n",
    "index_ref = vectorized_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling through standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsclr = StandardScaler()\n",
    "standardized_df = pd.DataFrame(stdsclr.fit_transform(vectorized_df.astype(float)), index=index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "reduced = pca.fit_transform(standardized_df)\n",
    "reduced_df = pd.DataFrame(reduced, index=index_ref, columns = ['title', 'cluster', 'essay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide for output to visualize effectiveness of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_df.to_csv('new.csv', sep='\\t', index=False, header=False)\n",
    "#pd.DataFrame(index_ref).to_csv('index.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering w/ k-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 314 ms, sys: 110 ms, total: 424 ms\n",
      "Wall time: 215 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=12, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 12\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, init=\"k-means++\", max_iter=100)\n",
    "\n",
    "%time km.fit(reduced_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     597\n",
      "6     245\n",
      "3     196\n",
      "9     188\n",
      "11     78\n",
      "10     58\n",
      "4      57\n",
      "5      50\n",
      "2      50\n",
      "7      27\n",
      "8      19\n",
      "1       8\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "output = reduced_df\n",
    "output['cluster'] = kmeans_model.labels_\n",
    "print(output['cluster'].value_counts())\n",
    "output['essay'] = tokenized_essays.values()\n",
    "output['title'] = tokenized_essays.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model/Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(kmeans_model,'doc_cluster.pkl')\n",
    "\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "clusters = kmeans_model.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top terms per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Topic #0 : call , case , cell , come , court , crime , day , even , give , inmate , know , law , life , make , many , need , people , right , see , sentence , state , system , take , time , well , work , write , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #1 : appear , become , black , body , come , consciousness , even , experience , inside , life , make , male , member , men , mind , move , new , people , reflection , see , seem , social , society , state , system , take , think , time , way , world\n",
      "\n",
      "\n",
      "\n",
      "Topic #2 : back , come , day , even , give , god , good , know , life , look , love , make , mother , much , never , people , place , say , see , still , take , think , time , want , way , well , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #3 : back , become , change , come , day , even , family , give , help , know , life , look , love , make , man , many , much , need , never , people , say , see , take , think , time , want , way , well , world , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #4 : behavior , change , crime , criminal , experience , inmate , law , life , make , many , may , must , need , people , program , right , see , social , society , state , system , take , think , time , way , well , work , world , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #5 : ask , back , call , cell , come , day , door , even , give , guard , inmate , know , look , make , never , officer , place , see , take , time , told , two , want , way , well , work , yard , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #6 : american , become , change , come , crime , even , human , inmate , law , life , make , many , must , need , people , program , right , see , society , state , system , take , time , way , well , work , world , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #7 : back , case , child , come , court , crime , day , family , federal , good , inmate , know , law , life , make , man , many , new , offender , people , right , sentence , sex , state , take , time , well , work , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #8 : back , become , body , change , come , day , even , experience , help , know , learn , life , look , make , mind , need , people , personal , plan , see , self , take , think , time , way , well , work , world , write , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #9 : back , call , cell , come , day , even , give , guy , inmate , know , life , look , make , many , never , officer , people , say , see , take , time , try , two , want , way , well , work , write , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #10 : case , cell , correction , court , day , department , even , facility , give , inmate , law , life , make , many , officer , official , order , right , see , staff , state , system , take , time , two , unit , work , year\n",
      "\n",
      "\n",
      "\n",
      "Topic #11 : case , child , court , crime , criminal , even , incarcerate , incarceration , inmate , justice , law , life , make , many , need , offender , parole , people , person , program , release , sentence , serve , society , state , system , time , without , year\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_clusters) :\n",
    "    print(\"\\n\\n\")\n",
    "    context = [' '.join(tokens) for tokens in list(output[output.cluster == i].essay)]\n",
    "    \n",
    "    m1 =TfidfVectorizer(max_features=30)\n",
    "    m1.fit(context)\n",
    "    print(\"Topic #{} : {}\".format(i , \" , \".join(term for term in m1.get_feature_names() if term not in combined_stopwords)))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize cluster (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "\n",
    "#%matplotlib inline\n",
    "#plt.figure\n",
    "\n",
    "#cluster_colors = []\n",
    "#for i in range(num_clusters):\n",
    "#    r = lambda: random.randint(0,255)\n",
    "#    cluster_colors.append('#%02X%02X%02X' % (r(),r(),r()))\n",
    "\n",
    "#color = [i for i in cluster_colors]\n",
    "#plt.scatter(datapoint[:, 0], datapoint[:, 1])\n",
    "#centroids = kmeans_model.cluster_centers_\n",
    "#centroidpoint = pca.transform(centroids)\n",
    "#plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker=\"^\", s=150, c=\"#000000\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
