{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import csv\n",
    "import gensim\n",
    "import logging\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Essays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_path = \"/Users/samfarber/Desktop/Larson_Project/Essays/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir(essay_path)\n",
    "\n",
    "essays = {}\n",
    "compound_mapping = [ (\"broken home\", \"brokenhome\"), (\"fit in\", \"fitin\"), (\"home boys\", \"homeboys\"),\n",
    "(\"crime partners\", \"crimepartners\"), (\"road dogs\", \"roaddogs\"), (\"step father\", \"stepfather\"),\n",
    "(\"old lady\", \"oldlady\"), (\"strip search\", \"stripsearch\"), (\"pecking order\", \"peckingorder\"),\n",
    "(\"solid crime\", \"solidcrime\"), (\"dirty crime\", \"dirtycrime\"), (\"skin crime\", \"skincrime\"),\n",
    "(\"solid prisoner\", \"solidprisoner\"), (\"dick sucker\", \"dicksucker\"), (\"cock sucker\", \"cocksucker\"),\n",
    "(\"shot caller\", \"shotcaller\"), (\"butt pirate\", \"buttpirate\"), (\"falsely accuse\", \"falselyaccuse\"),\n",
    "(\"born again\", \"bornagain\"), (\"good guy\", \"goodguy\"), (\"habeas corpus\", \"habeascorpus\"),\n",
    "(\"time barred\", \"timebarred\"), (\"successive petitions\", \"successivepetitions\"), (\"hunger strike\", \"hungerstrike\"),\n",
    "(\"1983 lawsuits\", \"1983lawsuits\"), (\"civil rights complaints\", \"civilrightscomplaints\"), (\"tax payers\", \"taxpayers\"),\n",
    "(\"private prisons\", \"privateprisons\"), (\"prison-industrial complex\", \"prison-industrialcomplex\"), (\"make money off of us\", \"makemoneyoffofus\"),\n",
    "(\"higher education\", \"highereducation\"), (\"correspondence courses\", \"correspondencecourses\"), (\"self help\", \"selfhelp\"),\n",
    "(\"mental health\", \"mentalhealth\"), (\"psych meds\", \"psychmeds\"), (\"kehea meds\", \"keheameds\"), (\"deliberate indifference\", \"deliberateindifference\") ]\n",
    "\n",
    "for file in files:\n",
    "    # attempt to confidently guess encoding; otherwise, default to ISO-8859-1\n",
    "    encoding = \"ISO-8859-1\"\n",
    "    guess = chardet.detect(open(essay_path + file, \"rb\").read())\n",
    "    if (guess[\"confidence\"] >= 0.95):\n",
    "        encoding = guess[\"encoding\"]\n",
    "    \n",
    "    with open(essay_path + file, \"r\", encoding=encoding) as f:\n",
    "        essays[file] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize + Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_essays = {label: gensim.utils.simple_preprocess(corpus, deacc=True, min_len=2, max_len=20) for (label, corpus) in essays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "custom_stopwords = [\n",
    "        \"prison\",\n",
    "        \"jail\",\n",
    "        \"prisoner\",\n",
    "        \"also\",\n",
    "        \"said\",\n",
    "        \"would\",\n",
    "        \"could\",\n",
    "        \"should\",\n",
    "        \"first\",\n",
    "        \"like\",\n",
    "        \"get\",\n",
    "        \"going\",\n",
    "        \"thing\",\n",
    "        \"something\",\n",
    "        \"use\",\n",
    "        \"get\",\n",
    "        \"go\",\n",
    "        \"one\",\n",
    "        \"mr\",\n",
    "        \"many\",\n",
    "        \"much\",\n",
    "        \"see\",\n",
    "        \"take\",\n",
    "        \"another\",\n",
    "        \"aroud\",\n",
    "        \"away\",\n",
    "        \"back\",\n",
    "        \"even\",\n",
    "        \"every\",\n",
    "        \"guy\",\n",
    "        \"know\",\n",
    "        \"let\",\n",
    "        \"make\",\n",
    "        \"look\",\n",
    "        \"person\",\n",
    "        \"place\",\n",
    "        \"day\",\n",
    "        \"year\",\n",
    "        \"well\",\n",
    "        \"good\",\n",
    "        \"bad\",\n",
    "        \"with\",\n",
    "        \"without\",\n",
    "        \"may\",\n",
    "        \"new\",\n",
    "        \"two\",\n",
    "        \"want\",\n",
    "        \"people\",\n",
    "        \"within\",\n",
    "        \"upon\",\n",
    "        \"come\",\n",
    "        \"tilocblob\",\n",
    "        \"yyyyyy\",\n",
    "        \"way\",\n",
    "        \"around\",\n",
    "        \"high\",\n",
    "        \"inside\",\n",
    "        \"long\",\n",
    "        \"men\",\n",
    "        \"must\",\n",
    "        \"need\",\n",
    "        \"never\",\n",
    "        \"old\",\n",
    "        \"other\",\n",
    "        \"others\",\n",
    "        \"really\",\n",
    "        \"say\",\n",
    "        \"seem\",\n",
    "        \"still\",\n",
    "        \"try\",\n",
    "        \"become\",\n",
    "        \"allow\",\n",
    "        \"give\",\n",
    "        \"month\",\n",
    "        \"result\",\n",
    "        \"always\",\n",
    "        \"ask\",\n",
    "        \"begin\",\n",
    "        \"end\",\n",
    "        \"hour\",\n",
    "        \"man\",\n",
    "        \"woman\",\n",
    "        \"put\",\n",
    "        \"someone\",\n",
    "        \"start\",\n",
    "        \"next\",\n",
    "        \"act\",\n",
    "        \"create\",\n",
    "        \"yet\",\n",
    "        \"time\",\n",
    "        \"case\",\n",
    "        \"cell\",\n",
    "        \"work\",\n",
    "        \"call\",\n",
    "        \"world\",\n",
    "        \"tell\",\n",
    "        \"week\",\n",
    "        \"told\",\n",
    "        \"lot\",\n",
    "        \"change\",\n",
    "        \"self\",\n",
    "        \"since\",\n",
    "        \"life\",\n",
    "        \"u\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokenized_essays = {label: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in token_lst if (w not in string.punctuation and w not in english_stopwords and w not in custom_stopwords)] for (label, token_lst) in tokenized_essays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_essays = {label: [w for w in token_lst if (w not in english_stopwords and w not in custom_stopwords)] for (label, token_lst) in tokenized_essays.items()}\n",
    "        \n",
    "lst = list(tokenized_essays.values())\n",
    "for i in range(len(lst)):\n",
    "    essay = ' '.join(lst[i])\n",
    "    for k, v in compound_mapping:\n",
    "        essay = re.sub(k, v, essay, flags=re.IGNORECASE) # replaces spaced words with their compounded versions, ignoring case\n",
    "    lst[i] = essay.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize w/ doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledSentence1 = gensim.models.doc2vec.TaggedDocument\n",
    "all_content_train = []\n",
    "j=0\n",
    "for essay in tokenized_essays.values():\n",
    "    all_content_train.append(LabeledSentence1(essay, [j]))\n",
    "    j+=1\n",
    "    \n",
    "print(\"Number of texts processed: \", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d2v_model = Doc2Vec(all_content_train, vector_size = 100, window = 10, min_count = 500, workers=7, dm = 1,alpha=0.025, min_alpha=0.001)\n",
    "d2v_model = Doc2Vec(all_content_train)\n",
    "\n",
    "#d2v_model.train(all_content_train, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=-0.016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_vectors = d2v_model.docvecs.vectors_docs\n",
    "vectorized_df = pd.DataFrame(essay_vectors)\n",
    "index_ref = vectorized_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling through standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsclr = StandardScaler()\n",
    "standardized_df = pd.DataFrame(stdsclr.fit_transform(vectorized_df.astype(float)), index=index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "reduced_df = pd.DataFrame(pca.fit_transform(standardized_df), index=index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide for output to visualize effectiveness of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_df.to_csv('new1.csv', sep='\\t', index=False, header=False)\n",
    "#pd.DataFrame(index_ref).to_csv('index.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering w/ k-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, init=\"k-means++\", max_iter=100)\n",
    "\n",
    "%time km.fit(reduced_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essays per cluster / Theme(s) per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = reduced_df\n",
    "output['cluster'] = km.labels_\n",
    "print(output['cluster'].value_counts())\n",
    "output['essay'] = tokenized_essays.values()\n",
    "output['title'] = tokenized_essays.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_index = {'Autobiographical, Paths to Prison': 'parent, damage, abuse, gang, alcohol, drug, neighborhood, hood, youth, pressure, fit, broken', \n",
    "                 'Family': 'family, abandonment, relation, visit, partner, mother, father, sibling, child, wife, husband, abuse, support', \n",
    "                 'Physical Conditions and Security': 'physical, condition, security, search, censorship, food, cold, hygiene, heat, misfunction, infestation, solitary, strip, search, fear, filth, violence, staff, abuse', \n",
    "                 'Prison Culture/Community/Society': 'violence, fear, staff, sexual, crime, outcasts, racial, cellmate, gay, LGBTQ, dehumanize, uniform, pecking, order, hierarchy, solid, dirty, skin, chomo',\n",
    "                 'Staff/prison Abuse of IP': 'abuse, sexual, torture, humiliation, racist, assault, antagonism, exacerbation, right, violation, food, hygiene, environment, legal, extraction, search, taunt',\n",
    "                 'Personal/Intern Change/Coping': 'survival, art, reading, writing, peace, faith, prayer, meditation, practice, community, activities, hobbies, cooking, remorse, motivation, education, discipline, coping, adjustment, responsibility, god, redemption, transformation',\n",
    "                 'Judicial Misconduct and Legal Remediation': 'judicial, incompetence, corruption, witness, evidence, excessive, political, jailhouse, lawyer, misconduct, unfair, pretender, plra, plea, grievance',\n",
    "                 'Political and Intellectual Labor among IP': 'activism, resistence, critique, race, class, change, policies, practices, write, organize, strike, solidarity',\n",
    "                 'Prison Industry/Prison as Business': 'labor, slave, condition, safety, health, profit, job, budget, tax, taxpayer, exploitation, corruption, mismanagement, nepotism',\n",
    "                 'Education, Re-entry, Other Programs': 'rehabilitation, entry, education, indifference, college, vocation',\n",
    "                 'Health Care': 'health, care, negligence, hostility, incompetence, indifference, death, injury, treatment, medication',\n",
    "                 'Social Alienation, Indifference, Hostility': 'public, misperception, identity, stigma'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import shutil\n",
    "\n",
    "src = \"/Users/inesayara/Desktop/senior_seminar/essays\"\n",
    "dst = \"/Users/inesayara/Desktop/clusters_1_0/cluster_\"\n",
    "\n",
    "#files = [i for i in os.listdir(src) if path.isfile(path.join(src, i))]\n",
    "#for f in files:\n",
    "#    shutil.copy(path.join(src, f), dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "essays_per_cluster = {}\n",
    "for i in range(num_clusters):\n",
    "    essays_per_cluster[i] = []\n",
    "    #print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in output[output.cluster == i]['title'].values.tolist():\n",
    "        #print(' %s,' % title, end='')\n",
    "        essays_per_cluster[i].append(title)\n",
    "        \n",
    "print(\"***** Random sample essays per cluster *****\")\n",
    "print()\n",
    "\n",
    "terms_per_essays = {}\n",
    "themes_per_essays = {}\n",
    "for i in range(num_clusters):\n",
    "    ten = []\n",
    "    if len(essays_per_cluster[i]) < 10:\n",
    "        ten = essays_per_cluster[i]\n",
    "    else:\n",
    "        while len(ten) < 10:\n",
    "            essay = random.choice(essays_per_cluster[i])\n",
    "            if essay not in ten:\n",
    "                ten.append(essay)\n",
    "\n",
    "    print(\"** Cluster %d: **\" % i)\n",
    "    print()\n",
    "    context = [' '.join(tokens) for tokens in list(output[output.cluster == i].essay)]\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=50)\n",
    "    tfidf_vectorizer.fit(context)\n",
    "    \n",
    "    #terms_per_essays[i] = [term for term in tfidf_vectorizer.get_feature_names() if term not in english_stopwords + custom_stopwords]\n",
    "    terms_per_essays[i] = [term for term in tfidf_vectorizer.get_feature_names()]\n",
    "    \n",
    "    print(\"Top terms #{} : {}\".format(i, terms_per_essays[i]))\n",
    "    print()\n",
    "    \n",
    "    themes_per_essays[i] = set()\n",
    "    theme_term_count = {}\n",
    "    for term in terms_per_essays[i]:\n",
    "        for key, value in theme_index.items():\n",
    "            if term in value:\n",
    "                if key in theme_term_count:\n",
    "                    theme_term_count[key] += 1\n",
    "                else:\n",
    "                    theme_term_count[key] = 1\n",
    "                themes_per_essays[i].add(key)\n",
    "    \n",
    "    themes_per_essays[i] = [theme for theme, count in theme_term_count.items() if count > 1]\n",
    "    print(\"Dominant theme(s): {}\".format(themes_per_essays[i]))\n",
    "    print()\n",
    "    print(sorted(theme_term_count.items(), key=lambda x : x[1])[::-1])\n",
    "    \n",
    "    print()\n",
    "    print(\"Randomly selected essays:\", ten)\n",
    "    files = [i for i in os.listdir(src) if i in ten and path.isfile(path.join(src, i))]\n",
    "    for f in files:\n",
    "        shutil.copy(path.join(src, f), dst + str(i))\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA on each cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "terms_per_cluster = {}\n",
    "themes_per_cluster = {}\n",
    "\n",
    "#Apply LDA for each cluster (and for each essay in cluster)\n",
    "for i in range(num_clusters):\n",
    "    print(\"************ Cluster # {} ************\".format(i))\n",
    "    essays_in_cluster = [tokens for tokens in list(output[output.cluster == i].essay)]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(essays_in_cluster)\n",
    "    \n",
    "    cluster_corpus = [dictionary.doc2bow(essay) for essay in essays_in_cluster]\n",
    "    \n",
    "    lda = models.ldamodel.LdaModel(corpus=cluster_corpus, id2word=dictionary, num_topics=1, passes=10)\n",
    "    \n",
    "    term_score = {}\n",
    "    terms_per_cluster[i] = []\n",
    "    for idx, terms in lda.print_topics(i, 20):\n",
    "        #terms_per_cluster[i] = terms\n",
    "        \n",
    "        print('Top terms: {}'.format(terms.split('+')))\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        for term_with_score in terms.split('+'):\n",
    "            term = term_with_score.split('*')[1][1:-2]\n",
    "            score = term_with_score.split('*')[0]\n",
    "            \n",
    "            #print(\"term is {} with score {}\".format(term, score))\n",
    "            \n",
    "            terms_per_cluster[i].append(term)\n",
    "            term_score[term] = float(score)\n",
    "    \n",
    "    print(\"Terms per cluster:\", terms_per_cluster)\n",
    "    print()\n",
    "    print(\"Terms/Scores:\", term_score)\n",
    "    print()\n",
    "    \n",
    "    themes_per_cluster[i] = {}\n",
    "    theme_term_score = {}\n",
    "    for term in terms_per_cluster[i]:\n",
    "        for theme, defining_terms in theme_index.items():\n",
    "            if term in defining_terms:\n",
    "                if theme in theme_term_score:\n",
    "                    theme_term_score[theme] += term_score[term]\n",
    "                else:\n",
    "                    theme_term_score[theme] = term_score[term]\n",
    "                \n",
    "    themes_per_cluster[i] = sorted(theme_term_score.items(), key = lambda x : x[1])[::-1]\n",
    "    print(\"Themes ranked strongest to weakest: {}\".format(themes_per_cluster[i]))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model/Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(km,'doc_cluster.pkl')\n",
    "\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize cluster (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "\n",
    "#%matplotlib inline\n",
    "#plt.figure\n",
    "\n",
    "#cluster_colors = []\n",
    "#for i in range(num_clusters):\n",
    "#    r = lambda: random.randint(0,255)\n",
    "#    cluster_colors.append('#%02X%02X%02X' % (r(),r(),r()))\n",
    "\n",
    "#color = [i for i in cluster_colors]\n",
    "#plt.scatter(datapoint[:, 0], datapoint[:, 1])\n",
    "#centroids = kmeans_model.cluster_centers_\n",
    "#centroidpoint = pca.transform(centroids)\n",
    "#plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker=\"^\", s=150, c=\"#000000\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
