{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import csv\n",
    "import gensim\n",
    "import logging\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Essays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_path = 'essays/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(essay_path)\n",
    "\n",
    "essays = {}\n",
    "for file in files:\n",
    "    # attempt to confidently guess encoding; otherwise, default to ISO-8859-1\n",
    "    encoding = \"ISO-8859-1\"\n",
    "    guess = chardet.detect(open(essay_path + file, \"rb\").read())\n",
    "    if (guess[\"confidence\"] >= 0.95):\n",
    "        encoding = guess[\"encoding\"]\n",
    "    \n",
    "    with open(essay_path + file, \"r\", encoding=encoding) as f:\n",
    "        essays[file] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize + Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_essays = {label: gensim.utils.simple_preprocess(corpus, deacc=True, min_len=2, max_len=15) for (label, corpus) in essays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "custom_stopwords = [\n",
    "        \"prison\",\n",
    "        \"prisoner\",\n",
    "        \"also\",\n",
    "        \"said\",\n",
    "        \"would\",\n",
    "        \"could\",\n",
    "        \"should\",\n",
    "        \"first\",\n",
    "        \"like\",\n",
    "        \"get\",\n",
    "        \"going\",\n",
    "        \"thing\",\n",
    "        \"something\",\n",
    "        \"use\",\n",
    "        \"get\",\n",
    "        \"go\",\n",
    "        \"one\"\n",
    "    ]\n",
    "\n",
    "tokenized_essays = {label: [w for w in token_lst if w not in english_stopwords and w not in custom_stopwords] for (label, token_lst) in tokenized_essays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokenized_essays = {label: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in token_lst if w not in string.punctuation] for (label, token_lst) in tokenized_essays.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize w/ doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts processed:  1573\n"
     ]
    }
   ],
   "source": [
    "LabeledSentence1 = gensim.models.doc2vec.TaggedDocument\n",
    "all_content_train = []\n",
    "j=0\n",
    "for essay in tokenized_essays.values():\n",
    "    all_content_train.append(LabeledSentence1(essay, [j]))\n",
    "    j+=1\n",
    "    \n",
    "print(\"Number of texts processed: \", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(all_content_train, vector_size = 100, window = 10, min_count = 500, workers=7, dm = 1,alpha=0.025, min_alpha=0.001)\n",
    "d2v_model.train(all_content_train, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=-0.016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_vectors = d2v_model.docvecs.vectors_docs\n",
    "vectorized_df = pd.DataFrame(essay_vectors)\n",
    "index_ref = vectorized_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling through standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsclr = StandardScaler()\n",
    "standardized_df = pd.DataFrame(stdsclr.fit_transform(vectorized_df.astype(float)), index=index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "#reduced_df = pd.DataFrame(reduced, index=index_ref, columns = ['title', 'cluster', 'essay'])\n",
    "reduced_df = pd.DataFrame(pca.fit_transform(standardized_df), index=index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide for output to visualize effectiveness of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_df.to_csv('new.csv', sep='\\t', index=False, header=False)\n",
    "#pd.DataFrame(index_ref).to_csv('index.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering w/ k-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 215 ms, sys: 5.75 ms, total: 220 ms\n",
      "Wall time: 240 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=12, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 12\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, init=\"k-means++\", max_iter=100)\n",
    "\n",
    "%time km.fit(reduced_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     583\n",
      "1     222\n",
      "5     200\n",
      "9     158\n",
      "2     149\n",
      "3      57\n",
      "4      48\n",
      "6      40\n",
      "7      36\n",
      "8      29\n",
      "11     28\n",
      "10     23\n",
      "Name: cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "output = reduced_df\n",
    "output['cluster'] = km.labels_\n",
    "print(output['cluster'].value_counts())\n",
    "output['essay'] = tokenized_essays.values()\n",
    "output['title'] = tokenized_essays.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_index = {'Autobiographical, Paths to Prison': 'socioeconomic, parent, damage, abuse, gang, alcohol, drug', \n",
    "                 'Family': 'family, abandonment, relation, visit, partner, mother, father, sibling', \n",
    "                 'Physical Conditions and Security': 'physical, condition, security, search, censorship, food, cold, hygiene, heat, misfunction, infestation, solitary', \n",
    "                 'Prison Culture/Community/Society': 'violence, fear, staff, sexual, crime, outcasts, racial, cellmate, gay, LGBTQ, dehumanize, uniform',\n",
    "                 'Staff/prison Abuse of IP': 'abuse, sexual, torture, humiliation, racist, assault, antagonism, exacerbation, right, violation, food, hygiene, environment, legal',\n",
    "                 'Personal/Intern Change/Copin': 'survival, art, reading, writing, peace, faith, prayer, meditation, practice, community, activities, hobbies, cooking, remorse, motivation, education, discipline, coping, adjustment',\n",
    "                 'Judicial Misconduct and Legal Remediation': 'judicial, incompetence, corruption, witness, evidence, excessive, political, jailhouse, lawyer',\n",
    "                 'Political and Intellectual Labor among IP': 'activism, resistence, critique, race, class, change, policies, practices',\n",
    "                 'Prison Industry/Prison as Business': 'labor, slave, condition, safety, health',\n",
    "                 'Education, Re-entry, Other Programs': 'rehabilitation, re-entry, education, indifference',\n",
    "                 'Health Care': 'health, care, negligence, hostility, incompetence, indifference, death, injury, treatment, medication',\n",
    "                 'Social Alienation, Indifference, Hostility': 'public, mispercetion, identity, stigma'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** 10 random sample essays per cluster *****\n",
      "\n",
      "** Cluster 0: **\n",
      "\n",
      "Topic #0 : ['back', 'come', 'crime', 'day', 'even', 'give', 'inmate', 'know', 'law', 'life', 'make', 'many', 'need', 'never', 'people', 'right', 'say', 'see', 'sentence', 'state', 'system', 'take', 'time', 'want', 'way', 'well', 'work', 'write', 'year']\n",
      "\n",
      "Theme(s): {'Prison Culture/Community/Society', 'Staff/prison Abuse of IP', 'Judicial Misconduct and Legal Remediation'}\n",
      "\n",
      "['essay_775.txt', 'essay_1329.txt', 'essay_950.txt', 'essay_1121.txt', 'essay_470.txt', 'essay_1365.txt', 'essay_504.txt', 'essay_83.txt', 'essay_1416.txt', 'essay_1154.txt', 'essay_264.txt', 'essay_1083.txt', 'essay_1138.txt', 'essay_1243.txt', 'essay_1239.txt', 'essay_1410.txt', 'essay_718.txt', 'essay_1560.txt', 'essay_1283.txt', 'essay_1422.txt']\n",
      "\n",
      "\n",
      "** Cluster 1: **\n",
      "\n",
      "Topic #1 : ['american', 'court', 'crime', 'criminal', 'even', 'incarcerate', 'inmate', 'justice', 'law', 'life', 'make', 'many', 'need', 'parole', 'people', 'program', 'right', 'see', 'sentence', 'society', 'state', 'system', 'take', 'time', 'way', 'well', 'work', 'year']\n",
      "\n",
      "Theme(s): {'Prison Culture/Community/Society', 'Staff/prison Abuse of IP', 'Judicial Misconduct and Legal Remediation'}\n",
      "\n",
      "['essay_902.txt', 'essay_413.txt', 'essay_756.txt', 'essay_1322.txt', 'essay_1468.txt', 'essay_631.txt', 'essay_269.txt', 'essay_1037.txt', 'essay_381.txt', 'essay_191.txt', 'essay_24.txt', 'essay_475.txt', 'essay_608.txt', 'essay_1109.txt', 'essay_481.txt', 'essay_543.txt', 'essay_796.txt', 'essay_1109.txt', 'essay_95.txt', 'essay_977.txt']\n",
      "\n",
      "\n",
      "** Cluster 2: **\n",
      "\n",
      "Topic #2 : ['call', 'case', 'cell', 'correction', 'correctional', 'court', 'day', 'even', 'facility', 'give', 'inmate', 'law', 'life', 'make', 'many', 'officer', 'people', 'program', 'right', 'staff', 'state', 'system', 'take', 'time', 'well', 'work', 'write', 'year']\n",
      "\n",
      "Theme(s): {'Prison Culture/Community/Society', 'Staff/prison Abuse of IP', 'Judicial Misconduct and Legal Remediation'}\n",
      "\n",
      "['essay_1474.txt', 'essay_526.txt', 'essay_658.txt', 'essay_874.txt', 'essay_5.txt', 'essay_526.txt', 'essay_298.txt', 'essay_132.txt', 'essay_1304.txt', 'essay_1108.txt', 'essay_1022.txt', 'essay_315.txt', 'essay_619.txt', 'essay_209.txt', 'essay_959.txt', 'essay_681.txt', 'essay_813.txt', 'essay_177.txt', 'essay_1041.txt', 'essay_1159.txt']\n",
      "\n",
      "\n",
      "** Cluster 3: **\n",
      "\n",
      "Topic #3 : ['back', 'come', 'day', 'even', 'feel', 'give', 'god', 'know', 'life', 'look', 'love', 'make', 'many', 'mother', 'much', 'never', 'people', 'place', 'say', 'see', 'still', 'take', 'think', 'time', 'want', 'way', 'well', 'year']\n",
      "\n",
      "Theme(s): {'Family'}\n",
      "\n",
      "['essay_461.txt', 'essay_1403.txt', 'essay_357.txt', 'essay_514.txt', 'essay_1522.txt', 'essay_369.txt', 'essay_517.txt', 'essay_46.txt', 'essay_596.txt', 'essay_461.txt', 'essay_357.txt', 'essay_183.txt', 'essay_1049.txt', 'essay_1363.txt', 'essay_858.txt', 'essay_461.txt', 'essay_1495.txt', 'essay_1518.txt', 'essay_183.txt', 'essay_360.txt']\n",
      "\n",
      "\n",
      "** Cluster 4: **\n",
      "\n",
      "Topic #4 : ['behavior', 'black', 'change', 'criminal', 'even', 'inmate', 'law', 'life', 'make', 'many', 'may', 'men', 'must', 'need', 'people', 'program', 'right', 'see', 'social', 'society', 'state', 'system', 'take', 'time', 'way', 'well', 'within', 'work', 'world']\n",
      "\n",
      "Theme(s): {'Health Care', 'Political and Intellectual Labor among IP', 'Family', 'Judicial Misconduct and Legal Remediation', 'Staff/prison Abuse of IP', 'Personal/Intern Change/Copin'}\n",
      "\n",
      "['essay_577.txt', 'essay_167.txt', 'essay_442.txt', 'essay_601.txt', 'essay_212.txt', 'essay_212.txt', 'essay_68.txt', 'essay_1349.txt', 'essay_210.txt', 'essay_1033.txt', 'essay_891.txt', 'essay_210.txt', 'essay_64.txt', 'essay_598.txt', 'essay_115.txt', 'essay_446.txt', 'essay_1551.txt', 'essay_378.txt', 'essay_446.txt', 'essay_143.txt']\n",
      "\n",
      "\n",
      "** Cluster 5: **\n",
      "\n",
      "Topic #5 : ['back', 'call', 'cell', 'come', 'day', 'even', 'give', 'guy', 'inmate', 'know', 'life', 'look', 'make', 'man', 'many', 'never', 'officer', 'people', 'say', 'see', 'take', 'time', 'try', 'two', 'want', 'way', 'well', 'write', 'year']\n",
      "\n",
      "Theme(s): {'Prison Culture/Community/Society', 'Education, Re-entry, Other Programs'}\n",
      "\n",
      "['essay_324.txt', 'essay_677.txt', 'essay_322.txt', 'essay_190.txt', 'essay_1227.txt', 'essay_125.txt', 'essay_1486.txt', 'essay_666.txt', 'essay_1355.txt', 'essay_1063.txt', 'essay_190.txt', 'essay_802.txt', 'essay_1547.txt', 'essay_1222.txt', 'essay_1399.txt', 'essay_339.txt', 'essay_1521.txt', 'essay_1459.txt', 'essay_1252.txt', 'essay_344.txt']\n",
      "\n",
      "\n",
      "** Cluster 6: **\n",
      "\n",
      "Topic #6 : ['case', 'child', 'court', 'crime', 'criminal', 'even', 'family', 'federal', 'incarceration', 'inmate', 'justice', 'law', 'life', 'make', 'many', 'offender', 'parole', 'people', 'program', 'public', 'rate', 'release', 'sentence', 'society', 'state', 'system', 'time', 'without', 'year']\n",
      "\n",
      "Theme(s): {'Prison Culture/Community/Society', 'Family', 'Judicial Misconduct and Legal Remediation', 'Social Alienation, Indifference, Hostility'}\n",
      "\n",
      "['essay_630.txt', 'essay_1005.txt', 'essay_1554.txt', 'essay_393.txt', 'essay_630.txt', 'essay_1490.txt', 'essay_812.txt', 'essay_1554.txt', 'essay_19.txt', 'essay_630.txt', 'essay_384.txt', 'essay_630.txt', 'essay_1506.txt', 'essay_75.txt', 'essay_1395.txt', 'essay_1111.txt', 'essay_483.txt', 'essay_1005.txt', 'essay_26.txt', 'essay_131.txt']\n",
      "\n",
      "\n",
      "** Cluster 7: **\n",
      "\n",
      "Topic #7 : ['ask', 'back', 'call', 'cell', 'come', 'day', 'door', 'even', 'give', 'guard', 'guy', 'inmate', 'know', 'life', 'look', 'make', 'never', 'room', 'say', 'see', 'take', 'time', 'told', 'two', 'want', 'way', 'work', 'year']\n",
      "\n",
      "Theme(s): {'Prison Culture/Community/Society'}\n",
      "\n",
      "['essay_616.txt', 'essay_10.txt', 'essay_614.txt', 'essay_2.txt', 'essay_1134.txt', 'essay_614.txt', 'essay_942.txt', 'essay_1225.txt', 'essay_1284.txt', 'essay_998.txt', 'essay_1.txt', 'essay_1219.txt', 'essay_616.txt', 'essay_242.txt', 'essay_1024.txt', 'essay_1.txt', 'essay_831.txt', 'essay_259.txt', 'essay_777.txt', 'essay_880.txt']\n",
      "\n",
      "\n",
      "** Cluster 8: **\n",
      "\n",
      "Topic #8 : ['another', 'back', 'cell', 'come', 'court', 'day', 'give', 'guard', 'inmate', 'know', 'make', 'many', 'officer', 'official', 'order', 'place', 'right', 'see', 'staff', 'state', 'system', 'take', 'time', 'two', 'unit', 'well', 'yard', 'year']\n",
      "\n",
      "Theme(s): {'Prison Culture/Community/Society', 'Staff/prison Abuse of IP', 'Personal/Intern Change/Copin'}\n",
      "\n",
      "['essay_175.txt', 'essay_1505.txt', 'essay_849.txt', 'essay_1198.txt', 'essay_510.txt', 'essay_1491.txt', 'essay_997.txt', 'essay_175.txt', 'essay_694.txt', 'essay_849.txt', 'essay_1509.txt', 'essay_1015.txt', 'essay_1405.txt', 'essay_849.txt', 'essay_849.txt', 'essay_1405.txt', 'essay_303.txt', 'essay_1493.txt', 'essay_694.txt', 'essay_1505.txt']\n",
      "\n",
      "\n",
      "** Cluster 9: **\n",
      "\n",
      "Topic #9 : ['back', 'become', 'change', 'come', 'day', 'even', 'family', 'give', 'know', 'life', 'love', 'make', 'man', 'many', 'mind', 'much', 'need', 'never', 'people', 'person', 'see', 'self', 'society', 'take', 'think', 'time', 'way', 'well', 'world', 'year']\n",
      "\n",
      "Theme(s): {'Political and Intellectual Labor among IP', 'Family', 'Prison Culture/Community/Society'}\n",
      "\n",
      "['essay_308.txt', 'essay_1404.txt', 'essay_169.txt', 'essay_145.txt', 'essay_451.txt', 'essay_1342.txt', 'essay_708.txt', 'essay_130.txt', 'essay_907.txt', 'essay_1000.txt', 'essay_856.txt', 'essay_61.txt', 'essay_30.txt', 'essay_579.txt', 'essay_786.txt', 'essay_611.txt', 'essay_471.txt', 'essay_448.txt', 'essay_1404.txt', 'essay_1241.txt']\n",
      "\n",
      "\n",
      "** Cluster 10: **\n",
      "\n",
      "Topic #10 : ['appear', 'become', 'body', 'come', 'consciousness', 'even', 'experience', 'inside', 'know', 'learn', 'life', 'look', 'make', 'mind', 'need', 'people', 'plan', 'see', 'seem', 'society', 'state', 'take', 'think', 'time', 'way', 'well', 'work', 'world', 'write', 'year']\n",
      "\n",
      "Theme(s): set()\n",
      "\n",
      "['essay_118.txt', 'essay_108.txt', 'essay_1236.txt', 'essay_165.txt', 'essay_597.txt', 'essay_116.txt', 'essay_965.txt', 'essay_113.txt', 'essay_110.txt', 'essay_113.txt', 'essay_457.txt', 'essay_1235.txt', 'essay_597.txt', 'essay_113.txt', 'essay_164.txt', 'essay_118.txt', 'essay_116.txt', 'essay_1235.txt', 'essay_111.txt', 'essay_1236.txt']\n",
      "\n",
      "\n",
      "** Cluster 11: **\n",
      "\n",
      "Topic #11 : ['back', 'case', 'child', 'come', 'court', 'crime', 'day', 'family', 'good', 'inmate', 'jail', 'know', 'law', 'life', 'make', 'man', 'need', 'new', 'offender', 'people', 'right', 'sentence', 'sex', 'state', 'take', 'time', 'well', 'work', 'year']\n",
      "\n",
      "Theme(s): {'Staff/prison Abuse of IP', 'Prison Culture/Community/Society', 'Family', 'Judicial Misconduct and Legal Remediation'}\n",
      "\n",
      "['essay_254.txt', 'essay_807.txt', 'essay_808.txt', 'essay_525.txt', 'essay_809.txt', 'essay_811.txt', 'essay_655.txt', 'essay_808.txt', 'essay_1390.txt', 'essay_915.txt', 'essay_665.txt', 'essay_811.txt', 'essay_809.txt', 'essay_1014.txt', 'essay_1214.txt', 'essay_810.txt', 'essay_525.txt', 'essay_219.txt', 'essay_1106.txt', 'essay_176.txt']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "essays_per_cluster = {}\n",
    "for i in range(num_clusters):\n",
    "    essays_per_cluster[i] = []\n",
    "    #print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in output[output.cluster == i]['title'].values.tolist():\n",
    "        #print(' %s,' % title, end='')\n",
    "        essays_per_cluster[i].append(title)\n",
    "        \n",
    "print(\"***** 10 random sample essays per cluster *****\")\n",
    "print()\n",
    "\n",
    "terms_per_essays = {}\n",
    "themes_per_essays = {}\n",
    "for i in range(num_clusters):\n",
    "    ten = [random.choice(essays_per_cluster[i]) for _ in range(20)]\n",
    "    print(\"** Cluster %d: **\" % i)\n",
    "    print()\n",
    "    context = [' '.join(tokens) for tokens in list(output[output.cluster == i].essay)]\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=30)\n",
    "    tfidf_vectorizer.fit(context)\n",
    "    \n",
    "    terms_per_essays[i] = [term for term in tfidf_vectorizer.get_feature_names() if term not in english_stopwords + custom_stopwords]\n",
    "    \n",
    "    print(\"Topic #{} : {}\".format(i , terms_per_essays[i]))\n",
    "    print()\n",
    "    \n",
    "    themes_per_essays[i] = set()\n",
    "    for term in terms_per_essays[i]:\n",
    "        #print(terms_per_essays[i])\n",
    "        for key, value in theme_index.items():\n",
    "            if term in value and key not in themes_per_essays[i]:\n",
    "                #print(term, value)\n",
    "                themes_per_essays[i].add(key)\n",
    "    \n",
    "    print(\"Theme(s): {}\".format(themes_per_essays[i]))\n",
    "    \n",
    "    print()\n",
    "    print(ten)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model/Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(kmeans_model,'doc_cluster.pkl')\n",
    "\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "clusters = kmeans_model.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top terms per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_clusters) :\n",
    "    print(\"\\n\\n\")\n",
    "    context = [' '.join(tokens) for tokens in list(output[output.cluster == i].essay)]\n",
    "    \n",
    "    m1 =TfidfVectorizer(max_features=30)\n",
    "    m1.fit(context)\n",
    "    print(\"Topic #{} : {}\".format(i , \" , \".join(term for term in m1.get_feature_names() if term not in combined_stopwords)))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize cluster (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "\n",
    "#%matplotlib inline\n",
    "#plt.figure\n",
    "\n",
    "#cluster_colors = []\n",
    "#for i in range(num_clusters):\n",
    "#    r = lambda: random.randint(0,255)\n",
    "#    cluster_colors.append('#%02X%02X%02X' % (r(),r(),r()))\n",
    "\n",
    "#color = [i for i in cluster_colors]\n",
    "#plt.scatter(datapoint[:, 0], datapoint[:, 1])\n",
    "#centroids = kmeans_model.cluster_centers_\n",
    "#centroidpoint = pca.transform(centroids)\n",
    "#plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker=\"^\", s=150, c=\"#000000\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
