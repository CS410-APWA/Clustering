{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APWA Latent Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import csv\n",
    "import gensim\n",
    "import logging\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load essays into hash table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.dirname(os.path.realpath('__file__'))\n",
    "essay_path = root + '/../essays/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(essay_path)\n",
    "\n",
    "essays = {}\n",
    "for file in files:\n",
    "    # attempt to confidently guess encoding; otherwise, default to ISO-8859-1\n",
    "    encoding = \"ISO-8859-1\"\n",
    "    guess = chardet.detect(open(essay_path + file, \"rb\").read())\n",
    "    if (guess[\"confidence\"] >= 0.95):\n",
    "        encoding = guess[\"encoding\"]\n",
    "    \n",
    "    with open(essay_path + file, \"r\", encoding=encoding) as f:\n",
    "        essays[file] = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "tokenized_essays = {label: gensim.utils.simple_preprocess(corpus, deacc=True, min_len=2, max_len=20) for (label, corpus) in essays.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokenized_essays = {label: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in token_lst if w not in string.punctuation] for (label, token_lst) in tokenized_essays.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "custom_stopwords = open(\"custom_stopwords.txt\", \"r\").read().splitlines()\n",
    "tokenized_essays = {label: [w for w in token_lst if (w not in english_stopwords and w not in custom_stopwords)] for (label, token_lst) in tokenized_essays.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:11:37,658 : INFO : collecting all words and their counts\n",
      "2019-10-28 19:11:37,660 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-10-28 19:11:38,173 : INFO : collected 57681 word types and 1573 unique tags from a corpus of 1573 examples and 963713 words\n",
      "2019-10-28 19:11:38,174 : INFO : Loading a fresh vocabulary\n",
      "2019-10-28 19:11:38,583 : INFO : effective_min_count=5 retains 13711 unique words (23% of original 57681, drops 43970)\n",
      "2019-10-28 19:11:38,584 : INFO : effective_min_count=5 leaves 902048 word corpus (93% of original 963713, drops 61665)\n",
      "2019-10-28 19:11:38,645 : INFO : deleting the raw counts dictionary of 57681 items\n",
      "2019-10-28 19:11:38,650 : INFO : sample=0.001 downsamples 9 most-common words\n",
      "2019-10-28 19:11:38,650 : INFO : downsampling leaves estimated 896167 word corpus (99.3% of prior 902048)\n",
      "2019-10-28 19:11:38,762 : INFO : estimated required memory for 13711 words and 100 dimensions: 18453500 bytes\n",
      "2019-10-28 19:11:38,763 : INFO : resetting layer weights\n",
      "2019-10-28 19:11:42,290 : INFO : training model with 3 workers on 13711 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-10-28 19:11:43,301 : INFO : EPOCH 1 - PROGRESS: at 78.77% examples, 706462 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-28 19:11:43,581 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-28 19:11:43,600 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-28 19:11:43,608 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-28 19:11:43,609 : INFO : EPOCH - 1 : training on 963713 raw words (897779 effective words) took 1.3s, 682037 effective words/s\n",
      "2019-10-28 19:11:44,621 : INFO : EPOCH 2 - PROGRESS: at 73.17% examples, 663878 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-28 19:11:44,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-28 19:11:44,935 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-28 19:11:44,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-28 19:11:44,938 : INFO : EPOCH - 2 : training on 963713 raw words (897760 effective words) took 1.3s, 680552 effective words/s\n",
      "2019-10-28 19:11:45,945 : INFO : EPOCH 3 - PROGRESS: at 73.17% examples, 664730 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-28 19:11:46,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-28 19:11:46,277 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-28 19:11:46,278 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-28 19:11:46,279 : INFO : EPOCH - 3 : training on 963713 raw words (897679 effective words) took 1.3s, 672211 effective words/s\n",
      "2019-10-28 19:11:47,306 : INFO : EPOCH 4 - PROGRESS: at 83.79% examples, 739826 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-28 19:11:47,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-28 19:11:47,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-28 19:11:47,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-28 19:11:47,511 : INFO : EPOCH - 4 : training on 963713 raw words (897692 effective words) took 1.2s, 731358 effective words/s\n",
      "2019-10-28 19:11:48,522 : INFO : EPOCH 5 - PROGRESS: at 77.43% examples, 697774 words/s, in_qsize 5, out_qsize 0\n",
      "2019-10-28 19:11:48,812 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-10-28 19:11:48,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-10-28 19:11:48,821 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-10-28 19:11:48,823 : INFO : EPOCH - 5 : training on 963713 raw words (897814 effective words) took 1.3s, 686395 effective words/s\n",
      "2019-10-28 19:11:48,824 : INFO : training on a 4818565 raw words (4488724 effective words) took 6.5s, 687060 effective words/s\n"
     ]
    }
   ],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_essays.values())]\n",
    "d2v_model = Doc2Vec(documents, vector_size=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA before clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 21:17:09,853 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-28 21:17:11,239 : INFO : built Dictionary(57681 unique tokens: ['aggressive', 'air', 'anger', 'anyone', 'anything']...) from 1573 documents (total 963713 corpus positions)\n",
      "2019-10-28 21:17:12,314 : INFO : using symmetric alpha at 0.09090909090909091\n",
      "2019-10-28 21:17:12,316 : INFO : using symmetric eta at 0.09090909090909091\n",
      "2019-10-28 21:17:12,328 : INFO : using serial LDA version on this node\n",
      "2019-10-28 21:17:12,363 : INFO : running online (multi-pass) LDA training, 11 topics, 10 passes over the supplied corpus of 1573 documents, updating model once every 1573 documents, evaluating perplexity every 1573 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-28 21:17:17,445 : INFO : -10.446 per-word bound, 1395.0 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:17:17,446 : INFO : PROGRESS: pass 0, at document #1573/1573\n",
      "2019-10-28 21:17:19,451 : INFO : topic #3 (0.091): 0.007*\"state\" + 0.003*\"system\" + 0.003*\"help\" + 0.003*\"feel\" + 0.003*\"think\" + 0.003*\"mind\" + 0.003*\"inmate\" + 0.003*\"right\" + 0.003*\"law\" + 0.002*\"write\"\n",
      "2019-10-28 21:17:19,453 : INFO : topic #6 (0.091): 0.006*\"state\" + 0.005*\"crime\" + 0.004*\"inmate\" + 0.003*\"love\" + 0.003*\"sentence\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"law\" + 0.003*\"system\" + 0.002*\"society\"\n",
      "2019-10-28 21:17:19,454 : INFO : topic #1 (0.091): 0.012*\"inmate\" + 0.007*\"state\" + 0.004*\"officer\" + 0.004*\"law\" + 0.003*\"system\" + 0.003*\"court\" + 0.003*\"crime\" + 0.003*\"right\" + 0.003*\"sentence\" + 0.003*\"society\"\n",
      "2019-10-28 21:17:19,456 : INFO : topic #8 (0.091): 0.005*\"state\" + 0.005*\"inmate\" + 0.005*\"system\" + 0.004*\"program\" + 0.003*\"officer\" + 0.003*\"right\" + 0.003*\"law\" + 0.003*\"write\" + 0.002*\"society\" + 0.002*\"criminal\"\n",
      "2019-10-28 21:17:19,459 : INFO : topic #2 (0.091): 0.007*\"inmate\" + 0.004*\"state\" + 0.004*\"write\" + 0.004*\"right\" + 0.003*\"think\" + 0.003*\"staff\" + 0.003*\"officer\" + 0.003*\"crime\" + 0.003*\"mind\" + 0.003*\"family\"\n",
      "2019-10-28 21:17:19,461 : INFO : topic diff=3.235165, rho=1.000000\n",
      "2019-10-28 21:17:23,416 : INFO : -8.437 per-word bound, 346.5 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:17:23,416 : INFO : PROGRESS: pass 1, at document #1573/1573\n",
      "2019-10-28 21:17:25,347 : INFO : topic #5 (0.091): 0.006*\"law\" + 0.006*\"state\" + 0.005*\"right\" + 0.005*\"society\" + 0.005*\"system\" + 0.005*\"sentence\" + 0.004*\"parole\" + 0.004*\"human\" + 0.003*\"crime\" + 0.003*\"think\"\n",
      "2019-10-28 21:17:25,349 : INFO : topic #2 (0.091): 0.009*\"inmate\" + 0.005*\"write\" + 0.004*\"officer\" + 0.004*\"staff\" + 0.004*\"right\" + 0.003*\"think\" + 0.003*\"family\" + 0.003*\"mind\" + 0.003*\"state\" + 0.002*\"help\"\n",
      "2019-10-28 21:17:25,351 : INFO : topic #9 (0.091): 0.006*\"state\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.005*\"inmate\" + 0.005*\"system\" + 0.004*\"parole\" + 0.004*\"family\" + 0.004*\"program\" + 0.004*\"court\" + 0.004*\"write\"\n",
      "2019-10-28 21:17:25,353 : INFO : topic #10 (0.091): 0.007*\"inmate\" + 0.006*\"state\" + 0.005*\"black\" + 0.004*\"society\" + 0.004*\"program\" + 0.003*\"right\" + 0.003*\"law\" + 0.003*\"experience\" + 0.003*\"crime\" + 0.003*\"system\"\n",
      "2019-10-28 21:17:25,355 : INFO : topic #4 (0.091): 0.004*\"state\" + 0.004*\"right\" + 0.004*\"system\" + 0.004*\"crime\" + 0.004*\"sentence\" + 0.004*\"law\" + 0.004*\"criminal\" + 0.003*\"black\" + 0.003*\"justice\" + 0.003*\"write\"\n",
      "2019-10-28 21:17:25,357 : INFO : topic diff=0.936819, rho=0.577350\n",
      "2019-10-28 21:17:29,248 : INFO : -8.311 per-word bound, 317.6 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:17:29,249 : INFO : PROGRESS: pass 2, at document #1573/1573\n",
      "2019-10-28 21:17:31,162 : INFO : topic #4 (0.091): 0.004*\"right\" + 0.004*\"police\" + 0.004*\"state\" + 0.004*\"criminal\" + 0.004*\"black\" + 0.004*\"system\" + 0.004*\"law\" + 0.004*\"justice\" + 0.004*\"crime\" + 0.003*\"write\"\n",
      "2019-10-28 21:17:31,164 : INFO : topic #0 (0.091): 0.011*\"state\" + 0.006*\"death\" + 0.005*\"sentence\" + 0.005*\"system\" + 0.004*\"law\" + 0.004*\"court\" + 0.004*\"crime\" + 0.003*\"society\" + 0.003*\"right\" + 0.003*\"justice\"\n",
      "2019-10-28 21:17:31,167 : INFO : topic #10 (0.091): 0.007*\"black\" + 0.006*\"state\" + 0.006*\"inmate\" + 0.004*\"society\" + 0.004*\"experience\" + 0.004*\"program\" + 0.003*\"american\" + 0.003*\"body\" + 0.003*\"system\" + 0.003*\"right\"\n",
      "2019-10-28 21:17:31,168 : INFO : topic #7 (0.091): 0.007*\"state\" + 0.007*\"system\" + 0.005*\"law\" + 0.004*\"right\" + 0.004*\"inmate\" + 0.003*\"society\" + 0.003*\"black\" + 0.003*\"criminal\" + 0.003*\"program\" + 0.003*\"think\"\n",
      "2019-10-28 21:17:31,171 : INFO : topic #6 (0.091): 0.006*\"mother\" + 0.005*\"love\" + 0.005*\"state\" + 0.004*\"god\" + 0.004*\"family\" + 0.004*\"crime\" + 0.003*\"father\" + 0.003*\"child\" + 0.003*\"home\" + 0.003*\"help\"\n",
      "2019-10-28 21:17:31,172 : INFO : topic diff=0.802482, rho=0.500000\n",
      "2019-10-28 21:17:35,118 : INFO : -8.232 per-word bound, 300.7 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:17:35,119 : INFO : PROGRESS: pass 3, at document #1573/1573\n",
      "2019-10-28 21:17:37,004 : INFO : topic #4 (0.091): 0.005*\"police\" + 0.004*\"right\" + 0.004*\"criminal\" + 0.004*\"state\" + 0.004*\"think\" + 0.004*\"write\" + 0.004*\"law\" + 0.004*\"plan\" + 0.004*\"black\" + 0.004*\"society\"\n",
      "2019-10-28 21:17:37,005 : INFO : topic #5 (0.091): 0.008*\"society\" + 0.007*\"law\" + 0.006*\"system\" + 0.006*\"state\" + 0.005*\"right\" + 0.005*\"human\" + 0.004*\"parole\" + 0.004*\"crime\" + 0.004*\"sentence\" + 0.004*\"process\"\n",
      "2019-10-28 21:17:37,008 : INFO : topic #2 (0.091): 0.011*\"inmate\" + 0.006*\"write\" + 0.005*\"officer\" + 0.004*\"staff\" + 0.004*\"right\" + 0.004*\"god\" + 0.004*\"think\" + 0.003*\"help\" + 0.003*\"love\" + 0.003*\"family\"\n",
      "2019-10-28 21:17:37,009 : INFO : topic #7 (0.091): 0.007*\"state\" + 0.007*\"system\" + 0.005*\"law\" + 0.004*\"right\" + 0.003*\"inmate\" + 0.003*\"staff\" + 0.003*\"criminal\" + 0.003*\"black\" + 0.003*\"program\" + 0.003*\"society\"\n",
      "2019-10-28 21:17:37,011 : INFO : topic #9 (0.091): 0.008*\"sentence\" + 0.007*\"crime\" + 0.007*\"state\" + 0.006*\"program\" + 0.005*\"parole\" + 0.005*\"offender\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.005*\"family\" + 0.004*\"law\"\n",
      "2019-10-28 21:17:37,013 : INFO : topic diff=0.628950, rho=0.447214\n",
      "2019-10-28 21:17:40,875 : INFO : -8.186 per-word bound, 291.3 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:17:40,875 : INFO : PROGRESS: pass 4, at document #1573/1573\n",
      "2019-10-28 21:17:42,777 : INFO : topic #4 (0.091): 0.005*\"police\" + 0.004*\"think\" + 0.004*\"plan\" + 0.004*\"write\" + 0.004*\"right\" + 0.004*\"criminal\" + 0.004*\"society\" + 0.004*\"release\" + 0.004*\"help\" + 0.004*\"state\"\n",
      "2019-10-28 21:17:42,779 : INFO : topic #1 (0.091): 0.024*\"inmate\" + 0.009*\"state\" + 0.007*\"officer\" + 0.006*\"court\" + 0.005*\"law\" + 0.005*\"system\" + 0.005*\"staff\" + 0.005*\"right\" + 0.004*\"department\" + 0.003*\"correction\"\n",
      "2019-10-28 21:17:42,781 : INFO : topic #2 (0.091): 0.012*\"inmate\" + 0.007*\"write\" + 0.006*\"officer\" + 0.004*\"god\" + 0.004*\"right\" + 0.004*\"staff\" + 0.004*\"think\" + 0.004*\"help\" + 0.003*\"love\" + 0.003*\"talk\"\n",
      "2019-10-28 21:17:42,782 : INFO : topic #7 (0.091): 0.007*\"state\" + 0.007*\"system\" + 0.005*\"law\" + 0.004*\"right\" + 0.004*\"staff\" + 0.003*\"court\" + 0.003*\"black\" + 0.003*\"justice\" + 0.003*\"criminal\" + 0.003*\"inmate\"\n",
      "2019-10-28 21:17:42,784 : INFO : topic #3 (0.091): 0.004*\"feel\" + 0.004*\"love\" + 0.003*\"think\" + 0.003*\"mind\" + 0.003*\"friend\" + 0.003*\"thought\" + 0.003*\"little\" + 0.003*\"live\" + 0.003*\"face\" + 0.003*\"hand\"\n",
      "2019-10-28 21:17:42,787 : INFO : topic diff=0.499029, rho=0.408248\n",
      "2019-10-28 21:17:46,636 : INFO : -8.157 per-word bound, 285.5 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:17:46,637 : INFO : PROGRESS: pass 5, at document #1573/1573\n",
      "2019-10-28 21:17:48,518 : INFO : topic #2 (0.091): 0.013*\"inmate\" + 0.007*\"write\" + 0.007*\"officer\" + 0.005*\"god\" + 0.004*\"right\" + 0.004*\"help\" + 0.004*\"think\" + 0.004*\"staff\" + 0.004*\"talk\" + 0.004*\"love\"\n",
      "2019-10-28 21:17:48,519 : INFO : topic #3 (0.091): 0.004*\"feel\" + 0.004*\"love\" + 0.003*\"think\" + 0.003*\"mind\" + 0.003*\"friend\" + 0.003*\"thought\" + 0.003*\"little\" + 0.003*\"live\" + 0.003*\"face\" + 0.003*\"hand\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 21:17:48,521 : INFO : topic #8 (0.091): 0.006*\"inmate\" + 0.005*\"officer\" + 0.004*\"state\" + 0.004*\"unit\" + 0.004*\"write\" + 0.004*\"staff\" + 0.003*\"money\" + 0.003*\"guard\" + 0.003*\"food\" + 0.003*\"yard\"\n",
      "2019-10-28 21:17:48,523 : INFO : topic #7 (0.091): 0.007*\"state\" + 0.007*\"system\" + 0.005*\"law\" + 0.004*\"right\" + 0.004*\"staff\" + 0.003*\"court\" + 0.003*\"tha\" + 0.003*\"black\" + 0.003*\"justice\" + 0.003*\"criminal\"\n",
      "2019-10-28 21:17:48,525 : INFO : topic #9 (0.091): 0.009*\"sentence\" + 0.008*\"crime\" + 0.008*\"state\" + 0.007*\"program\" + 0.007*\"offender\" + 0.006*\"parole\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.005*\"law\" + 0.005*\"family\"\n",
      "2019-10-28 21:17:48,527 : INFO : topic diff=0.403387, rho=0.377964\n",
      "2019-10-28 21:17:52,369 : INFO : -8.137 per-word bound, 281.5 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:17:52,370 : INFO : PROGRESS: pass 6, at document #1573/1573\n",
      "2019-10-28 21:17:54,249 : INFO : topic #9 (0.091): 0.010*\"sentence\" + 0.008*\"crime\" + 0.008*\"program\" + 0.008*\"state\" + 0.007*\"offender\" + 0.006*\"parole\" + 0.005*\"system\" + 0.005*\"law\" + 0.005*\"inmate\" + 0.005*\"child\"\n",
      "2019-10-28 21:17:54,251 : INFO : topic #10 (0.091): 0.012*\"black\" + 0.006*\"state\" + 0.005*\"experience\" + 0.005*\"american\" + 0.004*\"consciousness\" + 0.004*\"body\" + 0.004*\"white\" + 0.004*\"system\" + 0.003*\"inmate\" + 0.003*\"appear\"\n",
      "2019-10-28 21:17:54,253 : INFO : topic #7 (0.091): 0.007*\"state\" + 0.006*\"system\" + 0.005*\"law\" + 0.004*\"right\" + 0.004*\"staff\" + 0.003*\"tha\" + 0.003*\"court\" + 0.003*\"mdoc\" + 0.003*\"black\" + 0.003*\"justice\"\n",
      "2019-10-28 21:17:54,254 : INFO : topic #5 (0.091): 0.009*\"society\" + 0.007*\"system\" + 0.007*\"law\" + 0.006*\"human\" + 0.005*\"state\" + 0.005*\"right\" + 0.004*\"crime\" + 0.004*\"mind\" + 0.004*\"criminal\" + 0.003*\"social\"\n",
      "2019-10-28 21:17:54,256 : INFO : topic #4 (0.091): 0.006*\"police\" + 0.005*\"think\" + 0.005*\"write\" + 0.005*\"plan\" + 0.005*\"society\" + 0.004*\"criminal\" + 0.004*\"help\" + 0.004*\"right\" + 0.004*\"release\" + 0.004*\"learn\"\n",
      "2019-10-28 21:17:54,258 : INFO : topic diff=0.331628, rho=0.353553\n",
      "2019-10-28 21:17:58,173 : INFO : -8.122 per-word bound, 278.6 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:17:58,174 : INFO : PROGRESS: pass 7, at document #1573/1573\n",
      "2019-10-28 21:18:00,024 : INFO : topic #5 (0.091): 0.009*\"society\" + 0.007*\"system\" + 0.006*\"law\" + 0.006*\"human\" + 0.005*\"state\" + 0.004*\"right\" + 0.004*\"crime\" + 0.004*\"mind\" + 0.004*\"criminal\" + 0.004*\"social\"\n",
      "2019-10-28 21:18:00,026 : INFO : topic #3 (0.091): 0.004*\"feel\" + 0.004*\"love\" + 0.003*\"think\" + 0.003*\"mind\" + 0.003*\"thought\" + 0.003*\"friend\" + 0.003*\"little\" + 0.003*\"face\" + 0.003*\"hand\" + 0.003*\"live\"\n",
      "2019-10-28 21:18:00,028 : INFO : topic #10 (0.091): 0.013*\"black\" + 0.006*\"state\" + 0.005*\"experience\" + 0.005*\"american\" + 0.005*\"consciousness\" + 0.004*\"white\" + 0.004*\"body\" + 0.004*\"appear\" + 0.004*\"system\" + 0.003*\"member\"\n",
      "2019-10-28 21:18:00,030 : INFO : topic #9 (0.091): 0.010*\"sentence\" + 0.009*\"crime\" + 0.008*\"program\" + 0.008*\"state\" + 0.008*\"offender\" + 0.007*\"parole\" + 0.005*\"system\" + 0.005*\"law\" + 0.005*\"inmate\" + 0.005*\"release\"\n",
      "2019-10-28 21:18:00,031 : INFO : topic #7 (0.091): 0.007*\"state\" + 0.006*\"system\" + 0.005*\"law\" + 0.004*\"staff\" + 0.004*\"right\" + 0.004*\"tha\" + 0.003*\"mdoc\" + 0.003*\"court\" + 0.003*\"black\" + 0.003*\"ta\"\n",
      "2019-10-28 21:18:00,035 : INFO : topic diff=0.276768, rho=0.333333\n",
      "2019-10-28 21:18:03,882 : INFO : -8.110 per-word bound, 276.4 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:18:03,883 : INFO : PROGRESS: pass 8, at document #1573/1573\n",
      "2019-10-28 21:18:05,746 : INFO : topic #2 (0.091): 0.015*\"inmate\" + 0.008*\"officer\" + 0.008*\"write\" + 0.005*\"god\" + 0.005*\"help\" + 0.004*\"think\" + 0.004*\"right\" + 0.004*\"talk\" + 0.004*\"love\" + 0.004*\"fight\"\n",
      "2019-10-28 21:18:05,748 : INFO : topic #3 (0.091): 0.004*\"feel\" + 0.004*\"love\" + 0.003*\"think\" + 0.003*\"mind\" + 0.003*\"thought\" + 0.003*\"friend\" + 0.003*\"little\" + 0.003*\"face\" + 0.003*\"hand\" + 0.003*\"eye\"\n",
      "2019-10-28 21:18:05,750 : INFO : topic #0 (0.091): 0.014*\"state\" + 0.009*\"court\" + 0.008*\"death\" + 0.006*\"sentence\" + 0.006*\"justice\" + 0.006*\"law\" + 0.006*\"system\" + 0.005*\"crime\" + 0.005*\"trial\" + 0.005*\"judge\"\n",
      "2019-10-28 21:18:05,754 : INFO : topic #10 (0.091): 0.014*\"black\" + 0.006*\"state\" + 0.006*\"experience\" + 0.005*\"american\" + 0.005*\"consciousness\" + 0.005*\"white\" + 0.005*\"body\" + 0.004*\"appear\" + 0.004*\"member\" + 0.004*\"system\"\n",
      "2019-10-28 21:18:05,756 : INFO : topic #9 (0.091): 0.010*\"sentence\" + 0.009*\"crime\" + 0.009*\"program\" + 0.008*\"state\" + 0.008*\"offender\" + 0.007*\"parole\" + 0.005*\"system\" + 0.005*\"law\" + 0.005*\"release\" + 0.005*\"inmate\"\n",
      "2019-10-28 21:18:05,762 : INFO : topic diff=0.234104, rho=0.316228\n",
      "2019-10-28 21:18:09,588 : INFO : -8.101 per-word bound, 274.6 perplexity estimate based on a held-out corpus of 1573 documents with 852913 words\n",
      "2019-10-28 21:18:09,589 : INFO : PROGRESS: pass 9, at document #1573/1573\n",
      "2019-10-28 21:18:11,443 : INFO : topic #2 (0.091): 0.015*\"inmate\" + 0.008*\"officer\" + 0.008*\"write\" + 0.006*\"god\" + 0.005*\"help\" + 0.004*\"think\" + 0.004*\"talk\" + 0.004*\"right\" + 0.004*\"love\" + 0.004*\"fight\"\n",
      "2019-10-28 21:18:11,444 : INFO : topic #5 (0.091): 0.010*\"society\" + 0.007*\"system\" + 0.006*\"law\" + 0.006*\"human\" + 0.005*\"state\" + 0.004*\"right\" + 0.004*\"crime\" + 0.004*\"mind\" + 0.004*\"criminal\" + 0.004*\"social\"\n",
      "2019-10-28 21:18:11,447 : INFO : topic #4 (0.091): 0.006*\"write\" + 0.006*\"think\" + 0.006*\"plan\" + 0.006*\"police\" + 0.005*\"society\" + 0.005*\"help\" + 0.005*\"release\" + 0.004*\"learn\" + 0.004*\"criminal\" + 0.004*\"personal\"\n",
      "2019-10-28 21:18:11,449 : INFO : topic #8 (0.091): 0.007*\"inmate\" + 0.006*\"officer\" + 0.005*\"unit\" + 0.004*\"guard\" + 0.004*\"state\" + 0.004*\"food\" + 0.004*\"write\" + 0.004*\"staff\" + 0.003*\"money\" + 0.003*\"yard\"\n",
      "2019-10-28 21:18:11,450 : INFO : topic #10 (0.091): 0.015*\"black\" + 0.006*\"state\" + 0.006*\"experience\" + 0.005*\"american\" + 0.005*\"white\" + 0.005*\"consciousness\" + 0.005*\"body\" + 0.004*\"appear\" + 0.004*\"member\" + 0.004*\"system\"\n",
      "2019-10-28 21:18:11,452 : INFO : topic diff=0.200376, rho=0.301511\n"
     ]
    }
   ],
   "source": [
    "essays_matt = list(tokenized_essays.values())\n",
    "\n",
    "dictionary_matt = corpora.Dictionary(essays_matt)\n",
    "\n",
    "corpus_matt = [dictionary.doc2bow(essay) for essay in essays_matt]\n",
    "\n",
    "# m2 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=2, passes=10)\n",
    "# m3 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=3, passes=10)\n",
    "# m4 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=4, passes=10)\n",
    "# m5 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=5, passes=10)\n",
    "# m6 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=6, passes=10)\n",
    "# m7 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=7, passes=10)\n",
    "# m8 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=8, passes=10)\n",
    "# m9 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=9, passes=10)\n",
    "# m10 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=10, passes=10)\n",
    "m11 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=11, passes=10)\n",
    "# m12 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=12, passes=10)\n",
    "# m13 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=13, passes=10)\n",
    "# m14 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=14, passes=10)\n",
    "# m15 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=15, passes=10)\n",
    "# m16 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=16, passes=10)\n",
    "# m17 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=17, passes=10)\n",
    "# m18 = models.ldamodel.LdaModel(corpus=corpus_matt, id2word=dictionary, num_topics=18, passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 20:54:27,909 : INFO : using ParallelWordOccurrenceAccumulator(processes=3, batch_size=64) to estimate probabilities from sliding windows\n",
      "2019-10-28 20:54:27,993 : INFO : 1 batches submitted to accumulate stats from 64 documents (38321 virtual)\n",
      "2019-10-28 20:54:28,043 : INFO : 2 batches submitted to accumulate stats from 128 documents (72346 virtual)\n",
      "2019-10-28 20:54:28,097 : INFO : 3 batches submitted to accumulate stats from 192 documents (104099 virtual)\n",
      "2019-10-28 20:54:28,194 : INFO : 4 batches submitted to accumulate stats from 256 documents (137126 virtual)\n",
      "2019-10-28 20:54:28,262 : INFO : 5 batches submitted to accumulate stats from 320 documents (177345 virtual)\n",
      "2019-10-28 20:54:28,318 : INFO : 6 batches submitted to accumulate stats from 384 documents (216371 virtual)\n",
      "2019-10-28 20:54:30,195 : INFO : 7 batches submitted to accumulate stats from 448 documents (250140 virtual)\n",
      "2019-10-28 20:54:30,295 : INFO : 8 batches submitted to accumulate stats from 512 documents (275141 virtual)\n",
      "2019-10-28 20:54:30,572 : INFO : 9 batches submitted to accumulate stats from 576 documents (313859 virtual)\n",
      "2019-10-28 20:54:32,054 : INFO : 10 batches submitted to accumulate stats from 640 documents (350725 virtual)\n",
      "2019-10-28 20:54:32,550 : INFO : 11 batches submitted to accumulate stats from 704 documents (383522 virtual)\n",
      "2019-10-28 20:54:32,787 : INFO : 12 batches submitted to accumulate stats from 768 documents (409011 virtual)\n",
      "2019-10-28 20:54:33,997 : INFO : 13 batches submitted to accumulate stats from 832 documents (444374 virtual)\n",
      "2019-10-28 20:54:34,160 : INFO : 14 batches submitted to accumulate stats from 896 documents (474571 virtual)\n",
      "2019-10-28 20:54:34,956 : INFO : 15 batches submitted to accumulate stats from 960 documents (499106 virtual)\n",
      "2019-10-28 20:54:35,953 : INFO : 16 batches submitted to accumulate stats from 1024 documents (534660 virtual)\n",
      "2019-10-28 20:54:36,247 : INFO : 17 batches submitted to accumulate stats from 1088 documents (560818 virtual)\n",
      "2019-10-28 20:54:36,437 : INFO : 18 batches submitted to accumulate stats from 1152 documents (592292 virtual)\n",
      "2019-10-28 20:54:37,911 : INFO : 19 batches submitted to accumulate stats from 1216 documents (622397 virtual)\n",
      "2019-10-28 20:54:37,944 : INFO : 20 batches submitted to accumulate stats from 1280 documents (648336 virtual)\n",
      "2019-10-28 20:54:38,056 : INFO : 21 batches submitted to accumulate stats from 1344 documents (681948 virtual)\n",
      "2019-10-28 20:54:39,506 : INFO : 22 batches submitted to accumulate stats from 1408 documents (713828 virtual)\n",
      "2019-10-28 20:54:39,744 : INFO : 23 batches submitted to accumulate stats from 1472 documents (743388 virtual)\n",
      "2019-10-28 20:54:39,998 : INFO : 24 batches submitted to accumulate stats from 1536 documents (776473 virtual)\n",
      "2019-10-28 20:54:41,291 : INFO : 25 batches submitted to accumulate stats from 1600 documents (792256 virtual)\n",
      "2019-10-28 20:54:43,210 : INFO : serializing accumulator to return to master...\n",
      "2019-10-28 20:54:43,231 : INFO : accumulator serialized\n",
      "2019-10-28 20:54:43,743 : INFO : serializing accumulator to return to master...\n",
      "2019-10-28 20:54:43,748 : INFO : accumulator serialized\n",
      "2019-10-28 20:54:43,856 : INFO : serializing accumulator to return to master...\n",
      "2019-10-28 20:54:43,863 : INFO : accumulator serialized\n",
      "2019-10-28 20:54:44,043 : INFO : 3 accumulators retrieved from output queue\n",
      "2019-10-28 20:54:44,089 : INFO : accumulated word occurrence stats for 796855 virtual documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41437948400493707"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CoherenceModel(model=m11, texts=essays_matt, coherence='c_v')\n",
    "coherence = cm.get_coherence()\n",
    "coherence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some notes\n",
    "2: 0.3992337026308589\n",
    "3: 0.371993080904979\n",
    "4: 0.40025270112726735\n",
    "5: 0.4174421259723099\n",
    "6: 0.37134097989863984\n",
    "7: 0.42337526139475695\n",
    "8: 0.39162248222910934\n",
    "9: 0.379028631160457\n",
    "10: 0.4137827965771722\n",
    "11: 0.4500098234881914\n",
    "12: 0.4318264581457793\n",
    "13: 0.4127596451219076\n",
    "14: 0.4039530315591046\n",
    "15: 0.4117885886817264\n",
    "16: 0.41180308515646974\n",
    "17: 0.41060383798197586\n",
    "18: 0.4321508008523154\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.147509</td>\n",
       "      <td>-0.014784</td>\n",
       "      <td>-0.011532</td>\n",
       "      <td>0.024092</td>\n",
       "      <td>0.127395</td>\n",
       "      <td>0.085249</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.096642</td>\n",
       "      <td>-0.029476</td>\n",
       "      <td>-0.004723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>-0.047249</td>\n",
       "      <td>-0.017429</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>0.062582</td>\n",
       "      <td>0.022023</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>0.196062</td>\n",
       "      <td>0.048372</td>\n",
       "      <td>-0.250067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.336653</td>\n",
       "      <td>0.249148</td>\n",
       "      <td>-0.043525</td>\n",
       "      <td>0.706586</td>\n",
       "      <td>-0.233412</td>\n",
       "      <td>0.320358</td>\n",
       "      <td>0.295057</td>\n",
       "      <td>-0.073638</td>\n",
       "      <td>0.076726</td>\n",
       "      <td>-0.360512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013873</td>\n",
       "      <td>0.582860</td>\n",
       "      <td>-0.063119</td>\n",
       "      <td>-0.274440</td>\n",
       "      <td>-0.174419</td>\n",
       "      <td>0.134983</td>\n",
       "      <td>0.172573</td>\n",
       "      <td>-0.251397</td>\n",
       "      <td>0.078798</td>\n",
       "      <td>-0.403040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.265048</td>\n",
       "      <td>0.034929</td>\n",
       "      <td>-0.143668</td>\n",
       "      <td>0.082906</td>\n",
       "      <td>-0.179479</td>\n",
       "      <td>0.300218</td>\n",
       "      <td>0.028328</td>\n",
       "      <td>0.027059</td>\n",
       "      <td>-0.208596</td>\n",
       "      <td>-0.336358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087104</td>\n",
       "      <td>0.156282</td>\n",
       "      <td>0.074614</td>\n",
       "      <td>-0.152939</td>\n",
       "      <td>-0.126736</td>\n",
       "      <td>0.070535</td>\n",
       "      <td>0.031246</td>\n",
       "      <td>0.155973</td>\n",
       "      <td>0.047020</td>\n",
       "      <td>-0.156064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.255031</td>\n",
       "      <td>0.146072</td>\n",
       "      <td>-0.028686</td>\n",
       "      <td>0.045554</td>\n",
       "      <td>0.538038</td>\n",
       "      <td>-0.046723</td>\n",
       "      <td>0.412369</td>\n",
       "      <td>0.086602</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.103039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>-0.231739</td>\n",
       "      <td>-0.091940</td>\n",
       "      <td>-0.061719</td>\n",
       "      <td>0.060482</td>\n",
       "      <td>0.107179</td>\n",
       "      <td>0.110714</td>\n",
       "      <td>0.380833</td>\n",
       "      <td>0.094955</td>\n",
       "      <td>-0.390728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.213440</td>\n",
       "      <td>-0.071343</td>\n",
       "      <td>-0.144156</td>\n",
       "      <td>-0.248104</td>\n",
       "      <td>0.478187</td>\n",
       "      <td>-0.186624</td>\n",
       "      <td>0.323790</td>\n",
       "      <td>0.397954</td>\n",
       "      <td>0.111181</td>\n",
       "      <td>0.156982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130110</td>\n",
       "      <td>-0.255869</td>\n",
       "      <td>0.064748</td>\n",
       "      <td>-0.137551</td>\n",
       "      <td>0.240025</td>\n",
       "      <td>0.097368</td>\n",
       "      <td>0.145716</td>\n",
       "      <td>0.605082</td>\n",
       "      <td>0.207671</td>\n",
       "      <td>-0.496480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1568</td>\n",
       "      <td>0.344591</td>\n",
       "      <td>0.289569</td>\n",
       "      <td>0.018266</td>\n",
       "      <td>0.122705</td>\n",
       "      <td>-0.734210</td>\n",
       "      <td>0.738716</td>\n",
       "      <td>0.212049</td>\n",
       "      <td>-0.188311</td>\n",
       "      <td>-0.145424</td>\n",
       "      <td>-0.807569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100896</td>\n",
       "      <td>0.090312</td>\n",
       "      <td>0.048385</td>\n",
       "      <td>-0.260574</td>\n",
       "      <td>-0.413471</td>\n",
       "      <td>0.172159</td>\n",
       "      <td>0.087545</td>\n",
       "      <td>0.064364</td>\n",
       "      <td>-0.199121</td>\n",
       "      <td>-0.041960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1569</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.163935</td>\n",
       "      <td>-0.092415</td>\n",
       "      <td>-0.060433</td>\n",
       "      <td>0.213361</td>\n",
       "      <td>-0.010032</td>\n",
       "      <td>0.184890</td>\n",
       "      <td>0.044825</td>\n",
       "      <td>0.166070</td>\n",
       "      <td>-0.033748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032203</td>\n",
       "      <td>-0.115270</td>\n",
       "      <td>0.050044</td>\n",
       "      <td>-0.100978</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>0.094837</td>\n",
       "      <td>0.095134</td>\n",
       "      <td>0.153526</td>\n",
       "      <td>0.131756</td>\n",
       "      <td>-0.229616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.225892</td>\n",
       "      <td>0.031512</td>\n",
       "      <td>-0.134012</td>\n",
       "      <td>-0.058413</td>\n",
       "      <td>0.240788</td>\n",
       "      <td>-0.042440</td>\n",
       "      <td>0.290294</td>\n",
       "      <td>0.264335</td>\n",
       "      <td>0.151973</td>\n",
       "      <td>0.036415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067169</td>\n",
       "      <td>-0.171892</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>-0.155270</td>\n",
       "      <td>0.095842</td>\n",
       "      <td>0.093631</td>\n",
       "      <td>0.148598</td>\n",
       "      <td>0.370317</td>\n",
       "      <td>0.119133</td>\n",
       "      <td>-0.386080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1571</td>\n",
       "      <td>0.166018</td>\n",
       "      <td>0.086796</td>\n",
       "      <td>0.046333</td>\n",
       "      <td>0.045508</td>\n",
       "      <td>-0.083053</td>\n",
       "      <td>0.337955</td>\n",
       "      <td>0.130337</td>\n",
       "      <td>-0.031834</td>\n",
       "      <td>-0.177276</td>\n",
       "      <td>-0.198782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056631</td>\n",
       "      <td>-0.030734</td>\n",
       "      <td>-0.055120</td>\n",
       "      <td>-0.001568</td>\n",
       "      <td>-0.095165</td>\n",
       "      <td>0.070136</td>\n",
       "      <td>0.056693</td>\n",
       "      <td>0.085705</td>\n",
       "      <td>-0.054171</td>\n",
       "      <td>-0.165933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1572</td>\n",
       "      <td>0.060727</td>\n",
       "      <td>0.034369</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.060557</td>\n",
       "      <td>0.134430</td>\n",
       "      <td>0.019685</td>\n",
       "      <td>0.076578</td>\n",
       "      <td>0.074183</td>\n",
       "      <td>-0.069745</td>\n",
       "      <td>-0.017958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011998</td>\n",
       "      <td>0.036038</td>\n",
       "      <td>-0.038246</td>\n",
       "      <td>-0.076991</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>0.057518</td>\n",
       "      <td>0.033724</td>\n",
       "      <td>0.060836</td>\n",
       "      <td>0.032595</td>\n",
       "      <td>-0.111047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1573 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.147509 -0.014784 -0.011532  0.024092  0.127395  0.085249  0.138889   \n",
       "1     0.336653  0.249148 -0.043525  0.706586 -0.233412  0.320358  0.295057   \n",
       "2     0.265048  0.034929 -0.143668  0.082906 -0.179479  0.300218  0.028328   \n",
       "3     0.255031  0.146072 -0.028686  0.045554  0.538038 -0.046723  0.412369   \n",
       "4     0.213440 -0.071343 -0.144156 -0.248104  0.478187 -0.186624  0.323790   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1568  0.344591  0.289569  0.018266  0.122705 -0.734210  0.738716  0.212049   \n",
       "1569  0.167273  0.163935 -0.092415 -0.060433  0.213361 -0.010032  0.184890   \n",
       "1570  0.225892  0.031512 -0.134012 -0.058413  0.240788 -0.042440  0.290294   \n",
       "1571  0.166018  0.086796  0.046333  0.045508 -0.083053  0.337955  0.130337   \n",
       "1572  0.060727  0.034369 -0.000007  0.060557  0.134430  0.019685  0.076578   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0     0.096642 -0.029476 -0.004723  ...  0.008632 -0.047249 -0.017429   \n",
       "1    -0.073638  0.076726 -0.360512  ...  0.013873  0.582860 -0.063119   \n",
       "2     0.027059 -0.208596 -0.336358  ...  0.087104  0.156282  0.074614   \n",
       "3     0.086602  0.085714  0.103039  ...  0.001472 -0.231739 -0.091940   \n",
       "4     0.397954  0.111181  0.156982  ... -0.130110 -0.255869  0.064748   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1568 -0.188311 -0.145424 -0.807569  ...  0.100896  0.090312  0.048385   \n",
       "1569  0.044825  0.166070 -0.033748  ...  0.032203 -0.115270  0.050044   \n",
       "1570  0.264335  0.151973  0.036415  ... -0.067169 -0.171892  0.011226   \n",
       "1571 -0.031834 -0.177276 -0.198782  ...  0.056631 -0.030734 -0.055120   \n",
       "1572  0.074183 -0.069745 -0.017958  ... -0.011998  0.036038 -0.038246   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0    -0.007457  0.062582  0.022023  0.085102  0.196062  0.048372 -0.250067  \n",
       "1    -0.274440 -0.174419  0.134983  0.172573 -0.251397  0.078798 -0.403040  \n",
       "2    -0.152939 -0.126736  0.070535  0.031246  0.155973  0.047020 -0.156064  \n",
       "3    -0.061719  0.060482  0.107179  0.110714  0.380833  0.094955 -0.390728  \n",
       "4    -0.137551  0.240025  0.097368  0.145716  0.605082  0.207671 -0.496480  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1568 -0.260574 -0.413471  0.172159  0.087545  0.064364 -0.199121 -0.041960  \n",
       "1569 -0.100978  0.039179  0.094837  0.095134  0.153526  0.131756 -0.229616  \n",
       "1570 -0.155270  0.095842  0.093631  0.148598  0.370317  0.119133 -0.386080  \n",
       "1571 -0.001568 -0.095165  0.070136  0.056693  0.085705 -0.054171 -0.165933  \n",
       "1572 -0.076991 -0.000478  0.057518  0.033724  0.060836  0.032595 -0.111047  \n",
       "\n",
       "[1573 rows x 100 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_df = pd.DataFrame(d2v_model.docvecs.vectors_docs)\n",
    "vectorized_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling through standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsclr = StandardScaler()\n",
    "standardized_df = pd.DataFrame(stdsclr.fit_transform(vectorized_df.astype(float)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "reduced_df = pd.DataFrame(pca.fit_transform(standardized_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to visualize effectiveness of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.to_csv('new1.csv', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering w/ k-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 139 ms, sys: 8.55 ms, total: 147 ms\n",
      "Wall time: 183 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=7, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 7\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, init=\"k-means++\", max_iter=100)\n",
    "\n",
    "%time km.fit(reduced_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essays per cluster / Theme(s) per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    570\n",
       "3    341\n",
       "5    336\n",
       "4     94\n",
       "2     85\n",
       "6     80\n",
       "1     67\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = reduced_df\n",
    "output['cluster'] = km.labels_\n",
    "output['essay'] = tokenized_essays.values()\n",
    "output['title'] = tokenized_essays.keys()\n",
    "\n",
    "output['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_index = {\n",
    "                'Autobiographical, Paths to Prison': 'parent, damage, abuse, gang, alcohol, drug, neighborhood, hood, youth, pressure, fit, broken', \n",
    "                'Family': 'family, abandonment, relation, visit, partner, mother, father, sibling, child, wife, husband, abuse, support', \n",
    "                'Physical Conditions and Security': 'physical, condition, security, search, censorship, food, cold, hygiene, heat, misfunction, infestation, solitary, strip, search, fear, filth, violence, staff, abuse', \n",
    "                'Prison Culture/Community/Society': 'violence, fear, staff, sexual, crime, outcasts, racial, cellmate, gay, LGBTQ, dehumanize, uniform, pecking, order, hierarchy, solid, dirty, skin, chomo',\n",
    "                'Staff/prison Abuse of IP': 'abuse, sexual, torture, humiliation, racist, assault, antagonism, exacerbation, right, violation, food, hygiene, environment, legal, extraction, search, taunt',\n",
    "                'Personal/Intern Change/Coping': 'survival, art, reading, writing, peace, faith, prayer, meditation, practice, community, activities, hobbies, cooking, remorse, motivation, education, discipline, coping, adjustment, responsibility, god, redemption, transformation',\n",
    "                'Judicial Misconduct and Legal Remediation': 'judicial, incompetence, corruption, witness, evidence, excessive, political, jailhouse, lawyer, misconduct, unfair, pretender, plra, plea, grievance',\n",
    "                'Political and Intellectual Labor among IP': 'activism, resistence, critique, race, class, change, policies, practices, write, organize, strike, solidarity',\n",
    "                'Prison Industry/Prison as Business': 'labor, slave, condition, safety, health, profit, job, budget, tax, taxpayer, exploitation, corruption, mismanagement, nepotism',\n",
    "                'Education, Re-entry, Other Programs': 'rehabilitation, entry, education, indifference, college, vocation',\n",
    "                'Health Care': 'health, care, negligence, hostility, incompetence, indifference, death, injury, treatment, medication',\n",
    "                'Social Alienation, Indifference, Hostility': 'public, misperception, identity, stigma'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Random sample essays per cluster *****\n",
      "\n",
      "\n",
      "** Cluster 0: **\n",
      "\n",
      "Top terms #0 : ['african', 'air', 'alone', 'along', 'black', 'business', 'condition', 'consciousness', 'custody', 'deal', 'emotional', 'environment', 'everyday', 'exercise', 'existence', 'experience', 'extreme', 'family', 'force', 'free', 'house', 'inmate', 'intelligence', 'level', 'market', 'move', 'nature', 'official', 'order', 'outside', 'pm', 'policy', 'reflection', 'roughly', 'sentence', 'set', 'side', 'speak', 'state', 'strong', 'symbolic', 'system', 'talk', 'tax', 'think', 'threat', 'undifferentiated', 'value', 'victim', 'wrong']\n",
      "\n",
      "Dominant theme(s): ['Judicial Misconduct and Legal Remediation', 'Prison Industry/Prison as Business']\n",
      "\n",
      "[('Prison Industry/Prison as Business', 2), ('Judicial Misconduct and Legal Remediation', 2), ('Prison Culture/Community/Society', 1), ('Family', 1), ('Staff/prison Abuse of IP', 1), ('Physical Conditions and Security', 1)]\n",
      "\n",
      "Randomly selected essays: ['apw_12347608.txt', 'apw_12343099.txt', 'apw_12349638.txt', 'apw_12346660.txt', 'apw_12353008.txt', 'apw_12350477.txt', 'apw_246.txt', 'apw_12348173.txt', 'apw_12347979.txt', 'apw_12345699.txt']\n",
      "\n",
      "\n",
      "** Cluster 1: **\n",
      "\n",
      "Top terms #1 : ['action', 'american', 'answer', 'appear', 'black', 'body', 'common', 'course', 'criminal', 'desire', 'ever', 'experience', 'feel', 'form', 'grassroot', 'great', 'help', 'human', 'keep', 'lead', 'learn', 'little', 'member', 'mind', 'move', 'movement', 'personal', 'phantom', 'plan', 'political', 'post', 'power', 'problem', 'process', 'program', 'question', 'release', 'right', 'rule', 'sense', 'social', 'society', 'soul', 'state', 'struggle', 'system', 'think', 'thought', 'turn', 'united']\n",
      "\n",
      "Dominant theme(s): ['Staff/prison Abuse of IP', 'Prison Culture/Community/Society']\n",
      "\n",
      "[('Prison Culture/Community/Society', 2), ('Staff/prison Abuse of IP', 2), ('Judicial Misconduct and Legal Remediation', 1), ('Personal/Intern Change/Coping', 1)]\n",
      "\n",
      "Randomly selected essays: ['apw_12342978.txt', 'apw_12345368.txt', 'apw_12344818.txt', 'apw_12342578.txt', 'apw_12343912.txt', 'apw_12345244.txt', 'apw_12351839.txt', 'apw_12347769.txt', 'apw_299.txt', 'apw_12346439.txt']\n",
      "\n",
      "\n",
      "** Cluster 2: **\n",
      "\n",
      "Top terms #2 : ['american', 'board', 'cdcr', 'child', 'convict', 'conviction', 'court', 'crime', 'criminal', 'death', 'decision', 'defendant', 'error', 'fact', 'federal', 'government', 'guilty', 'however', 'incarceration', 'inmate', 'innocent', 'issue', 'judge', 'jury', 'justice', 'juvenile', 'law', 'local', 'louisiana', 'military', 'offender', 'often', 'ohio', 'page', 'parish', 'parole', 'population', 'prosecutor', 'public', 'rate', 'release', 'right', 'sentence', 'serve', 'state', 'supreme', 'system', 'three', 'trial', 'united']\n",
      "\n",
      "Dominant theme(s): ['Health Care']\n",
      "\n",
      "[('Health Care', 2), ('Staff/prison Abuse of IP', 1), ('Social Alienation, Indifference, Hostility', 1), ('Judicial Misconduct and Legal Remediation', 1), ('Prison Culture/Community/Society', 1), ('Family', 1)]\n",
      "\n",
      "Randomly selected essays: ['apw_12349026.txt', 'apw_12352659.txt', 'apw_12344616.txt', 'apw_12349187.txt', 'apw_12348982.txt', 'apw_12344628.txt', 'apw_12342754.txt', 'apw_12346026.txt', 'apw_12341453.txt', 'apw_12341407.txt']\n",
      "\n",
      "\n",
      "** Cluster 3: **\n",
      "\n",
      "Top terms #3 : ['abuse', 'assistant', 'attica', 'ccpoa', 'child', 'citizen', 'community', 'control', 'correction', 'crime', 'death', 'education', 'etc', 'family', 'follow', 'food', 'force', 'human', 'inmate', 'institution', 'justice', 'law', 'learn', 'living', 'mean', 'medium', 'number', 'often', 'organization', 'parole', 'penal', 'policy', 'political', 'politician', 'population', 'power', 'principal', 'punishment', 'race', 'right', 'school', 'sentence', 'social', 'society', 'state', 'student', 'system', 'term', 'th', 'torture']\n",
      "\n",
      "Dominant theme(s): ['Autobiographical, Paths to Prison', 'Family', 'Physical Conditions and Security', 'Staff/prison Abuse of IP', 'Personal/Intern Change/Coping', 'Prison Culture/Community/Society', 'Health Care', 'Judicial Misconduct and Legal Remediation']\n",
      "\n",
      "[('Staff/prison Abuse of IP', 4), ('Family', 4), ('Personal/Intern Change/Coping', 3), ('Physical Conditions and Security', 3), ('Judicial Misconduct and Legal Remediation', 2), ('Health Care', 2), ('Prison Culture/Community/Society', 2), ('Autobiographical, Paths to Prison', 2), ('Prison Industry/Prison as Business', 1), ('Political and Intellectual Labor among IP', 1), ('Education, Re-entry, Other Programs', 1)]\n",
      "\n",
      "Randomly selected essays: ['apw_247.txt', 'apw_12345603.txt', 'apw_12349362.txt', 'apw_12352516.txt', 'apw_12345544.txt', 'apw_266.txt', 'apw_12344606.txt', 'apw_12351663.txt', 'apw_12351380.txt', 'apw_12345497.txt']\n",
      "\n",
      "\n",
      "** Cluster 4: **\n",
      "\n",
      "Top terms #4 : ['african', 'american', 'black', 'bladder', 'california', 'cancer', 'care', 'cdcr', 'center', 'charge', 'citizen', 'co', 'color', 'condition', 'contraband', 'crime', 'criminal', 'death', 'doctor', 'drug', 'employee', 'family', 'federal', 'force', 'housing', 'inmate', 'institution', 'kill', 'law', 'medical', 'officer', 'official', 'parole', 'phone', 'release', 'richard', 'risk', 'robinson', 'sentence', 'several', 'staff', 'state', 'system', 'ta', 'tha', 'three', 'unit', 'valley', 'white', 'write']\n",
      "\n",
      "Dominant theme(s): ['Health Care', 'Autobiographical, Paths to Prison', 'Physical Conditions and Security', 'Personal/Intern Change/Coping', 'Judicial Misconduct and Legal Remediation', 'Prison Industry/Prison as Business', 'Education, Re-entry, Other Programs', 'Prison Culture/Community/Society']\n",
      "\n",
      "[('Physical Conditions and Security', 4), ('Prison Culture/Community/Society', 3), ('Prison Industry/Prison as Business', 3), ('Personal/Intern Change/Coping', 3), ('Health Care', 3), ('Education, Re-entry, Other Programs', 2), ('Judicial Misconduct and Legal Remediation', 2), ('Autobiographical, Paths to Prison', 2), ('Political and Intellectual Labor among IP', 1), ('Staff/prison Abuse of IP', 1), ('Family', 1)]\n",
      "\n",
      "Randomly selected essays: ['apw_12348877.txt', 'apw_12342774.txt', 'apw_12351236.txt', 'apw_12345781.txt', 'apw_12352846.txt', 'apw_12351696.txt', 'apw_12351957.txt', 'apw_12353447.txt', 'apw_12344593.txt', 'apw_12352846.txt']\n",
      "\n",
      "\n",
      "** Cluster 5: **\n",
      "\n",
      "Top terms #5 : ['believe', 'brother', 'cage', 'court', 'death', 'door', 'family', 'fence', 'friend', 'gang', 'guard', 'hard', 'hear', 'inmate', 'kill', 'knew', 'last', 'light', 'lock', 'lose', 'love', 'matter', 'member', 'mh', 'mind', 'night', 'officer', 'rainey', 'range', 'razor', 'recovery', 'retribution', 'reï¬‚ect', 'ribbon', 'right', 'row', 'sentence', 'shower', 'society', 'state', 'system', 'talk', 'texas', 'think', 'though', 'thought', 'tray', 'truth', 'walk', 'wall']\n",
      "\n",
      "Dominant theme(s): []\n",
      "\n",
      "[('Staff/prison Abuse of IP', 1), ('Autobiographical, Paths to Prison', 1), ('Family', 1), ('Health Care', 1)]\n",
      "\n",
      "Randomly selected essays: ['apw_12345524.txt', 'apw_12343370.txt', 'apw_12350823.txt', 'apw_12354020.txt', 'apw_12344034.txt', 'apw_12351873.txt', 'apw_12343298.txt', 'apw_12351414.txt', 'apw_12349382.txt', 'apw_12345138.txt']\n",
      "\n",
      "\n",
      "** Cluster 6: **\n",
      "\n",
      "Top terms #6 : ['ada', 'aimee', 'attica', 'car', 'care', 'cf', 'christine', 'door', 'drug', 'eye', 'face', 'faiello', 'family', 'father', 'feel', 'felt', 'grandmother', 'guard', 'hand', 'head', 'home', 'house', 'laser', 'left', 'letter', 'line', 'little', 'love', 'marilyn', 'mother', 'night', 'open', 'patient', 'phone', 'relationship', 'right', 'room', 'skin', 'small', 'smile', 'table', 'tamia', 'think', 'thought', 'three', 'treatment', 'turn', 'visit', 'wait', 'white']\n",
      "\n",
      "Dominant theme(s): ['Health Care', 'Family']\n",
      "\n",
      "[('Family', 4), ('Health Care', 3), ('Prison Culture/Community/Society', 1), ('Staff/prison Abuse of IP', 1), ('Personal/Intern Change/Coping', 1), ('Judicial Misconduct and Legal Remediation', 1), ('Autobiographical, Paths to Prison', 1)]\n",
      "\n",
      "Randomly selected essays: ['apw_12349954.txt', 'apw_12353598.txt', 'apw_12348926.txt', 'apw_12351729.txt', 'apw_12344967.txt', 'apw_12345936.txt', 'apw_12345861.txt', 'apw_12351973.txt', 'apw_12345957.txt', 'apw_12348926.txt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "print(\"***** Random sample essays per cluster *****\")\n",
    "print()\n",
    "\n",
    "terms_per_essays = {}\n",
    "themes_per_essays = {}\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    ten = [random.choice(output[output.cluster == i]['title'].values.tolist()) for _ in range(10)]\n",
    "    print()\n",
    "    print(\"** Cluster %d: **\\n\" % i)\n",
    "    \n",
    "    context = []\n",
    "    for lst in [tokenized_essays[title] for title in ten]:\n",
    "        context += lst\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=50)\n",
    "    tfidf_vectorizer.fit(context)\n",
    "    \n",
    "    terms_per_essays[i] = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    print(\"Top terms #{} : {}\".format(i, terms_per_essays[i]))\n",
    "    print()\n",
    "    \n",
    "    themes_per_essays[i] = set()\n",
    "    theme_term_count = {}\n",
    "    for term in terms_per_essays[i]:\n",
    "        for key, value in theme_index.items():\n",
    "            if term in value:\n",
    "                if key in theme_term_count:\n",
    "                    theme_term_count[key] += 1\n",
    "                else:\n",
    "                    theme_term_count[key] = 1\n",
    "                themes_per_essays[i].add(key)\n",
    "    \n",
    "    themes_per_essays[i] = [theme for theme, count in theme_term_count.items() if count > 1]\n",
    "    print(\"Dominant theme(s): {}\".format(themes_per_essays[i]))\n",
    "    print()\n",
    "    print(sorted(theme_term_count.items(), key=lambda x : x[1])[::-1])\n",
    "    \n",
    "    print()\n",
    "    print(\"Randomly selected essays:\", ten)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model/Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'word2vec_cluster.pkl'\n",
    "pickle.dump(km, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:27:39,185 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-28 19:27:39,386 : INFO : built Dictionary(14822 unique tokens: ['aggressive', 'air', 'anger', 'anyone', 'anything']...) from 570 documents (total 125193 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Cluster # 0 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:27:39,512 : INFO : using symmetric alpha at 1.0\n",
      "2019-10-28 19:27:39,513 : INFO : using symmetric eta at 1.0\n",
      "2019-10-28 19:27:39,517 : INFO : using serial LDA version on this node\n",
      "2019-10-28 19:27:39,522 : INFO : running online (multi-pass) LDA training, 1 topics, 10 passes over the supplied corpus of 570 documents, updating model once every 570 documents, evaluating perplexity every 570 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-28 19:27:40,081 : INFO : -9.890 per-word bound, 948.9 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:40,082 : INFO : PROGRESS: pass 0, at document #570/570\n",
      "2019-10-28 19:27:40,194 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:40,194 : INFO : topic diff=1.208399, rho=1.000000\n",
      "2019-10-28 19:27:40,746 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:40,747 : INFO : PROGRESS: pass 1, at document #570/570\n",
      "2019-10-28 19:27:40,847 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:40,848 : INFO : topic diff=0.002423, rho=0.577350\n",
      "2019-10-28 19:27:41,385 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:41,386 : INFO : PROGRESS: pass 2, at document #570/570\n",
      "2019-10-28 19:27:41,491 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:41,492 : INFO : topic diff=0.000774, rho=0.500000\n",
      "2019-10-28 19:27:42,133 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:42,133 : INFO : PROGRESS: pass 3, at document #570/570\n",
      "2019-10-28 19:27:42,281 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:42,282 : INFO : topic diff=0.000414, rho=0.447214\n",
      "2019-10-28 19:27:42,851 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:42,852 : INFO : PROGRESS: pass 4, at document #570/570\n",
      "2019-10-28 19:27:42,950 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:42,951 : INFO : topic diff=0.000227, rho=0.408248\n",
      "2019-10-28 19:27:43,482 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:43,483 : INFO : PROGRESS: pass 5, at document #570/570\n",
      "2019-10-28 19:27:43,580 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:43,580 : INFO : topic diff=0.000124, rho=0.377964\n",
      "2019-10-28 19:27:44,111 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:44,112 : INFO : PROGRESS: pass 6, at document #570/570\n",
      "2019-10-28 19:27:44,207 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:44,207 : INFO : topic diff=0.000073, rho=0.353553\n",
      "2019-10-28 19:27:44,712 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:44,713 : INFO : PROGRESS: pass 7, at document #570/570\n",
      "2019-10-28 19:27:44,811 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:44,812 : INFO : topic diff=0.000045, rho=0.333333\n",
      "2019-10-28 19:27:45,409 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:45,410 : INFO : PROGRESS: pass 8, at document #570/570\n",
      "2019-10-28 19:27:45,509 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:45,510 : INFO : topic diff=0.000028, rho=0.316228\n",
      "2019-10-28 19:27:46,127 : INFO : -8.398 per-word bound, 337.4 perplexity estimate based on a held-out corpus of 570 documents with 125193 words\n",
      "2019-10-28 19:27:46,128 : INFO : PROGRESS: pass 9, at document #570/570\n",
      "2019-10-28 19:27:46,256 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\"\n",
      "2019-10-28 19:27:46,257 : INFO : topic diff=0.000017, rho=0.301511\n",
      "2019-10-28 19:27:46,260 : INFO : topic #0 (1.000): 0.009*\"inmate\" + 0.005*\"state\" + 0.004*\"system\" + 0.004*\"officer\" + 0.003*\"write\" + 0.003*\"right\" + 0.003*\"family\" + 0.003*\"help\" + 0.003*\"law\" + 0.003*\"society\" + 0.003*\"think\" + 0.003*\"sentence\" + 0.002*\"crime\" + 0.002*\"staff\" + 0.002*\"feel\" + 0.002*\"love\" + 0.002*\"program\" + 0.002*\"criminal\" + 0.002*\"correctional\" + 0.002*\"problem\"\n",
      "2019-10-28 19:27:46,264 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-28 19:27:46,418 : INFO : built Dictionary(13536 unique tokens: ['abide', 'ability', 'able', 'abnormal', 'absorbed']...) from 67 documents (total 112957 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms: ['0.009*\"inmate\" ', ' 0.005*\"state\" ', ' 0.004*\"system\" ', ' 0.004*\"officer\" ', ' 0.003*\"write\" ', ' 0.003*\"right\" ', ' 0.003*\"family\" ', ' 0.003*\"help\" ', ' 0.003*\"law\" ', ' 0.003*\"society\" ', ' 0.003*\"think\" ', ' 0.003*\"sentence\" ', ' 0.002*\"crime\" ', ' 0.002*\"staff\" ', ' 0.002*\"feel\" ', ' 0.002*\"love\" ', ' 0.002*\"program\" ', ' 0.002*\"criminal\" ', ' 0.002*\"correctional\" ', ' 0.002*\"problem\"']\n",
      "\n",
      "\n",
      "Terms per cluster: {0: ['inmate', 'state', 'system', 'officer', 'write', 'right', 'family', 'help', 'law', 'society', 'think', 'sentence', 'crime', 'staff', 'feel', 'love', 'program', 'criminal', 'correctional', 'proble']}\n",
      "\n",
      "Terms/Scores: {'inmate': 0.009, 'state': 0.005, 'system': 0.004, 'officer': 0.004, 'write': 0.003, 'right': 0.003, 'family': 0.003, 'help': 0.003, 'law': 0.003, 'society': 0.003, 'think': 0.003, 'sentence': 0.003, 'crime': 0.002, 'staff': 0.002, 'feel': 0.002, 'love': 0.002, 'program': 0.002, 'criminal': 0.002, 'correctional': 0.002, 'proble': 0.002}\n",
      "\n",
      "Themes ranked strongest to weakest: [('Prison Culture/Community/Society', 0.004), ('Judicial Misconduct and Legal Remediation', 0.003), ('Family', 0.003), ('Staff/prison Abuse of IP', 0.003), ('Political and Intellectual Labor among IP', 0.003), ('Physical Conditions and Security', 0.002)]\n",
      "\n",
      "************ Cluster # 1 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:27:46,518 : INFO : using symmetric alpha at 1.0\n",
      "2019-10-28 19:27:46,519 : INFO : using symmetric eta at 1.0\n",
      "2019-10-28 19:27:46,523 : INFO : using serial LDA version on this node\n",
      "2019-10-28 19:27:46,526 : INFO : running online (multi-pass) LDA training, 1 topics, 10 passes over the supplied corpus of 67 documents, updating model once every 67 documents, evaluating perplexity every 67 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-28 19:27:46,822 : INFO : -9.800 per-word bound, 891.2 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:46,823 : INFO : PROGRESS: pass 0, at document #67/67\n",
      "2019-10-28 19:27:46,897 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:46,900 : INFO : topic diff=1.156554, rho=1.000000\n",
      "2019-10-28 19:27:47,240 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:47,240 : INFO : PROGRESS: pass 1, at document #67/67\n",
      "2019-10-28 19:27:47,283 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:47,284 : INFO : topic diff=0.002031, rho=0.577350\n",
      "2019-10-28 19:27:47,583 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:47,585 : INFO : PROGRESS: pass 2, at document #67/67\n",
      "2019-10-28 19:27:47,626 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:47,627 : INFO : topic diff=0.000813, rho=0.500000\n",
      "2019-10-28 19:27:47,896 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:47,897 : INFO : PROGRESS: pass 3, at document #67/67\n",
      "2019-10-28 19:27:47,937 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:47,938 : INFO : topic diff=0.000372, rho=0.447214\n",
      "2019-10-28 19:27:48,231 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:48,232 : INFO : PROGRESS: pass 4, at document #67/67\n",
      "2019-10-28 19:27:48,275 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:48,276 : INFO : topic diff=0.000191, rho=0.408248\n",
      "2019-10-28 19:27:48,569 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:48,570 : INFO : PROGRESS: pass 5, at document #67/67\n",
      "2019-10-28 19:27:48,612 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:48,613 : INFO : topic diff=0.000088, rho=0.377964\n",
      "2019-10-28 19:27:48,898 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:48,898 : INFO : PROGRESS: pass 6, at document #67/67\n",
      "2019-10-28 19:27:48,940 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:48,941 : INFO : topic diff=0.000038, rho=0.353553\n",
      "2019-10-28 19:27:49,311 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:49,312 : INFO : PROGRESS: pass 7, at document #67/67\n",
      "2019-10-28 19:27:49,378 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:49,379 : INFO : topic diff=0.000036, rho=0.333333\n",
      "2019-10-28 19:27:49,769 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:49,770 : INFO : PROGRESS: pass 8, at document #67/67\n",
      "2019-10-28 19:27:49,813 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:49,819 : INFO : topic diff=0.000022, rho=0.316228\n",
      "2019-10-28 19:27:50,099 : INFO : -8.424 per-word bound, 343.4 perplexity estimate based on a held-out corpus of 67 documents with 112957 words\n",
      "2019-10-28 19:27:50,100 : INFO : PROGRESS: pass 9, at document #67/67\n",
      "2019-10-28 19:27:50,140 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "2019-10-28 19:27:50,141 : INFO : topic diff=0.000012, rho=0.301511\n",
      "2019-10-28 19:27:50,144 : INFO : topic #0 (1.000): 0.004*\"society\" + 0.004*\"black\" + 0.004*\"think\" + 0.004*\"experience\" + 0.003*\"mind\" + 0.003*\"body\" + 0.003*\"social\" + 0.003*\"write\" + 0.003*\"state\" + 0.003*\"system\" + 0.003*\"criminal\" + 0.003*\"consciousness\" + 0.003*\"learn\" + 0.002*\"appear\" + 0.002*\"human\" + 0.002*\"behavior\" + 0.002*\"process\" + 0.002*\"problem\" + 0.002*\"power\" + 0.002*\"member\"\n",
      "2019-10-28 19:27:50,149 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-28 19:27:50,293 : INFO : built Dictionary(15389 unique tokens: ['abolish', 'abolition', 'access', 'accord', 'achieve']...) from 85 documents (total 110007 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms: ['0.004*\"society\" ', ' 0.004*\"black\" ', ' 0.004*\"think\" ', ' 0.004*\"experience\" ', ' 0.003*\"mind\" ', ' 0.003*\"body\" ', ' 0.003*\"social\" ', ' 0.003*\"write\" ', ' 0.003*\"state\" ', ' 0.003*\"system\" ', ' 0.003*\"criminal\" ', ' 0.003*\"consciousness\" ', ' 0.003*\"learn\" ', ' 0.002*\"appear\" ', ' 0.002*\"human\" ', ' 0.002*\"behavior\" ', ' 0.002*\"process\" ', ' 0.002*\"problem\" ', ' 0.002*\"power\" ', ' 0.002*\"member\"']\n",
      "\n",
      "\n",
      "Terms per cluster: {0: ['inmate', 'state', 'system', 'officer', 'write', 'right', 'family', 'help', 'law', 'society', 'think', 'sentence', 'crime', 'staff', 'feel', 'love', 'program', 'criminal', 'correctional', 'proble'], 1: ['society', 'black', 'think', 'experience', 'mind', 'body', 'social', 'write', 'state', 'system', 'criminal', 'consciousness', 'learn', 'appear', 'human', 'behavior', 'process', 'problem', 'power', 'membe']}\n",
      "\n",
      "Terms/Scores: {'society': 0.004, 'black': 0.004, 'think': 0.004, 'experience': 0.004, 'mind': 0.003, 'body': 0.003, 'social': 0.003, 'write': 0.003, 'state': 0.003, 'system': 0.003, 'criminal': 0.003, 'consciousness': 0.003, 'learn': 0.003, 'appear': 0.002, 'human': 0.002, 'behavior': 0.002, 'process': 0.002, 'problem': 0.002, 'power': 0.002, 'membe': 0.002}\n",
      "\n",
      "Themes ranked strongest to weakest: [('Political and Intellectual Labor among IP', 0.003), ('Prison Culture/Community/Society', 0.002)]\n",
      "\n",
      "************ Cluster # 2 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:27:50,403 : INFO : using symmetric alpha at 1.0\n",
      "2019-10-28 19:27:50,404 : INFO : using symmetric eta at 1.0\n",
      "2019-10-28 19:27:50,410 : INFO : using serial LDA version on this node\n",
      "2019-10-28 19:27:50,412 : INFO : running online (multi-pass) LDA training, 1 topics, 10 passes over the supplied corpus of 85 documents, updating model once every 85 documents, evaluating perplexity every 85 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-28 19:27:50,714 : INFO : -9.927 per-word bound, 973.5 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:50,714 : INFO : PROGRESS: pass 0, at document #85/85\n",
      "2019-10-28 19:27:50,765 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:50,766 : INFO : topic diff=1.118190, rho=1.000000\n",
      "2019-10-28 19:27:51,052 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:51,053 : INFO : PROGRESS: pass 1, at document #85/85\n",
      "2019-10-28 19:27:51,096 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:51,097 : INFO : topic diff=0.002110, rho=0.577350\n",
      "2019-10-28 19:27:51,370 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:51,371 : INFO : PROGRESS: pass 2, at document #85/85\n",
      "2019-10-28 19:27:51,423 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:51,424 : INFO : topic diff=0.000868, rho=0.500000\n",
      "2019-10-28 19:27:51,705 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:51,706 : INFO : PROGRESS: pass 3, at document #85/85\n",
      "2019-10-28 19:27:51,748 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:51,749 : INFO : topic diff=0.000394, rho=0.447214\n",
      "2019-10-28 19:27:52,028 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:52,029 : INFO : PROGRESS: pass 4, at document #85/85\n",
      "2019-10-28 19:27:52,075 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:52,076 : INFO : topic diff=0.000201, rho=0.408248\n",
      "2019-10-28 19:27:52,351 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:52,352 : INFO : PROGRESS: pass 5, at document #85/85\n",
      "2019-10-28 19:27:52,399 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:52,400 : INFO : topic diff=0.000107, rho=0.377964\n",
      "2019-10-28 19:27:52,670 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:52,671 : INFO : PROGRESS: pass 6, at document #85/85\n",
      "2019-10-28 19:27:52,709 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:52,710 : INFO : topic diff=0.000047, rho=0.353553\n",
      "2019-10-28 19:27:52,985 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:52,986 : INFO : PROGRESS: pass 7, at document #85/85\n",
      "2019-10-28 19:27:53,030 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:53,030 : INFO : topic diff=0.000050, rho=0.333333\n",
      "2019-10-28 19:27:53,306 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:53,307 : INFO : PROGRESS: pass 8, at document #85/85\n",
      "2019-10-28 19:27:53,348 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:53,350 : INFO : topic diff=0.000020, rho=0.316228\n",
      "2019-10-28 19:27:53,633 : INFO : -8.450 per-word bound, 349.7 perplexity estimate based on a held-out corpus of 85 documents with 110007 words\n",
      "2019-10-28 19:27:53,634 : INFO : PROGRESS: pass 9, at document #85/85\n",
      "2019-10-28 19:27:53,677 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\"\n",
      "2019-10-28 19:27:53,678 : INFO : topic diff=0.000015, rho=0.301511\n",
      "2019-10-28 19:27:53,683 : INFO : topic #0 (1.000): 0.010*\"state\" + 0.008*\"law\" + 0.006*\"court\" + 0.006*\"sentence\" + 0.005*\"crime\" + 0.004*\"system\" + 0.004*\"criminal\" + 0.004*\"parole\" + 0.003*\"offender\" + 0.003*\"right\" + 0.003*\"justice\" + 0.003*\"inmate\" + 0.002*\"federal\" + 0.002*\"government\" + 0.002*\"judge\" + 0.002*\"program\" + 0.002*\"child\" + 0.002*\"public\" + 0.002*\"issue\" + 0.002*\"conviction\"\n",
      "2019-10-28 19:27:53,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms: ['0.010*\"state\" ', ' 0.008*\"law\" ', ' 0.006*\"court\" ', ' 0.006*\"sentence\" ', ' 0.005*\"crime\" ', ' 0.004*\"system\" ', ' 0.004*\"criminal\" ', ' 0.004*\"parole\" ', ' 0.003*\"offender\" ', ' 0.003*\"right\" ', ' 0.003*\"justice\" ', ' 0.003*\"inmate\" ', ' 0.002*\"federal\" ', ' 0.002*\"government\" ', ' 0.002*\"judge\" ', ' 0.002*\"program\" ', ' 0.002*\"child\" ', ' 0.002*\"public\" ', ' 0.002*\"issue\" ', ' 0.002*\"conviction\"']\n",
      "\n",
      "\n",
      "Terms per cluster: {0: ['inmate', 'state', 'system', 'officer', 'write', 'right', 'family', 'help', 'law', 'society', 'think', 'sentence', 'crime', 'staff', 'feel', 'love', 'program', 'criminal', 'correctional', 'proble'], 1: ['society', 'black', 'think', 'experience', 'mind', 'body', 'social', 'write', 'state', 'system', 'criminal', 'consciousness', 'learn', 'appear', 'human', 'behavior', 'process', 'problem', 'power', 'membe'], 2: ['state', 'law', 'court', 'sentence', 'crime', 'system', 'criminal', 'parole', 'offender', 'right', 'justice', 'inmate', 'federal', 'government', 'judge', 'program', 'child', 'public', 'issue', 'convictio']}\n",
      "\n",
      "Terms/Scores: {'state': 0.01, 'law': 0.008, 'court': 0.006, 'sentence': 0.006, 'crime': 0.005, 'system': 0.004, 'criminal': 0.004, 'parole': 0.004, 'offender': 0.003, 'right': 0.003, 'justice': 0.003, 'inmate': 0.003, 'federal': 0.002, 'government': 0.002, 'judge': 0.002, 'program': 0.002, 'child': 0.002, 'public': 0.002, 'issue': 0.002, 'convictio': 0.002}\n",
      "\n",
      "Themes ranked strongest to weakest: [('Judicial Misconduct and Legal Remediation', 0.008), ('Prison Culture/Community/Society', 0.005), ('Staff/prison Abuse of IP', 0.003), ('Social Alienation, Indifference, Hostility', 0.002), ('Family', 0.002)]\n",
      "\n",
      "************ Cluster # 3 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:27:54,007 : INFO : built Dictionary(22467 unique tokens: ['accomplishment', 'accord', 'across', 'active', 'actual']...) from 341 documents (total 205305 corpus positions)\n",
      "2019-10-28 19:27:54,235 : INFO : using symmetric alpha at 1.0\n",
      "2019-10-28 19:27:54,236 : INFO : using symmetric eta at 1.0\n",
      "2019-10-28 19:27:54,245 : INFO : using serial LDA version on this node\n",
      "2019-10-28 19:27:54,250 : INFO : running online (multi-pass) LDA training, 1 topics, 10 passes over the supplied corpus of 341 documents, updating model once every 341 documents, evaluating perplexity every 341 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-28 19:27:54,996 : INFO : -10.304 per-word bound, 1263.8 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:27:54,996 : INFO : PROGRESS: pass 0, at document #341/341\n",
      "2019-10-28 19:27:55,131 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:27:55,132 : INFO : topic diff=1.282173, rho=1.000000\n",
      "2019-10-28 19:27:55,942 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:27:55,943 : INFO : PROGRESS: pass 1, at document #341/341\n",
      "2019-10-28 19:27:56,054 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:27:56,055 : INFO : topic diff=0.003902, rho=0.577350\n",
      "2019-10-28 19:27:56,820 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:27:56,821 : INFO : PROGRESS: pass 2, at document #341/341\n",
      "2019-10-28 19:27:56,931 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:27:56,932 : INFO : topic diff=0.001655, rho=0.500000\n",
      "2019-10-28 19:27:57,733 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:27:57,734 : INFO : PROGRESS: pass 3, at document #341/341\n",
      "2019-10-28 19:27:57,840 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:27:57,841 : INFO : topic diff=0.000734, rho=0.447214\n",
      "2019-10-28 19:27:58,637 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:27:58,639 : INFO : PROGRESS: pass 4, at document #341/341\n",
      "2019-10-28 19:27:58,743 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:27:58,744 : INFO : topic diff=0.000361, rho=0.408248\n",
      "2019-10-28 19:27:59,485 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:27:59,485 : INFO : PROGRESS: pass 5, at document #341/341\n",
      "2019-10-28 19:27:59,593 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:27:59,594 : INFO : topic diff=0.000202, rho=0.377964\n",
      "2019-10-28 19:28:00,269 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:28:00,270 : INFO : PROGRESS: pass 6, at document #341/341\n",
      "2019-10-28 19:28:00,362 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:28:00,363 : INFO : topic diff=0.000120, rho=0.353553\n",
      "2019-10-28 19:28:01,158 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:28:01,158 : INFO : PROGRESS: pass 7, at document #341/341\n",
      "2019-10-28 19:28:01,265 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:28:01,266 : INFO : topic diff=0.000072, rho=0.333333\n",
      "2019-10-28 19:28:01,975 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:28:01,976 : INFO : PROGRESS: pass 8, at document #341/341\n",
      "2019-10-28 19:28:02,100 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:28:02,100 : INFO : topic diff=0.000046, rho=0.316228\n",
      "2019-10-28 19:28:02,958 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 341 documents with 205305 words\n",
      "2019-10-28 19:28:02,959 : INFO : PROGRESS: pass 9, at document #341/341\n",
      "2019-10-28 19:28:03,093 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\"\n",
      "2019-10-28 19:28:03,095 : INFO : topic diff=0.000030, rho=0.301511\n",
      "2019-10-28 19:28:03,100 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"system\" + 0.005*\"inmate\" + 0.004*\"society\" + 0.004*\"crime\" + 0.004*\"program\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"right\" + 0.002*\"criminal\" + 0.002*\"parole\" + 0.002*\"justice\" + 0.002*\"release\" + 0.002*\"family\" + 0.002*\"human\" + 0.002*\"death\" + 0.002*\"help\" + 0.002*\"incarcerate\" + 0.002*\"court\" + 0.002*\"public\"\n",
      "2019-10-28 19:28:03,105 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-28 19:28:03,297 : INFO : built Dictionary(19362 unique tokens: ['abide', 'ability', 'able', 'abridge', 'absolutely']...) from 94 documents (total 116082 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms: ['0.006*\"state\" ', ' 0.005*\"system\" ', ' 0.005*\"inmate\" ', ' 0.004*\"society\" ', ' 0.004*\"crime\" ', ' 0.004*\"program\" ', ' 0.003*\"sentence\" ', ' 0.003*\"law\" ', ' 0.003*\"right\" ', ' 0.002*\"criminal\" ', ' 0.002*\"parole\" ', ' 0.002*\"justice\" ', ' 0.002*\"release\" ', ' 0.002*\"family\" ', ' 0.002*\"human\" ', ' 0.002*\"death\" ', ' 0.002*\"help\" ', ' 0.002*\"incarcerate\" ', ' 0.002*\"court\" ', ' 0.002*\"public\"']\n",
      "\n",
      "\n",
      "Terms per cluster: {0: ['inmate', 'state', 'system', 'officer', 'write', 'right', 'family', 'help', 'law', 'society', 'think', 'sentence', 'crime', 'staff', 'feel', 'love', 'program', 'criminal', 'correctional', 'proble'], 1: ['society', 'black', 'think', 'experience', 'mind', 'body', 'social', 'write', 'state', 'system', 'criminal', 'consciousness', 'learn', 'appear', 'human', 'behavior', 'process', 'problem', 'power', 'membe'], 2: ['state', 'law', 'court', 'sentence', 'crime', 'system', 'criminal', 'parole', 'offender', 'right', 'justice', 'inmate', 'federal', 'government', 'judge', 'program', 'child', 'public', 'issue', 'convictio'], 3: ['state', 'system', 'inmate', 'society', 'crime', 'program', 'sentence', 'law', 'right', 'criminal', 'parole', 'justice', 'release', 'family', 'human', 'death', 'help', 'incarcerate', 'court', 'publi']}\n",
      "\n",
      "Terms/Scores: {'state': 0.006, 'system': 0.005, 'inmate': 0.005, 'society': 0.004, 'crime': 0.004, 'program': 0.004, 'sentence': 0.003, 'law': 0.003, 'right': 0.003, 'criminal': 0.002, 'parole': 0.002, 'justice': 0.002, 'release': 0.002, 'family': 0.002, 'human': 0.002, 'death': 0.002, 'help': 0.002, 'incarcerate': 0.002, 'court': 0.002, 'publi': 0.002}\n",
      "\n",
      "Themes ranked strongest to weakest: [('Prison Culture/Community/Society', 0.006), ('Staff/prison Abuse of IP', 0.003), ('Judicial Misconduct and Legal Remediation', 0.003), ('Social Alienation, Indifference, Hostility', 0.002), ('Health Care', 0.002), ('Family', 0.002)]\n",
      "\n",
      "************ Cluster # 4 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:28:03,419 : INFO : using symmetric alpha at 1.0\n",
      "2019-10-28 19:28:03,420 : INFO : using symmetric eta at 1.0\n",
      "2019-10-28 19:28:03,427 : INFO : using serial LDA version on this node\n",
      "2019-10-28 19:28:03,431 : INFO : running online (multi-pass) LDA training, 1 topics, 10 passes over the supplied corpus of 94 documents, updating model once every 94 documents, evaluating perplexity every 94 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-28 19:28:03,810 : INFO : -10.162 per-word bound, 1145.3 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:03,812 : INFO : PROGRESS: pass 0, at document #94/94\n",
      "2019-10-28 19:28:03,881 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:03,883 : INFO : topic diff=1.054163, rho=1.000000\n",
      "2019-10-28 19:28:04,243 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:04,244 : INFO : PROGRESS: pass 1, at document #94/94\n",
      "2019-10-28 19:28:04,296 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:04,297 : INFO : topic diff=0.002486, rho=0.577350\n",
      "2019-10-28 19:28:04,685 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:04,686 : INFO : PROGRESS: pass 2, at document #94/94\n",
      "2019-10-28 19:28:04,755 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:04,756 : INFO : topic diff=0.000901, rho=0.500000\n",
      "2019-10-28 19:28:05,199 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:05,200 : INFO : PROGRESS: pass 3, at document #94/94\n",
      "2019-10-28 19:28:05,275 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:05,280 : INFO : topic diff=0.000416, rho=0.447214\n",
      "2019-10-28 19:28:05,727 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:05,730 : INFO : PROGRESS: pass 4, at document #94/94\n",
      "2019-10-28 19:28:05,801 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:05,802 : INFO : topic diff=0.000225, rho=0.408248\n",
      "2019-10-28 19:28:06,253 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:06,254 : INFO : PROGRESS: pass 5, at document #94/94\n",
      "2019-10-28 19:28:06,307 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:06,308 : INFO : topic diff=0.000125, rho=0.377964\n",
      "2019-10-28 19:28:06,735 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:06,737 : INFO : PROGRESS: pass 6, at document #94/94\n",
      "2019-10-28 19:28:06,821 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:06,822 : INFO : topic diff=0.000072, rho=0.353553\n",
      "2019-10-28 19:28:07,270 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:07,271 : INFO : PROGRESS: pass 7, at document #94/94\n",
      "2019-10-28 19:28:07,329 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:07,330 : INFO : topic diff=0.000043, rho=0.333333\n",
      "2019-10-28 19:28:07,697 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:07,698 : INFO : PROGRESS: pass 8, at document #94/94\n",
      "2019-10-28 19:28:07,763 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:07,764 : INFO : topic diff=0.000027, rho=0.316228\n",
      "2019-10-28 19:28:08,147 : INFO : -8.776 per-word bound, 438.4 perplexity estimate based on a held-out corpus of 94 documents with 116082 words\n",
      "2019-10-28 19:28:08,148 : INFO : PROGRESS: pass 9, at document #94/94\n",
      "2019-10-28 19:28:08,204 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\"\n",
      "2019-10-28 19:28:08,205 : INFO : topic diff=0.000018, rho=0.301511\n",
      "2019-10-28 19:28:08,214 : INFO : topic #0 (1.000): 0.006*\"state\" + 0.005*\"inmate\" + 0.003*\"staff\" + 0.003*\"right\" + 0.003*\"officer\" + 0.003*\"sentence\" + 0.003*\"law\" + 0.003*\"court\" + 0.002*\"unit\" + 0.002*\"offender\" + 0.002*\"system\" + 0.002*\"write\" + 0.002*\"child\" + 0.002*\"crime\" + 0.002*\"death\" + 0.002*\"drug\" + 0.002*\"issue\" + 0.002*\"family\" + 0.002*\"letter\" + 0.002*\"federal\"\n",
      "2019-10-28 19:28:08,222 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms: ['0.006*\"state\" ', ' 0.005*\"inmate\" ', ' 0.003*\"staff\" ', ' 0.003*\"right\" ', ' 0.003*\"officer\" ', ' 0.003*\"sentence\" ', ' 0.003*\"law\" ', ' 0.003*\"court\" ', ' 0.002*\"unit\" ', ' 0.002*\"offender\" ', ' 0.002*\"system\" ', ' 0.002*\"write\" ', ' 0.002*\"child\" ', ' 0.002*\"crime\" ', ' 0.002*\"death\" ', ' 0.002*\"drug\" ', ' 0.002*\"issue\" ', ' 0.002*\"family\" ', ' 0.002*\"letter\" ', ' 0.002*\"federal\"']\n",
      "\n",
      "\n",
      "Terms per cluster: {0: ['inmate', 'state', 'system', 'officer', 'write', 'right', 'family', 'help', 'law', 'society', 'think', 'sentence', 'crime', 'staff', 'feel', 'love', 'program', 'criminal', 'correctional', 'proble'], 1: ['society', 'black', 'think', 'experience', 'mind', 'body', 'social', 'write', 'state', 'system', 'criminal', 'consciousness', 'learn', 'appear', 'human', 'behavior', 'process', 'problem', 'power', 'membe'], 2: ['state', 'law', 'court', 'sentence', 'crime', 'system', 'criminal', 'parole', 'offender', 'right', 'justice', 'inmate', 'federal', 'government', 'judge', 'program', 'child', 'public', 'issue', 'convictio'], 3: ['state', 'system', 'inmate', 'society', 'crime', 'program', 'sentence', 'law', 'right', 'criminal', 'parole', 'justice', 'release', 'family', 'human', 'death', 'help', 'incarcerate', 'court', 'publi'], 4: ['state', 'inmate', 'staff', 'right', 'officer', 'sentence', 'law', 'court', 'unit', 'offender', 'system', 'write', 'child', 'crime', 'death', 'drug', 'issue', 'family', 'letter', 'federa']}\n",
      "\n",
      "Terms/Scores: {'state': 0.006, 'inmate': 0.005, 'staff': 0.003, 'right': 0.003, 'officer': 0.003, 'sentence': 0.003, 'law': 0.003, 'court': 0.003, 'unit': 0.002, 'offender': 0.002, 'system': 0.002, 'write': 0.002, 'child': 0.002, 'crime': 0.002, 'death': 0.002, 'drug': 0.002, 'issue': 0.002, 'family': 0.002, 'letter': 0.002, 'federa': 0.002}\n",
      "\n",
      "Themes ranked strongest to weakest: [('Prison Culture/Community/Society', 0.005), ('Family', 0.004), ('Judicial Misconduct and Legal Remediation', 0.003), ('Staff/prison Abuse of IP', 0.003), ('Physical Conditions and Security', 0.003), ('Autobiographical, Paths to Prison', 0.002), ('Health Care', 0.002), ('Political and Intellectual Labor among IP', 0.002), ('Personal/Intern Change/Coping', 0.002)]\n",
      "\n",
      "************ Cluster # 5 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:28:08,583 : INFO : built Dictionary(21174 unique tokens: ['accord', 'administration', 'admire', 'admit', 'advocate']...) from 336 documents (total 185152 corpus positions)\n",
      "2019-10-28 19:28:08,819 : INFO : using symmetric alpha at 1.0\n",
      "2019-10-28 19:28:08,822 : INFO : using symmetric eta at 1.0\n",
      "2019-10-28 19:28:08,833 : INFO : using serial LDA version on this node\n",
      "2019-10-28 19:28:08,840 : INFO : running online (multi-pass) LDA training, 1 topics, 10 passes over the supplied corpus of 336 documents, updating model once every 336 documents, evaluating perplexity every 336 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-28 19:28:09,531 : INFO : -10.248 per-word bound, 1216.1 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:09,532 : INFO : PROGRESS: pass 0, at document #336/336\n",
      "2019-10-28 19:28:09,657 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:09,658 : INFO : topic diff=1.243589, rho=1.000000\n",
      "2019-10-28 19:28:10,326 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:10,327 : INFO : PROGRESS: pass 1, at document #336/336\n",
      "2019-10-28 19:28:10,433 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:10,434 : INFO : topic diff=0.003437, rho=0.577350\n",
      "2019-10-28 19:28:11,085 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:11,085 : INFO : PROGRESS: pass 2, at document #336/336\n",
      "2019-10-28 19:28:11,183 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:11,184 : INFO : topic diff=0.001481, rho=0.500000\n",
      "2019-10-28 19:28:11,967 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:11,967 : INFO : PROGRESS: pass 3, at document #336/336\n",
      "2019-10-28 19:28:12,079 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:12,081 : INFO : topic diff=0.000674, rho=0.447214\n",
      "2019-10-28 19:28:12,827 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:12,828 : INFO : PROGRESS: pass 4, at document #336/336\n",
      "2019-10-28 19:28:12,922 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:12,923 : INFO : topic diff=0.000344, rho=0.408248\n",
      "2019-10-28 19:28:13,628 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:13,629 : INFO : PROGRESS: pass 5, at document #336/336\n",
      "2019-10-28 19:28:13,721 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:13,722 : INFO : topic diff=0.000191, rho=0.377964\n",
      "2019-10-28 19:28:14,364 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:14,366 : INFO : PROGRESS: pass 6, at document #336/336\n",
      "2019-10-28 19:28:14,464 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:14,464 : INFO : topic diff=0.000114, rho=0.353553\n",
      "2019-10-28 19:28:15,215 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:15,216 : INFO : PROGRESS: pass 7, at document #336/336\n",
      "2019-10-28 19:28:15,319 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:15,320 : INFO : topic diff=0.000072, rho=0.333333\n",
      "2019-10-28 19:28:15,984 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:15,985 : INFO : PROGRESS: pass 8, at document #336/336\n",
      "2019-10-28 19:28:16,088 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:16,088 : INFO : topic diff=0.000046, rho=0.316228\n",
      "2019-10-28 19:28:16,739 : INFO : -8.656 per-word bound, 403.5 perplexity estimate based on a held-out corpus of 336 documents with 185152 words\n",
      "2019-10-28 19:28:16,740 : INFO : PROGRESS: pass 9, at document #336/336\n",
      "2019-10-28 19:28:16,840 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\"\n",
      "2019-10-28 19:28:16,841 : INFO : topic diff=0.000032, rho=0.301511\n",
      "2019-10-28 19:28:16,845 : INFO : topic #0 (1.000): 0.006*\"inmate\" + 0.003*\"write\" + 0.003*\"love\" + 0.003*\"think\" + 0.003*\"help\" + 0.003*\"officer\" + 0.002*\"right\" + 0.002*\"state\" + 0.002*\"family\" + 0.002*\"god\" + 0.002*\"friend\" + 0.002*\"feel\" + 0.002*\"mind\" + 0.002*\"thought\" + 0.002*\"word\" + 0.002*\"live\" + 0.002*\"care\" + 0.002*\"find\" + 0.002*\"death\" + 0.002*\"learn\"\n",
      "2019-10-28 19:28:16,850 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-28 19:28:17,020 : INFO : built Dictionary(15339 unique tokens: ['a_', 'a_half', 'acceptance', 'acute', 'adjust']...) from 80 documents (total 109017 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms: ['0.006*\"inmate\" ', ' 0.003*\"write\" ', ' 0.003*\"love\" ', ' 0.003*\"think\" ', ' 0.003*\"help\" ', ' 0.003*\"officer\" ', ' 0.002*\"right\" ', ' 0.002*\"state\" ', ' 0.002*\"family\" ', ' 0.002*\"god\" ', ' 0.002*\"friend\" ', ' 0.002*\"feel\" ', ' 0.002*\"mind\" ', ' 0.002*\"thought\" ', ' 0.002*\"word\" ', ' 0.002*\"live\" ', ' 0.002*\"care\" ', ' 0.002*\"find\" ', ' 0.002*\"death\" ', ' 0.002*\"learn\"']\n",
      "\n",
      "\n",
      "Terms per cluster: {0: ['inmate', 'state', 'system', 'officer', 'write', 'right', 'family', 'help', 'law', 'society', 'think', 'sentence', 'crime', 'staff', 'feel', 'love', 'program', 'criminal', 'correctional', 'proble'], 1: ['society', 'black', 'think', 'experience', 'mind', 'body', 'social', 'write', 'state', 'system', 'criminal', 'consciousness', 'learn', 'appear', 'human', 'behavior', 'process', 'problem', 'power', 'membe'], 2: ['state', 'law', 'court', 'sentence', 'crime', 'system', 'criminal', 'parole', 'offender', 'right', 'justice', 'inmate', 'federal', 'government', 'judge', 'program', 'child', 'public', 'issue', 'convictio'], 3: ['state', 'system', 'inmate', 'society', 'crime', 'program', 'sentence', 'law', 'right', 'criminal', 'parole', 'justice', 'release', 'family', 'human', 'death', 'help', 'incarcerate', 'court', 'publi'], 4: ['state', 'inmate', 'staff', 'right', 'officer', 'sentence', 'law', 'court', 'unit', 'offender', 'system', 'write', 'child', 'crime', 'death', 'drug', 'issue', 'family', 'letter', 'federa'], 5: ['inmate', 'write', 'love', 'think', 'help', 'officer', 'right', 'state', 'family', 'god', 'friend', 'feel', 'mind', 'thought', 'word', 'live', 'care', 'find', 'death', 'lear']}\n",
      "\n",
      "Terms/Scores: {'inmate': 0.006, 'write': 0.003, 'love': 0.003, 'think': 0.003, 'help': 0.003, 'officer': 0.003, 'right': 0.002, 'state': 0.002, 'family': 0.002, 'god': 0.002, 'friend': 0.002, 'feel': 0.002, 'mind': 0.002, 'thought': 0.002, 'word': 0.002, 'live': 0.002, 'care': 0.002, 'find': 0.002, 'death': 0.002, 'lear': 0.002}\n",
      "\n",
      "Themes ranked strongest to weakest: [('Health Care', 0.004), ('Political and Intellectual Labor among IP', 0.003), ('Personal/Intern Change/Coping', 0.002), ('Family', 0.002), ('Staff/prison Abuse of IP', 0.002)]\n",
      "\n",
      "************ Cluster # 6 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-28 19:28:17,136 : INFO : using symmetric alpha at 1.0\n",
      "2019-10-28 19:28:17,137 : INFO : using symmetric eta at 1.0\n",
      "2019-10-28 19:28:17,143 : INFO : using serial LDA version on this node\n",
      "2019-10-28 19:28:17,146 : INFO : running online (multi-pass) LDA training, 1 topics, 10 passes over the supplied corpus of 80 documents, updating model once every 80 documents, evaluating perplexity every 80 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2019-10-28 19:28:17,472 : INFO : -9.924 per-word bound, 971.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:17,473 : INFO : PROGRESS: pass 0, at document #80/80\n",
      "2019-10-28 19:28:17,531 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:17,532 : INFO : topic diff=1.086199, rho=1.000000\n",
      "2019-10-28 19:28:17,859 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:17,860 : INFO : PROGRESS: pass 1, at document #80/80\n",
      "2019-10-28 19:28:17,907 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:17,907 : INFO : topic diff=0.002012, rho=0.577350\n",
      "2019-10-28 19:28:18,233 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:18,234 : INFO : PROGRESS: pass 2, at document #80/80\n",
      "2019-10-28 19:28:18,281 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:18,282 : INFO : topic diff=0.000816, rho=0.500000\n",
      "2019-10-28 19:28:18,606 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:18,606 : INFO : PROGRESS: pass 3, at document #80/80\n",
      "2019-10-28 19:28:18,655 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:18,655 : INFO : topic diff=0.000370, rho=0.447214\n",
      "2019-10-28 19:28:18,983 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:18,984 : INFO : PROGRESS: pass 4, at document #80/80\n",
      "2019-10-28 19:28:19,032 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:19,033 : INFO : topic diff=0.000189, rho=0.408248\n",
      "2019-10-28 19:28:19,354 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:19,354 : INFO : PROGRESS: pass 5, at document #80/80\n",
      "2019-10-28 19:28:19,401 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:19,402 : INFO : topic diff=0.000103, rho=0.377964\n",
      "2019-10-28 19:28:19,737 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:19,738 : INFO : PROGRESS: pass 6, at document #80/80\n",
      "2019-10-28 19:28:19,785 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:19,786 : INFO : topic diff=0.000060, rho=0.353553\n",
      "2019-10-28 19:28:20,126 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:20,127 : INFO : PROGRESS: pass 7, at document #80/80\n",
      "2019-10-28 19:28:20,174 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:20,175 : INFO : topic diff=0.000034, rho=0.333333\n",
      "2019-10-28 19:28:20,493 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:20,494 : INFO : PROGRESS: pass 8, at document #80/80\n",
      "2019-10-28 19:28:20,547 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:20,549 : INFO : topic diff=0.000020, rho=0.316228\n",
      "2019-10-28 19:28:20,876 : INFO : -8.617 per-word bound, 392.7 perplexity estimate based on a held-out corpus of 80 documents with 109017 words\n",
      "2019-10-28 19:28:20,877 : INFO : PROGRESS: pass 9, at document #80/80\n",
      "2019-10-28 19:28:20,931 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\"\n",
      "2019-10-28 19:28:20,932 : INFO : topic diff=0.000010, rho=0.301511\n",
      "2019-10-28 19:28:20,938 : INFO : topic #0 (1.000): 0.003*\"guard\" + 0.003*\"door\" + 0.003*\"head\" + 0.003*\"hand\" + 0.002*\"face\" + 0.002*\"room\" + 0.002*\"inmate\" + 0.002*\"mother\" + 0.002*\"eye\" + 0.002*\"little\" + 0.002*\"right\" + 0.002*\"walk\" + 0.002*\"think\" + 0.002*\"night\" + 0.002*\"home\" + 0.002*\"state\" + 0.002*\"wall\" + 0.002*\"turn\" + 0.002*\"love\" + 0.002*\"thought\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms: ['0.003*\"guard\" ', ' 0.003*\"door\" ', ' 0.003*\"head\" ', ' 0.003*\"hand\" ', ' 0.002*\"face\" ', ' 0.002*\"room\" ', ' 0.002*\"inmate\" ', ' 0.002*\"mother\" ', ' 0.002*\"eye\" ', ' 0.002*\"little\" ', ' 0.002*\"right\" ', ' 0.002*\"walk\" ', ' 0.002*\"think\" ', ' 0.002*\"night\" ', ' 0.002*\"home\" ', ' 0.002*\"state\" ', ' 0.002*\"wall\" ', ' 0.002*\"turn\" ', ' 0.002*\"love\" ', ' 0.002*\"thought\"']\n",
      "\n",
      "\n",
      "Terms per cluster: {0: ['inmate', 'state', 'system', 'officer', 'write', 'right', 'family', 'help', 'law', 'society', 'think', 'sentence', 'crime', 'staff', 'feel', 'love', 'program', 'criminal', 'correctional', 'proble'], 1: ['society', 'black', 'think', 'experience', 'mind', 'body', 'social', 'write', 'state', 'system', 'criminal', 'consciousness', 'learn', 'appear', 'human', 'behavior', 'process', 'problem', 'power', 'membe'], 2: ['state', 'law', 'court', 'sentence', 'crime', 'system', 'criminal', 'parole', 'offender', 'right', 'justice', 'inmate', 'federal', 'government', 'judge', 'program', 'child', 'public', 'issue', 'convictio'], 3: ['state', 'system', 'inmate', 'society', 'crime', 'program', 'sentence', 'law', 'right', 'criminal', 'parole', 'justice', 'release', 'family', 'human', 'death', 'help', 'incarcerate', 'court', 'publi'], 4: ['state', 'inmate', 'staff', 'right', 'officer', 'sentence', 'law', 'court', 'unit', 'offender', 'system', 'write', 'child', 'crime', 'death', 'drug', 'issue', 'family', 'letter', 'federa'], 5: ['inmate', 'write', 'love', 'think', 'help', 'officer', 'right', 'state', 'family', 'god', 'friend', 'feel', 'mind', 'thought', 'word', 'live', 'care', 'find', 'death', 'lear'], 6: ['guard', 'door', 'head', 'hand', 'face', 'room', 'inmate', 'mother', 'eye', 'little', 'right', 'walk', 'think', 'night', 'home', 'state', 'wall', 'turn', 'love', 'though']}\n",
      "\n",
      "Terms/Scores: {'guard': 0.003, 'door': 0.003, 'head': 0.003, 'hand': 0.003, 'face': 0.002, 'room': 0.002, 'inmate': 0.002, 'mother': 0.002, 'eye': 0.002, 'little': 0.002, 'right': 0.002, 'walk': 0.002, 'think': 0.002, 'night': 0.002, 'home': 0.002, 'state': 0.002, 'wall': 0.002, 'turn': 0.002, 'love': 0.002, 'though': 0.002}\n",
      "\n",
      "Themes ranked strongest to weakest: [('Staff/prison Abuse of IP', 0.002), ('Family', 0.002)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms_per_cluster = {}\n",
    "themes_per_cluster = {}\n",
    "\n",
    "#Apply LDA for each cluster (and for each essay in cluster)\n",
    "for i in range(num_clusters):\n",
    "    print(\"************ Cluster # {} ************\".format(i))\n",
    "    essays_in_cluster = [tokens for tokens in list(output[output.cluster == i].essay)]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(essays_in_cluster)\n",
    "    \n",
    "    cluster_corpus = [dictionary.doc2bow(essay) for essay in essays_in_cluster]\n",
    "    \n",
    "    lda = models.ldamodel.LdaModel(corpus=cluster_corpus, id2word=dictionary, num_topics=1, passes=10)\n",
    "    \n",
    "    term_score = {}\n",
    "    terms_per_cluster[i] = []\n",
    "    for idx, terms in lda.print_topics(i, 20):\n",
    "        #terms_per_cluster[i] = terms\n",
    "        \n",
    "        print('Top terms: {}'.format(terms.split('+')))\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        for term_with_score in terms.split('+'):\n",
    "            term = term_with_score.split('*')[1][1:-2]\n",
    "            score = term_with_score.split('*')[0]\n",
    "            \n",
    "            #print(\"term is {} with score {}\".format(term, score))\n",
    "            \n",
    "            terms_per_cluster[i].append(term)\n",
    "            term_score[term] = float(score)\n",
    "    \n",
    "    print(\"Terms per cluster:\", terms_per_cluster)\n",
    "    print()\n",
    "    print(\"Terms/Scores:\", term_score)\n",
    "    print()\n",
    "    \n",
    "    themes_per_cluster[i] = {}\n",
    "    theme_term_score = {}\n",
    "    for term in terms_per_cluster[i]:\n",
    "        for theme, defining_terms in theme_index.items():\n",
    "            if term in defining_terms:\n",
    "                if theme in theme_term_score:\n",
    "                    theme_term_score[theme] += term_score[term]\n",
    "                else:\n",
    "                    theme_term_score[theme] = term_score[term]\n",
    "                \n",
    "    themes_per_cluster[i] = sorted(theme_term_score.items(), key = lambda x : x[1])[::-1]\n",
    "    print(\"Themes ranked strongest to weakest: {}\".format(themes_per_cluster[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
